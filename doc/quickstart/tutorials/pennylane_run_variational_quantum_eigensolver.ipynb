{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nVariational quantum eigensolver\n===============================\n\nThis example demonstrates the principle of a variational quantum\neigensolver (VQE), originally proposed in `Peruzzo et al.\n(2014) <https://www.nature.com/articles/ncomms5213>`__. To showcase the\nhybrid computational capabilities of PennyLane, we first train a quantum\ncircuit to minimize the squared energy expectation for a Hamiltonian\n$H$,\n\n\\begin{align}\\langle \\psi_v | H | \\psi_v \\rangle^2  =( 0.1 \\langle \\psi_{v} | X_2 |\n    \\psi_v \\rangle + 0.5 \\langle \\psi_v | Y_2 | \\psi_v \\rangle )^2.\\end{align}\n\nHere, $|\\psi_v\\rangle$ is the state\nobtained after applying a quantum circuit to an initial state\n$|0\\rangle$. The quantum circuit depends on trainable variables\n$v = \\{v_1, v_2\\}$, and $X_2$, $Y_2$ denote the\nPauli-X and Pauli-Y operator acting on the second qubit (*Note: We apply\nthe square to make the optimization landscapes more interesting, but in\ncommon applications the cost is directly the energy expectation value*).\n\nAfter doing this, we will then turn things around and use a fixed\nquantum circuit to prepare a state $|\\psi\\rangle$, but train the coefficients of\nthe Hamiltonian to minimize\n\n\\begin{align}\\langle \\psi | H | \\psi \\rangle^2  = (v_1 \\langle \\psi | X_2 | \\psi\n    \\rangle + v_2 \\langle \\psi | Y_2 | \\psi \\rangle )^2 .\\end{align}\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Optimizing the quantum circuit\n---------------------------------\n\nImports\n~~~~~~~\n\nWe begin by importing PennyLane, the PennyLane-wrapped version of NumPy,\nand the GradientDescentOptimizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.optimize import GradientDescentOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the default qubit simulator as a device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum nodes\n~~~~~~~~~~~~~\n\nThe quantum circuit of the variational eigensolver is an ansatz that\ndefines a manifold of possible quantum states. We use a Hadamard, two\nrotations and a CNOT gate to construct our circuit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ansatz(var):\n    qml.Rot(0.3, 1.8, 5.4, wires=1)\n    qml.RX(var[0], wires=0)\n    qml.RY(var[1], wires=1)\n    qml.CNOT(wires=[0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A variational eigensolver requires us to evaluate expectations of\ndifferent Pauli operators. In this example, the Hamiltonian is expressed\nby only two single-qubit Pauli operators, namely the X and Y operator\napplied to the first qubit.\n\nSince these operators will be measured on the same wire, we will need to\ncreate two quantum nodes (one for each operator whose expectation value\nwe measure), but we can reuse the same device.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If the Pauli observables were evaluated on different wires, we\n    could use one quantum node and return a tuple of expectations in only\n    one quantum node:\n    ``return qml.expectation.PauliX(0), qml.expectation.PauliY(1)``</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef circuit_X(var):\n    ansatz(var)\n    return qml.expval(qml.PauliX(1))\n\n\n@qml.qnode(dev)\ndef circuit_Y(var):\n    ansatz(var)\n    return qml.expval(qml.PauliY(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Objective\n~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The cost function to be optimized in VQE is simply a linear combination\n# of the expectations, which defines the expectation of the Hamiltonian we\n# are interested in. In our case, we square this cost function to provide\n# a more interesting landscape with the same minima.\n\n\ndef cost(var):\n    expX = circuit_X(var)\n    expY = circuit_Y(var)\n    return (0.1 * expX + 0.5 * expY) ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This cost defines the following landscape:\n\n*Note: To run the following cell you need the matplotlib library.*\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import MaxNLocator\n\nfig = plt.figure(figsize=(6, 4))\nax = fig.gca(projection=\"3d\")\n\nX = np.linspace(-3.0, 3.0, 20)\nY = np.linspace(-3.0, 3.0, 20)\nxx, yy = np.meshgrid(X, Y)\nZ = np.array([[cost([x, y]) for x in X] for y in Y]).reshape(len(Y), len(X))\nsurf = ax.plot_surface(xx, yy, Z, cmap=cm.coolwarm, antialiased=False)\n\nax.set_xlabel(\"v1\")\nax.set_ylabel(\"v2\")\nax.zaxis.set_major_locator(MaxNLocator(nbins=5, prune=\"lower\"))\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n~~~~~~~~~~~~\n\nWe create a GradientDescentOptimizer and use it to optimize the cost\nfunction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = GradientDescentOptimizer(0.5)\n\nvar = [0.3, 2.5]\nvar_gd = [var]\nfor it in range(20):\n    var = opt.step(cost, var)\n    var_gd.append(var)\n\n    print(\n        \"Cost after step {:5d}: {: .7f} | Variables: [{: .5f},{: .5f}]\".format(\n            it + 1, cost(var), var[0], var[1]\n        )\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the path that the variables took during gradient descent. To\nmake the plot more clear, we will shorten the range for $v_2$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 4))\nax = fig.gca(projection=\"3d\")\n\nX = np.linspace(-3, np.pi / 2, 20)\nY = np.linspace(-3, 3, 20)\nxx, yy = np.meshgrid(X, Y)\nZ = np.array([[cost([x, y]) for x in X] for y in Y]).reshape(len(Y), len(X))\nsurf = ax.plot_surface(xx, yy, Z, cmap=cm.coolwarm, antialiased=False)\n\npath_z = [cost(var) + 1e-8 for var in var_gd]\npath_x = [v[0] for v in var_gd]\npath_y = [v[1] for v in var_gd]\nax.plot(path_x, path_y, path_z, c=\"green\", marker=\".\", label=\"graddesc\")\n\nax.set_xlabel(\"v1\")\nax.set_ylabel(\"v2\")\nax.zaxis.set_major_locator(MaxNLocator(nbins=5, prune=\"lower\"))\n\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Optimizing the Hamiltonian coefficients\n------------------------------------------\n\nInstead of optimizing the circuit parameters, we can also use a fixed\ncircuit,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ansatz():\n    qml.Rot(0.3, 1.8, 5.4, wires=1)\n    qml.RX(-0.5, wires=0)\n    qml.RY(0.5, wires=1)\n    qml.CNOT(wires=[0, 1])\n\n\n@qml.qnode(dev)\ndef circuit_X():\n    ansatz()\n    return qml.expval(qml.PauliX(1))\n\n\n@qml.qnode(dev)\ndef circuit_Y():\n    ansatz()\n    return qml.expval(qml.PauliY(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and make the classical coefficients that appear in the Hamiltonian the\ntrainable variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(var):\n    expX = circuit_X()\n    expY = circuit_Y()\n    return (var[0] * expX + var[1] * expY) ** 2\n\n\nopt = GradientDescentOptimizer(0.5)\n\nvar = [0.3, 2.5]\nvar_gd = [var]\nfor it in range(20):\n    var = opt.step(cost, var)\n    var_gd.append(var)\n\n    print(\n        \"Cost after step {:5d}: {: .7f} | Variables: [{: .5f},{: .5f}]\".format(\n            it + 1, cost(var), var[0], var[1]\n        )\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The landscape has a quadratic shape.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(6, 4))\nax = fig.gca(projection=\"3d\")\n\nX = np.linspace(-3, np.pi / 2, 20)\nY = np.linspace(-3, 3, 20)\nxx, yy = np.meshgrid(X, Y)\nZ = np.array([[cost([x, y]) for x in X] for y in Y]).reshape(len(Y), len(X))\nsurf = ax.plot_surface(xx, yy, Z, cmap=cm.coolwarm, antialiased=False)\n\npath_z = [cost(var) + 1e-8 for var in var_gd]\npath_x = [v[0] for v in var_gd]\npath_y = [v[1] for v in var_gd]\nax.plot(path_x, path_y, path_z, c=\"pink\", marker=\".\", label=\"graddesc\")\n\nax.set_xlabel(\"v1\")\nax.set_ylabel(\"v2\")\nax.zaxis.set_major_locator(MaxNLocator(nbins=5, prune=\"lower\"))\n\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Optimizing classical and quantum parameters\n----------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Finally, we can optimize *classical* and *quantum* weights together by\n# combining the two approaches from above.\n\n\ndef ansatz(var):\n\n    qml.Rot(0.3, 1.8, 5.4, wires=1)\n    qml.RX(var[0], wires=0)\n    qml.RY(var[1], wires=1)\n    qml.CNOT(wires=[0, 1])\n\n\n@qml.qnode(dev)\ndef circuit_X(var):\n    ansatz(var)\n    return qml.expval(qml.PauliX(1))\n\n\n@qml.qnode(dev)\ndef circuit_Y(var):\n    ansatz(var)\n    return qml.expval(qml.PauliY(1))\n\n\ndef cost(var):\n\n    expX = circuit_X(var)\n    expY = circuit_Y(var)\n\n    return (var[2] * expX + var[3] * expY) ** 2\n\n\nopt = GradientDescentOptimizer(0.5)\nvar = [0.3, 2.5, 0.3, 2.5]\n\nfor it in range(10):\n    var = opt.step(cost, var)\n    print(\"Cost after step {:5d}: {: 0.7f}\".format(it + 1, cost(var)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}