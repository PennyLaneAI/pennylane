{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nPyTorch and noisy devices\n=========================\n\nLet's revisit the original `qubit rotation <qubit_rotation>` tutorial, but instead of\nusing the default NumPy/autograd QNode interface, we'll use the `torch_qnode`.\nWe'll also replace the ``default.qubit`` device with a noisy ``forest.qvm`` device, to\nsee how the optimization responds to noisy qubits.\n\n\nTo follow along with this tutorial on your own computer, you will require the\nfollowing dependencies:\n\n* The `Forest SDK <https://rigetti.com/forest>`_, which contains the quantum virtual\n  machine (QVM) and quilc quantum compiler. Once installed, the QVM and quilc can be\n  started by running the commands ``quilc -S`` and ``qvm -S`` in separate terminal windows.\n\n* `PennyLane-Forest plugin <https://pennylane-forest.readthedocs.io>`_, in order\n  to access the QVM as a PennyLane device. This can be installed via pip:\n\n  .. code-block:: bash\n\n      pip install pennylane-forest\n\n* `PyTorch <https://pytorch.org/get-started/locally/>`_, in order to access the PyTorch\n  QNode interface. Follow the link for instructions on the best way to install PyTorch\n  for your system.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up the device\n---------------------\n\nOnce the dependencies above are installed, let's begin importing the required packages\nand setting up our quantum device.\n\nTo start with, we import PennyLane, and, as we are using the PyTorch interface,\nPyTorch as well:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nimport torch\nfrom torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we do not need to import the wrapped version of NumPy provided by PennyLane,\nas we are not using the default QNode NumPy interface. If NumPy is needed, it is fine to\nimport vanilla NumPy for use with PyTorch and TensorFlow.\n\nNext, we will create our device:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"forest.qvm\", device=\"2q\", noisy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we create a noisy two-qubit system, simulated via the QVM. If we wish, we could\nalso build the model on a physical device, such as the ``Aspen-1`` QPU.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Constructing the QNode\n----------------------\n\nNow that we have initialized the device, we can construct our quantum node. Like the\nother tutorials, we use the :mod:`qnode decorator <pennylane.decorator>` to convert\nour quantum function (encoded by the circuit above) into a quantum node\nrunning on the QVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev, interface=\"torch\")\ndef circuit(phi, theta):\n    qml.RX(theta, wires=0)\n    qml.RZ(phi, wires=0)\n    return qml.expval(qml.PauliZ(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make the QNode 'PyTorch aware', we need to specify that the QNode interfaces\nwith PyTorch. This is done by passing the ``interface='torch'`` keyword argument.\n\nAs a result, this QNode will be set up to accept and return PyTorch tensors, and will\nalso automatically calculate any analytic gradients when PyTorch performs backpropagation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n------------\n\nWe can now create our optimization cost function. To introduce some additional\ncomplexity into the system, rather than simply training the variational circuit\nto 'flip a qubit' from state $\\left|0\\right\\rangle$ to state $\\left|1\\right\\rangle$, let's also\nmodify the target state every 100 steps. For example, for the first 100 steps,\nthe target state will be $\\left|1\\right\\rangle$; this will then change to $\\left|0\\right\\rangle$\nfor steps 100 and 200, before changing back to state $\\left|1\\right\\rangle$ for steps 200\nto 300, and so on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(phi, theta, step):\n    target = -(-1) ** (step // 100)\n    return torch.abs(circuit(phi, theta) - target) ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the cost function is defined, we can begin the PyTorch optimization.\nWe create two variables, representing the two free parameters of the variational\ncircuit, and initialize an Adam optimizer:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "phi = Variable(torch.tensor(1.0), requires_grad=True)\ntheta = Variable(torch.tensor(0.05), requires_grad=True)\nopt = torch.optim.Adam([phi, theta], lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we are using the PyTorch interface, we must use PyTorch optimizers,\n*not* the built-in optimizers provided by PennyLane. The built-in optimizers\nonly apply to the default NumPy/autograd interface.\n\nOptimizing the system for 400 steps:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in range(400):\n    opt.zero_grad()\n    loss = cost(phi, theta, i)\n    loss.backward()\n    opt.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now check the final values of the parameters, as well as the final\ncircuit output and cost function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(phi)\nprint(theta)\nprint(circuit(phi, theta))\nprint(cost(phi, theta, 400))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n   tensor(-0.7055, requires_grad=True)\n   tensor(6.1330, requires_grad=True)\n   tensor(0.9551, dtype=torch.float64, grad_fn=<_TorchQNodeBackward>)\n   tensor(3.7162, dtype=torch.float64, grad_fn=<PowBackward0>)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the cost function is step-dependent, this does not provide enough detail to\ndetermine if the optimization was successful; instead, let's plot the output\nstate of the circuit over time on a Bloch sphere:\n\n.. figure:: ../../examples/figures/bloch.gif\n    :align: center\n    :target: javascript:void(0);\n\nHere, the red x is the target state of the variational circuit, and the arrow is\nthe variational circuit output state. As the target state changes, the circuit\nlearns to produce the new target state!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hybrid GPU-QPU optimization\n---------------------------\n\nAs PyTorch natively supports GPU-accelerated classical processing, and Forest provides\nquantum hardware access in the form of QPUs, with very little modification, we can run\nthe above code as a hybrid GPU-QPU optimization (note that to run the following\nscript, you will need to be using Rigetti's QCS service):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nimport torch\nfrom torch.autograd import Variable\n\nqpu = qml.device(\"forest.qpu\", device=\"Aspen-1-2Q-B\")\n\n\n@qml.qnode(dev, interface=\"torch\")\ndef circuit(phi, theta):\n    qml.RX(theta, wires=0)\n    qml.RZ(phi, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef cost(phi, theta, step):\n    target = -(-1) ** (step // 100)\n    return torch.abs(circuit(phi, theta) - target) ** 2\n\n\nphi = Variable(torch.tensor(1.0, device=\"cuda\"), requires_grad=True)\ntheta = Variable(torch.tensor(0.05, device=\"cuda\"), requires_grad=True)\nopt = torch.optim.Adam([phi, theta], lr=0.1)\n\nfor i in range(400):\n    opt.zero_grad()\n    loss = cost(phi, theta, i)\n    loss.backward()\n    opt.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When using a classical interface that supports GPUs, the QNode will automatically\ncopy any tensor arguments to the CPU, before applying them on the specified quantum\ndevice. Once done, it will return a tensor containing the QNode result, and\nautomatically copy it back to the GPU for any further classical processing.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>For more details on the PyTorch interface, see `torch_qnode`.</p></div>\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}