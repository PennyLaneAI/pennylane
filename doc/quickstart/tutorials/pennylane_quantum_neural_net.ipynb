{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nFunction fitting with a quantum neural network\n==============================================\n\nIn this example we show how a variational circuit can be used to learn a\nfit for a one-dimensional function when being trained with noisy samples\nfrom that function.\n\nThe variational circuit we use is the continuous-variable quantum neural\nnetwork model described in `Killoran et al.\n(2018) <https://arxiv.org/abs/1806.06871>`__.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n~~~~~~~\n\nWe import PennyLane, the wrapped version of NumPy provided by PennyLane,\nand an optimizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.optimize import AdamOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The device we use is the Strawberry Fields simulator, this time with\nonly one quantum mode (or ``wire``). You will need to have the\nStrawberry Fields plugin for PennyLane installed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"strawberryfields.fock\", wires=1, cutoff_dim=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum node\n~~~~~~~~~~~~\n\nFor a single quantum mode, each layer of the variational circuit is\ndefined as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(v):\n    # Matrix multiplication of input layer\n    qml.Rotation(v[0], wires=0)\n    qml.Squeezing(v[1], 0.0, wires=0)\n    qml.Rotation(v[2], wires=0)\n\n    # Bias\n    qml.Displacement(v[3], 0.0, wires=0)\n\n    # Element-wise nonlinear transformation\n    qml.Kerr(v[4], wires=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variational circuit in the quantum node first encodes the input into\nthe displacement of the mode, and then executes the layers. The output\nis the expectation of the x-quadrature.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef quantum_neural_net(var, x=None):\n    # Encode input x into quantum state\n    qml.Displacement(x, 0.0, wires=0)\n\n    # \"layer\" subcircuits\n    for v in var:\n        layer(v)\n\n    return qml.expval(qml.X(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Objective\n~~~~~~~~~\n\nAs an objective we take the square loss between target labels and model\npredictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def square_loss(labels, predictions):\n    loss = 0\n    for l, p in zip(labels, predictions):\n        loss = loss + (l - p) ** 2\n\n    loss = loss / len(labels)\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the cost function, we compute the outputs from the variational\ncircuit. Function fitting is a regression problem, and we interpret the\nexpectations from the quantum node as predictions (i.e., without\napplying postprocessing such as thresholding).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(var, features, labels):\n    preds = [quantum_neural_net(var, x=x) for x in features]\n    return square_loss(labels, preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n~~~~~~~~~~~~\n\nWe load noisy data samples of a sine function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"data/sine.txt\")\nX = data[:, 0]\nY = data[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before training a model, let\u2019s examine the data.\n\n*Note: For the next cell to work you need the matplotlib library.*\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(X, Y)\nplt.xlabel(\"x\", fontsize=18)\nplt.ylabel(\"f(x)\", fontsize=18)\nplt.tick_params(axis=\"both\", which=\"major\", labelsize=16)\nplt.tick_params(axis=\"both\", which=\"minor\", labelsize=16)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../../examples/figures/qnn_output_20_0.png)\n\n\nThe network\u2019s weights (called ``var`` here) are initialized with values\nsampled from a normal distribution. We use 4 layers; performance has\nbeen found to plateau at around 6 layers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\nnum_layers = 4\nvar_init = 0.05 * np.random.randn(num_layers, 5)\nprint(var_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n   array([[ 0.08820262,  0.02000786,  0.0489369 ,  0.11204466,  0.0933779 ],\n          [-0.04886389,  0.04750442, -0.00756786, -0.00516094,  0.02052993],\n          [ 0.00720218,  0.07271368,  0.03805189,  0.00608375,  0.02219316],\n          [ 0.01668372,  0.07470395, -0.01025791,  0.01565339, -0.04270479]])\n\nUsing the Adam optimizer, we update the weights for 500 steps (this\ntakes some time). More steps will lead to a better fit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = AdamOptimizer(0.01, beta1=0.9, beta2=0.999)\n\nvar = var_init\nfor it in range(500):\n    var = opt.step(lambda v: cost(v, X, Y), var)\n    print(\"Iter: {:5d} | Cost: {:0.7f} \".format(it + 1, cost(var, X, Y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. rst-class:: sphx-glr-script-out\n\n Out:\n\n .. code-block:: none\n\n   Iter:     1 | Cost: 0.2689702\n   Iter:     2 | Cost: 0.2472125\n   Iter:     3 | Cost: 0.2300139\n   Iter:     4 | Cost: 0.2157100\n   Iter:     5 | Cost: 0.2035455\n   Iter:     6 | Cost: 0.1931103\n   Iter:     7 | Cost: 0.1841536\n   Iter:     8 | Cost: 0.1765061\n   Iter:     9 | Cost: 0.1700410\n   Iter:    10 | Cost: 0.1646527\n   Iter:    11 | Cost: 0.1602444\n   Iter:    12 | Cost: 0.1567201\n   Iter:    13 | Cost: 0.1539806\n   Iter:    14 | Cost: 0.1519220\n   Iter:    15 | Cost: 0.1504356\n   Iter:    16 | Cost: 0.1494099\n   Iter:    17 | Cost: 0.1487330\n   Iter:    18 | Cost: 0.1482962\n   Iter:    19 | Cost: 0.1479980\n   Iter:    20 | Cost: 0.1477470\n   Iter:    21 | Cost: 0.1474655\n   Iter:    22 | Cost: 0.1470914\n   Iter:    23 | Cost: 0.1465799\n   Iter:    24 | Cost: 0.1459034\n   Iter:    25 | Cost: 0.1450506\n   Iter:    26 | Cost: 0.1440251\n   Iter:    27 | Cost: 0.1428427\n   Iter:    28 | Cost: 0.1415282\n   Iter:    29 | Cost: 0.1401125\n   Iter:    30 | Cost: 0.1386296\n   Iter:    31 | Cost: 0.1371132\n   Iter:    32 | Cost: 0.1355946\n   Iter:    33 | Cost: 0.1341006\n   Iter:    34 | Cost: 0.1326526\n   Iter:    35 | Cost: 0.1312654\n   Iter:    36 | Cost: 0.1299478\n   Iter:    37 | Cost: 0.1287022\n   Iter:    38 | Cost: 0.1275259\n   Iter:    39 | Cost: 0.1264120\n   Iter:    40 | Cost: 0.1253502\n   Iter:    41 | Cost: 0.1243284\n   Iter:    42 | Cost: 0.1233333\n   Iter:    43 | Cost: 0.1223521\n   Iter:    44 | Cost: 0.1213726\n   Iter:    45 | Cost: 0.1203843\n   Iter:    46 | Cost: 0.1193790\n   Iter:    47 | Cost: 0.1183506\n   Iter:    48 | Cost: 0.1172959\n   Iter:    49 | Cost: 0.1162138\n   Iter:    50 | Cost: 0.1151057\n   Iter:    51 | Cost: 0.1139748\n   Iter:    52 | Cost: 0.1128259\n   Iter:    53 | Cost: 0.1116647\n   Iter:    54 | Cost: 0.1104972\n   Iter:    55 | Cost: 0.1093295\n   Iter:    56 | Cost: 0.1081673\n   Iter:    57 | Cost: 0.1070151\n   Iter:    58 | Cost: 0.1058764\n   Iter:    59 | Cost: 0.1047533\n   Iter:    60 | Cost: 0.1036464\n   Iter:    61 | Cost: 0.1025554\n   Iter:    62 | Cost: 0.1014787\n   Iter:    63 | Cost: 0.1004141\n   Iter:    64 | Cost: 0.0993591\n   Iter:    65 | Cost: 0.0983111\n   Iter:    66 | Cost: 0.0972679\n   Iter:    67 | Cost: 0.0962278\n   Iter:    68 | Cost: 0.0951897\n   Iter:    69 | Cost: 0.0941534\n   Iter:    70 | Cost: 0.0931195\n   Iter:    71 | Cost: 0.0920891\n   Iter:    72 | Cost: 0.0910638\n   Iter:    73 | Cost: 0.0900453\n   Iter:    74 | Cost: 0.0890357\n   Iter:    75 | Cost: 0.0880366\n   Iter:    76 | Cost: 0.0870493\n   Iter:    77 | Cost: 0.0860751\n   Iter:    78 | Cost: 0.0851144\n   Iter:    79 | Cost: 0.0841675\n   Iter:    80 | Cost: 0.0832342\n   Iter:    81 | Cost: 0.0823143\n   Iter:    82 | Cost: 0.0814072\n   Iter:    83 | Cost: 0.0805125\n   Iter:    84 | Cost: 0.0796296\n   Iter:    85 | Cost: 0.0787583\n   Iter:    86 | Cost: 0.0778983\n   Iter:    87 | Cost: 0.0770497\n   Iter:    88 | Cost: 0.0762127\n   Iter:    89 | Cost: 0.0753874\n   Iter:    90 | Cost: 0.0745742\n   Iter:    91 | Cost: 0.0737733\n   Iter:    92 | Cost: 0.0729849\n   Iter:    93 | Cost: 0.0722092\n   Iter:    94 | Cost: 0.0714462\n   Iter:    95 | Cost: 0.0706958\n   Iter:    96 | Cost: 0.0699578\n   Iter:    97 | Cost: 0.0692319\n   Iter:    98 | Cost: 0.0685177\n   Iter:    99 | Cost: 0.0678151\n   Iter:   100 | Cost: 0.0671236\n   Iter:   101 | Cost: 0.0664430\n   Iter:   102 | Cost: 0.0657732\n   Iter:   103 | Cost: 0.0651139\n   Iter:   104 | Cost: 0.0644650\n   Iter:   105 | Cost: 0.0638264\n   Iter:   106 | Cost: 0.0631981\n   Iter:   107 | Cost: 0.0625800\n   Iter:   108 | Cost: 0.0619719\n   Iter:   109 | Cost: 0.0613737\n   Iter:   110 | Cost: 0.0607853\n   Iter:   111 | Cost: 0.0602064\n   Iter:   112 | Cost: 0.0596368\n   Iter:   113 | Cost: 0.0590764\n   Iter:   114 | Cost: 0.0585249\n   Iter:   115 | Cost: 0.0579820\n   Iter:   116 | Cost: 0.0574476\n   Iter:   117 | Cost: 0.0569214\n   Iter:   118 | Cost: 0.0564033\n   Iter:   119 | Cost: 0.0558932\n   Iter:   120 | Cost: 0.0553908\n   Iter:   121 | Cost: 0.0548960\n   Iter:   122 | Cost: 0.0544086\n   Iter:   123 | Cost: 0.0539286\n   Iter:   124 | Cost: 0.0534557\n   Iter:   125 | Cost: 0.0529897\n   Iter:   126 | Cost: 0.0525306\n   Iter:   127 | Cost: 0.0520781\n   Iter:   128 | Cost: 0.0516320\n   Iter:   129 | Cost: 0.0511923\n   Iter:   130 | Cost: 0.0507587\n   Iter:   131 | Cost: 0.0503311\n   Iter:   132 | Cost: 0.0499094\n   Iter:   133 | Cost: 0.0494934\n   Iter:   134 | Cost: 0.0490830\n   Iter:   135 | Cost: 0.0486781\n   Iter:   136 | Cost: 0.0482785\n   Iter:   137 | Cost: 0.0478842\n   Iter:   138 | Cost: 0.0474949\n   Iter:   139 | Cost: 0.0471107\n   Iter:   140 | Cost: 0.0467313\n   Iter:   141 | Cost: 0.0463567\n   Iter:   142 | Cost: 0.0459868\n   Iter:   143 | Cost: 0.0456214\n   Iter:   144 | Cost: 0.0452604\n   Iter:   145 | Cost: 0.0449038\n   Iter:   146 | Cost: 0.0445514\n   Iter:   147 | Cost: 0.0442032\n   Iter:   148 | Cost: 0.0438590\n   Iter:   149 | Cost: 0.0435188\n   Iter:   150 | Cost: 0.0431825\n   Iter:   151 | Cost: 0.0428499\n   Iter:   152 | Cost: 0.0425211\n   Iter:   153 | Cost: 0.0421960\n   Iter:   154 | Cost: 0.0418744\n   Iter:   155 | Cost: 0.0415563\n   Iter:   156 | Cost: 0.0412416\n   Iter:   157 | Cost: 0.0409302\n   Iter:   158 | Cost: 0.0406222\n   Iter:   159 | Cost: 0.0403173\n   Iter:   160 | Cost: 0.0400156\n   Iter:   161 | Cost: 0.0397169\n   Iter:   162 | Cost: 0.0394213\n   Iter:   163 | Cost: 0.0391286\n   Iter:   164 | Cost: 0.0388389\n   Iter:   165 | Cost: 0.0385520\n   Iter:   166 | Cost: 0.0382679\n   Iter:   167 | Cost: 0.0379866\n   Iter:   168 | Cost: 0.0377079\n   Iter:   169 | Cost: 0.0374319\n   Iter:   170 | Cost: 0.0371585\n   Iter:   171 | Cost: 0.0368877\n   Iter:   172 | Cost: 0.0366194\n   Iter:   173 | Cost: 0.0363535\n   Iter:   174 | Cost: 0.0360901\n   Iter:   175 | Cost: 0.0358291\n   Iter:   176 | Cost: 0.0355704\n   Iter:   177 | Cost: 0.0353140\n   Iter:   178 | Cost: 0.0350599\n   Iter:   179 | Cost: 0.0348081\n   Iter:   180 | Cost: 0.0345585\n   Iter:   181 | Cost: 0.0343110\n   Iter:   182 | Cost: 0.0340658\n   Iter:   183 | Cost: 0.0338226\n   Iter:   184 | Cost: 0.0335815\n   Iter:   185 | Cost: 0.0333425\n   Iter:   186 | Cost: 0.0331056\n   Iter:   187 | Cost: 0.0328706\n   Iter:   188 | Cost: 0.0326377\n   Iter:   189 | Cost: 0.0324067\n   Iter:   190 | Cost: 0.0321777\n   Iter:   191 | Cost: 0.0319506\n   Iter:   192 | Cost: 0.0317255\n   Iter:   193 | Cost: 0.0315022\n   Iter:   194 | Cost: 0.0312808\n   Iter:   195 | Cost: 0.0310613\n   Iter:   196 | Cost: 0.0308436\n   Iter:   197 | Cost: 0.0306278\n   Iter:   198 | Cost: 0.0304138\n   Iter:   199 | Cost: 0.0302016\n   Iter:   200 | Cost: 0.0299912\n   Iter:   201 | Cost: 0.0297826\n   Iter:   202 | Cost: 0.0295757\n   Iter:   203 | Cost: 0.0293707\n   Iter:   204 | Cost: 0.0291674\n   Iter:   205 | Cost: 0.0289659\n   Iter:   206 | Cost: 0.0287661\n   Iter:   207 | Cost: 0.0285681\n   Iter:   208 | Cost: 0.0283718\n   Iter:   209 | Cost: 0.0281772\n   Iter:   210 | Cost: 0.0279844\n   Iter:   211 | Cost: 0.0277933\n   Iter:   212 | Cost: 0.0276039\n   Iter:   213 | Cost: 0.0274163\n   Iter:   214 | Cost: 0.0272304\n   Iter:   215 | Cost: 0.0270461\n   Iter:   216 | Cost: 0.0268636\n   Iter:   217 | Cost: 0.0266829\n   Iter:   218 | Cost: 0.0265038\n   Iter:   219 | Cost: 0.0263264\n   Iter:   220 | Cost: 0.0261508\n   Iter:   221 | Cost: 0.0259768\n   Iter:   222 | Cost: 0.0258046\n   Iter:   223 | Cost: 0.0256341\n   Iter:   224 | Cost: 0.0254652\n   Iter:   225 | Cost: 0.0252981\n   Iter:   226 | Cost: 0.0251327\n   Iter:   227 | Cost: 0.0249690\n   Iter:   228 | Cost: 0.0248070\n   Iter:   229 | Cost: 0.0246467\n   Iter:   230 | Cost: 0.0244881\n   Iter:   231 | Cost: 0.0243312\n   Iter:   232 | Cost: 0.0241760\n   Iter:   233 | Cost: 0.0240225\n   Iter:   234 | Cost: 0.0238707\n   Iter:   235 | Cost: 0.0237206\n   Iter:   236 | Cost: 0.0235721\n   Iter:   237 | Cost: 0.0234254\n   Iter:   238 | Cost: 0.0232803\n   Iter:   239 | Cost: 0.0231369\n   Iter:   240 | Cost: 0.0229952\n   Iter:   241 | Cost: 0.0228552\n   Iter:   242 | Cost: 0.0227168\n   Iter:   243 | Cost: 0.0225801\n   Iter:   244 | Cost: 0.0224450\n   Iter:   245 | Cost: 0.0223116\n   Iter:   246 | Cost: 0.0221798\n   Iter:   247 | Cost: 0.0220496\n   Iter:   248 | Cost: 0.0219211\n   Iter:   249 | Cost: 0.0217942\n   Iter:   250 | Cost: 0.0216688\n   Iter:   251 | Cost: 0.0215451\n   Iter:   252 | Cost: 0.0214230\n   Iter:   253 | Cost: 0.0213024\n   Iter:   254 | Cost: 0.0211835\n   Iter:   255 | Cost: 0.0210660\n   Iter:   256 | Cost: 0.0209502\n   Iter:   257 | Cost: 0.0208358\n   Iter:   258 | Cost: 0.0207230\n   Iter:   259 | Cost: 0.0206117\n   Iter:   260 | Cost: 0.0205019\n   Iter:   261 | Cost: 0.0203936\n   Iter:   262 | Cost: 0.0202867\n   Iter:   263 | Cost: 0.0201813\n   Iter:   264 | Cost: 0.0200773\n   Iter:   265 | Cost: 0.0199748\n   Iter:   266 | Cost: 0.0198737\n   Iter:   267 | Cost: 0.0197740\n   Iter:   268 | Cost: 0.0196757\n   Iter:   269 | Cost: 0.0195787\n   Iter:   270 | Cost: 0.0194831\n   Iter:   271 | Cost: 0.0193889\n   Iter:   272 | Cost: 0.0192959\n   Iter:   273 | Cost: 0.0192043\n   Iter:   274 | Cost: 0.0191140\n   Iter:   275 | Cost: 0.0190249\n   Iter:   276 | Cost: 0.0189371\n   Iter:   277 | Cost: 0.0188505\n   Iter:   278 | Cost: 0.0187651\n   Iter:   279 | Cost: 0.0186810\n   Iter:   280 | Cost: 0.0185980\n   Iter:   281 | Cost: 0.0185163\n   Iter:   282 | Cost: 0.0184356\n   Iter:   283 | Cost: 0.0183561\n   Iter:   284 | Cost: 0.0182777\n   Iter:   285 | Cost: 0.0182004\n   Iter:   286 | Cost: 0.0181242\n   Iter:   287 | Cost: 0.0180491\n   Iter:   288 | Cost: 0.0179750\n   Iter:   289 | Cost: 0.0179020\n   Iter:   290 | Cost: 0.0178299\n   Iter:   291 | Cost: 0.0177589\n   Iter:   292 | Cost: 0.0176888\n   Iter:   293 | Cost: 0.0176197\n   Iter:   294 | Cost: 0.0175515\n   Iter:   295 | Cost: 0.0174843\n   Iter:   296 | Cost: 0.0174180\n   Iter:   297 | Cost: 0.0173525\n   Iter:   298 | Cost: 0.0172880\n   Iter:   299 | Cost: 0.0172243\n   Iter:   300 | Cost: 0.0171614\n   Iter:   301 | Cost: 0.0170994\n   Iter:   302 | Cost: 0.0170382\n   Iter:   303 | Cost: 0.0169777\n   Iter:   304 | Cost: 0.0169181\n   Iter:   305 | Cost: 0.0168592\n   Iter:   306 | Cost: 0.0168010\n   Iter:   307 | Cost: 0.0167436\n   Iter:   308 | Cost: 0.0166869\n   Iter:   309 | Cost: 0.0166309\n   Iter:   310 | Cost: 0.0165756\n   Iter:   311 | Cost: 0.0165209\n   Iter:   312 | Cost: 0.0164669\n   Iter:   313 | Cost: 0.0164136\n   Iter:   314 | Cost: 0.0163608\n   Iter:   315 | Cost: 0.0163087\n   Iter:   316 | Cost: 0.0162572\n   Iter:   317 | Cost: 0.0162063\n   Iter:   318 | Cost: 0.0161559\n   Iter:   319 | Cost: 0.0161061\n   Iter:   320 | Cost: 0.0160568\n   Iter:   321 | Cost: 0.0160080\n   Iter:   322 | Cost: 0.0159598\n   Iter:   323 | Cost: 0.0159121\n   Iter:   324 | Cost: 0.0158649\n   Iter:   325 | Cost: 0.0158181\n   Iter:   326 | Cost: 0.0157719\n   Iter:   327 | Cost: 0.0157260\n   Iter:   328 | Cost: 0.0156807\n   Iter:   329 | Cost: 0.0156357\n   Iter:   330 | Cost: 0.0155912\n   Iter:   331 | Cost: 0.0155471\n   Iter:   332 | Cost: 0.0155034\n   Iter:   333 | Cost: 0.0154601\n   Iter:   334 | Cost: 0.0154172\n   Iter:   335 | Cost: 0.0153747\n   Iter:   336 | Cost: 0.0153325\n   Iter:   337 | Cost: 0.0152907\n   Iter:   338 | Cost: 0.0152492\n   Iter:   339 | Cost: 0.0152081\n   Iter:   340 | Cost: 0.0151673\n   Iter:   341 | Cost: 0.0151269\n   Iter:   342 | Cost: 0.0150867\n   Iter:   343 | Cost: 0.0150469\n   Iter:   344 | Cost: 0.0150073\n   Iter:   345 | Cost: 0.0149681\n   Iter:   346 | Cost: 0.0149291\n   Iter:   347 | Cost: 0.0148905\n   Iter:   348 | Cost: 0.0148521\n   Iter:   349 | Cost: 0.0148140\n   Iter:   350 | Cost: 0.0147761\n   Iter:   351 | Cost: 0.0147385\n   Iter:   352 | Cost: 0.0147012\n   Iter:   353 | Cost: 0.0146641\n   Iter:   354 | Cost: 0.0146273\n   Iter:   355 | Cost: 0.0145907\n   Iter:   356 | Cost: 0.0145543\n   Iter:   357 | Cost: 0.0145182\n   Iter:   358 | Cost: 0.0144824\n   Iter:   359 | Cost: 0.0144467\n   Iter:   360 | Cost: 0.0144113\n   Iter:   361 | Cost: 0.0143762\n   Iter:   362 | Cost: 0.0143412\n   Iter:   363 | Cost: 0.0143065\n   Iter:   364 | Cost: 0.0142720\n   Iter:   365 | Cost: 0.0142378\n   Iter:   366 | Cost: 0.0142037\n   Iter:   367 | Cost: 0.0141699\n   Iter:   368 | Cost: 0.0141363\n   Iter:   369 | Cost: 0.0141030\n   Iter:   370 | Cost: 0.0140699\n   Iter:   371 | Cost: 0.0140370\n   Iter:   372 | Cost: 0.0140043\n   Iter:   373 | Cost: 0.0139719\n   Iter:   374 | Cost: 0.0139397\n   Iter:   375 | Cost: 0.0139077\n   Iter:   376 | Cost: 0.0138760\n   Iter:   377 | Cost: 0.0138445\n   Iter:   378 | Cost: 0.0138132\n   Iter:   379 | Cost: 0.0137822\n   Iter:   380 | Cost: 0.0137515\n   Iter:   381 | Cost: 0.0137210\n   Iter:   382 | Cost: 0.0136907\n   Iter:   383 | Cost: 0.0136607\n   Iter:   384 | Cost: 0.0136310\n   Iter:   385 | Cost: 0.0136015\n   Iter:   386 | Cost: 0.0135723\n   Iter:   387 | Cost: 0.0135433\n   Iter:   388 | Cost: 0.0135146\n   Iter:   389 | Cost: 0.0134863\n   Iter:   390 | Cost: 0.0134581\n   Iter:   391 | Cost: 0.0134303\n   Iter:   392 | Cost: 0.0134027\n   Iter:   393 | Cost: 0.0133755\n   Iter:   394 | Cost: 0.0133485\n   Iter:   395 | Cost: 0.0133218\n   Iter:   396 | Cost: 0.0132954\n   Iter:   397 | Cost: 0.0132694\n   Iter:   398 | Cost: 0.0132436\n   Iter:   399 | Cost: 0.0132181\n   Iter:   400 | Cost: 0.0131929\n   Iter:   401 | Cost: 0.0131681\n   Iter:   402 | Cost: 0.0131435\n   Iter:   403 | Cost: 0.0131193\n   Iter:   404 | Cost: 0.0130953\n   Iter:   405 | Cost: 0.0130717\n   Iter:   406 | Cost: 0.0130484\n   Iter:   407 | Cost: 0.0130254\n   Iter:   408 | Cost: 0.0130028\n   Iter:   409 | Cost: 0.0129804\n   Iter:   410 | Cost: 0.0129584\n   Iter:   411 | Cost: 0.0129367\n   Iter:   412 | Cost: 0.0129153\n   Iter:   413 | Cost: 0.0128942\n   Iter:   414 | Cost: 0.0128735\n   Iter:   415 | Cost: 0.0128530\n   Iter:   416 | Cost: 0.0128329\n   Iter:   417 | Cost: 0.0128131\n   Iter:   418 | Cost: 0.0127935\n   Iter:   419 | Cost: 0.0127743\n   Iter:   420 | Cost: 0.0127554\n   Iter:   421 | Cost: 0.0127368\n   Iter:   422 | Cost: 0.0127185\n   Iter:   423 | Cost: 0.0127006\n   Iter:   424 | Cost: 0.0126829\n   Iter:   425 | Cost: 0.0126655\n   Iter:   426 | Cost: 0.0126483\n   Iter:   427 | Cost: 0.0126315\n   Iter:   428 | Cost: 0.0126150\n   Iter:   429 | Cost: 0.0125987\n   Iter:   430 | Cost: 0.0125827\n   Iter:   431 | Cost: 0.0125670\n   Iter:   432 | Cost: 0.0125516\n   Iter:   433 | Cost: 0.0125364\n   Iter:   434 | Cost: 0.0125215\n   Iter:   435 | Cost: 0.0125068\n   Iter:   436 | Cost: 0.0124924\n   Iter:   437 | Cost: 0.0124782\n   Iter:   438 | Cost: 0.0124643\n   Iter:   439 | Cost: 0.0124507\n   Iter:   440 | Cost: 0.0124372\n   Iter:   441 | Cost: 0.0124240\n   Iter:   442 | Cost: 0.0124110\n   Iter:   443 | Cost: 0.0123983\n   Iter:   444 | Cost: 0.0123857\n   Iter:   445 | Cost: 0.0123734\n   Iter:   446 | Cost: 0.0123613\n   Iter:   447 | Cost: 0.0123494\n   Iter:   448 | Cost: 0.0123377\n   Iter:   449 | Cost: 0.0123262\n   Iter:   450 | Cost: 0.0123149\n   Iter:   451 | Cost: 0.0123038\n   Iter:   452 | Cost: 0.0122929\n   Iter:   453 | Cost: 0.0122821\n   Iter:   454 | Cost: 0.0122715\n   Iter:   455 | Cost: 0.0122611\n   Iter:   456 | Cost: 0.0122509\n   Iter:   457 | Cost: 0.0122409\n   Iter:   458 | Cost: 0.0122310\n   Iter:   459 | Cost: 0.0122212\n   Iter:   460 | Cost: 0.0122116\n   Iter:   461 | Cost: 0.0122022\n   Iter:   462 | Cost: 0.0121929\n   Iter:   463 | Cost: 0.0121838\n   Iter:   464 | Cost: 0.0121748\n   Iter:   465 | Cost: 0.0121660\n   Iter:   466 | Cost: 0.0121572\n   Iter:   467 | Cost: 0.0121487\n   Iter:   468 | Cost: 0.0121402\n   Iter:   469 | Cost: 0.0121319\n   Iter:   470 | Cost: 0.0121237\n   Iter:   471 | Cost: 0.0121156\n   Iter:   472 | Cost: 0.0121076\n   Iter:   473 | Cost: 0.0120998\n   Iter:   474 | Cost: 0.0120921\n   Iter:   475 | Cost: 0.0120844\n   Iter:   476 | Cost: 0.0120769\n   Iter:   477 | Cost: 0.0120695\n   Iter:   478 | Cost: 0.0120622\n   Iter:   479 | Cost: 0.0120550\n   Iter:   480 | Cost: 0.0120479\n   Iter:   481 | Cost: 0.0120409\n   Iter:   482 | Cost: 0.0120340\n   Iter:   483 | Cost: 0.0120272\n   Iter:   484 | Cost: 0.0120205\n   Iter:   485 | Cost: 0.0120138\n   Iter:   486 | Cost: 0.0120073\n   Iter:   487 | Cost: 0.0120008\n   Iter:   488 | Cost: 0.0119944\n   Iter:   489 | Cost: 0.0119881\n   Iter:   490 | Cost: 0.0119819\n   Iter:   491 | Cost: 0.0119758\n   Iter:   492 | Cost: 0.0119697\n   Iter:   493 | Cost: 0.0119637\n   Iter:   494 | Cost: 0.0119578\n   Iter:   495 | Cost: 0.0119520\n   Iter:   496 | Cost: 0.0119462\n   Iter:   497 | Cost: 0.0119405\n   Iter:   498 | Cost: 0.0119349\n   Iter:   499 | Cost: 0.0119293\n   Iter:   500 | Cost: 0.0119238\n\n\nFinally, we collect the predictions of the trained model for 50 values\nin the range $[-1,1]$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_pred = np.linspace(-1, 1, 50)\npredictions = [quantum_neural_net(var, x=x_) for x_ in x_pred]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and plot the shape of the function that the model has \u201clearned\u201d from\nthe noisy data (green dots).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nplt.scatter(X, Y)\nplt.scatter(x_pred, predictions, color=\"green\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.tick_params(axis=\"both\", which=\"major\")\nplt.tick_params(axis=\"both\", which=\"minor\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../../examples/figures/qnn_output_28_0.png)\n\n\nThe model has learned to smooth the noisy data.\n\nIn fact, we can use PennyLane to look at typical functions that the\nmodel produces without being trained at all. The shape of these\nfunctions varies significantly with the variance hyperparameter for the\nweight initialization.\n\nSetting this hyperparameter to a small value produces almost linear\nfunctions, since all quantum gates in the variational circuit\napproximately perform the identity transformation in that case. Larger\nvalues produce smoothly oscillating functions with a period that depends\non the number of layers used (generically, the more layers, the smaller\nthe period).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "variance = 1.0\n\nplt.figure()\nx_pred = np.linspace(-2, 2, 50)\nfor i in range(7):\n    rnd_var = variance * np.random.randn(num_layers, 7)\n    predictions = [quantum_neural_net(rnd_var, x=x_) for x_ in x_pred]\n    plt.plot(x_pred, predictions, color=\"black\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.tick_params(axis=\"both\", which=\"major\")\nplt.tick_params(axis=\"both\", which=\"minor\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../../examples/figures/qnn_output_30_0.png)\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}