{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nVariational classifier\n======================\n\nIn this tutorial, we show how to use PennyLane to implement variational\nquantum classifiers - quantum circuits that can be trained from labelled\ndata to classify new data samples. The architecture is inspired by\n`Farhi and Neven (2018) <https://arxiv.org/abs/1802.06002>`__ as well as\n`Schuld et al. (2018) <https://arxiv.org/abs/1804.00633>`__.\n\nWe will first show that the variational quantum classifier can reproduce\nthe parity function\n\n\\begin{align}f: x \\in \\{0,1\\}^{\\otimes n} \\rightarrow y =\n    \\begin{cases} 1 \\text{  if uneven number of ones in } x \\\\ 0\n    \\text{ otherwise} \\end{cases}.\\end{align}\n\nThis optimization example demonstrates how to encode binary inputs into\nthe initial state of the variational circuit, which is simply a\ncomputational basis state.\n\nWe then show how to encode real vectors as amplitude vectors (*amplitude\nencoding*) and train the model to recognize the first two classes of\nflowers in the Iris dataset.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Fitting the parity function\n------------------------------\n\nImports\n~~~~~~~\n\nAs before, we import PennyLane, the PennyLane-provided version of NumPy,\nand an optimizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pennylane as qml\nfrom pennylane import numpy as np\nfrom pennylane.optimize import NesterovMomentumOptimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantum and classical nodes\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe create a quantum device with four \u201cwires\u201d (or qubits).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variational classifiers usually define a \u201clayer\u201d or \u201cblock\u201d, which is an\nelementary circuit architecture that gets repeated to build the\nvariational circuit.\n\nOur circuit layer consists of an arbitrary rotation on every qubit, as\nwell as CNOTs that entangle each qubit with its neighbour.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(W):\n\n    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n    qml.Rot(W[2, 0], W[2, 1], W[2, 2], wires=2)\n    qml.Rot(W[3, 0], W[3, 1], W[3, 2], wires=3)\n\n    qml.CNOT(wires=[0, 1])\n    qml.CNOT(wires=[1, 2])\n    qml.CNOT(wires=[2, 3])\n    qml.CNOT(wires=[3, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need a way to encode data inputs $x$ into the circuit, so\nthat the measured output depends on the inputs. In this first example,\nthe inputs are bitstrings, which we encode into the state of the qubits.\nThe quantum state $\\psi$ after\nstate preparation is a computational basis state that has 1s where\n$x$ has 1s, for example\n\n\\begin{align}x = 0101 \\rightarrow |\\psi \\rangle = |0101 \\rangle .\\end{align}\n\nWe use the ``BasisState`` function provided by PennyLane, which expects\n``x`` to be a list of zeros and ones, i.e. ``[0,1,0,1]``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def statepreparation(x):\n    qml.BasisState(x, wires=[0, 1, 2, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we define the quantum node as a state preparation routine, followed\nby a repetition of the layer structure. Borrowing from machine learning,\nwe call the parameters ``weights``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef circuit(weights, x=None):\n\n    statepreparation(x)\n\n    for W in weights:\n        layer(W)\n\n    return qml.expval(qml.PauliZ(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Different from previous examples, the quantum node takes the data as a\nkeyword argument ``x`` (with the default value ``None``). Keyword\narguments of a quantum node are considered as fixed when calculating a\ngradient; they are never trained.\n\nIf we want to add a \u201cclassical\u201d bias parameter, the variational quantum\nclassifer also needs some post-processing. We define the final model by\na classical node that uses the first variable, and feeds the remainder\ninto the quantum node. Before this, we reshape the list of remaining\nvariables for easy use in the quantum node.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def variational_classifier(var, x=None):\n    weights = var[0]\n    bias = var[1]\n    return circuit(weights, x=x) + bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cost\n~~~~\n\nIn supervised learning, the cost function is usually the sum of a loss\nfunction and a regularizer. We use the standard square loss that\nmeasures the distance between target labels and model predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def square_loss(labels, predictions):\n    loss = 0\n    for l, p in zip(labels, predictions):\n        loss = loss + (l - p) ** 2\n\n    loss = loss / len(labels)\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To monitor how many inputs the current classifier predicted correctly,\nwe also define the accuracy given target labels and model predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def accuracy(labels, predictions):\n\n    loss = 0\n    for l, p in zip(labels, predictions):\n        if abs(l - p) < 1e-5:\n            loss = loss + 1\n    loss = loss / len(labels)\n\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For learning tasks, the cost depends on the data - here the features and\nlabels considered in the iteration of the optimization routine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def cost(var, X, Y):\n    predictions = [variational_classifier(var, x=x) for x in X]\n    return square_loss(Y, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n~~~~~~~~~~~~\n\nLet\u2019s now load and preprocess some data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"data/parity.txt\")\nX = data[:, :-1]\nY = data[:, -1]\nY = Y * 2 - np.ones(len(Y))  # shift label from {0, 1} to {-1, 1}\n\nfor i in range(5):\n    print(\"X = {}, Y = {: d}\".format(X[i], int(Y[i])))\n\nprint(\"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We initialize the variables randomly (but fix a seed for\nreproducability). The first variable in the list is used as a bias,\nwhile the rest is fed into the gates of the variational circuit.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\nnum_qubits = 4\nnum_layers = 2\nvar_init = (0.01 * np.random.randn(num_layers, num_qubits, 3), 0.0)\n\nprint(var_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we create an optimizer and choose a batch size\u2026\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = NesterovMomentumOptimizer(0.5)\nbatch_size = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\u2026and train the optimizer. We track the accuracy - the share of correctly\nclassified data samples. For this we compute the outputs of the\nvariational classifier and turn them into predictions in\n$\\{-1,1\\}$ by taking the sign of the output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "var = var_init\nfor it in range(25):\n\n    # Update the weights by one optimizer step\n    batch_index = np.random.randint(0, len(X), (batch_size,))\n    X_batch = X[batch_index]\n    Y_batch = Y[batch_index]\n    var = opt.step(lambda v: cost(v, X_batch, Y_batch), var)\n\n    # Compute accuracy\n    predictions = [np.sign(variational_classifier(var, x=x)) for x in X]\n    acc = accuracy(Y, predictions)\n\n    print(\"Iter: {:5d} | Cost: {:0.7f} | Accuracy: {:0.7f} \".format(it + 1, cost(var, X, Y), acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Iris classification\n----------------------\n\nQuantum and classical nodes\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo encode real-valued vectors into the amplitudes of a quantum state, we\nuse a 2-qubit simulator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "State preparation is not as simple as when we represent a bitstring with\na basis state. Every input x has to be translated into a set of angles\nwhich can get fed into a small routine for state preparation. To\nsimplify things a bit, we will work with data from the positive\nsubspace, so that we can ignore signs (which would require another\ncascade of rotations around the z axis).\n\nThe circuit is coded according to the scheme in `M\u00f6tt\u00f6nen, et al.\n(2004) <https://arxiv.org/abs/quant-ph/0407010>`__, or\u2014as presented\nfor positive vectors only\u2014in `Schuld and Petruccione\n(2018) <https://link.springer.com/book/10.1007/978-3-319-96424-9>`__. We\nhad to also decompose controlled Y-axis rotations into more basic\ncircuits following `Nielsen and Chuang\n(2010) <http://www.michaelnielsen.org/qcqi/>`__.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_angles(x):\n\n    beta0 = 2 * np.arcsin(np.sqrt(x[1]) ** 2 / np.sqrt(x[0] ** 2 + x[1] ** 2 + 1e-12))\n    beta1 = 2 * np.arcsin(np.sqrt(x[3]) ** 2 / np.sqrt(x[2] ** 2 + x[3] ** 2 + 1e-12))\n    beta2 = 2 * np.arcsin(\n        np.sqrt(x[2] ** 2 + x[3] ** 2) / np.sqrt(x[0] ** 2 + x[1] ** 2 + x[2] ** 2 + x[3] ** 2)\n    )\n\n    return np.array([beta2, -beta1 / 2, beta1 / 2, -beta0 / 2, beta0 / 2])\n\n\ndef statepreparation(a):\n    qml.RY(a[0], wires=0)\n\n    qml.CNOT(wires=[0, 1])\n    qml.RY(a[1], wires=1)\n    qml.CNOT(wires=[0, 1])\n    qml.RY(a[2], wires=1)\n\n    qml.PauliX(wires=0)\n    qml.CNOT(wires=[0, 1])\n    qml.RY(a[3], wires=1)\n    qml.CNOT(wires=[0, 1])\n    qml.RY(a[4], wires=1)\n    qml.PauliX(wires=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u2019s test if this routine actually works.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = np.array([0.53896774, 0.79503606, 0.27826503, 0.0])\nang = get_angles(x)\n\n\n@qml.qnode(dev)\ndef test(angles=None):\n\n    statepreparation(angles)\n\n    return qml.expval(qml.PauliZ(0))\n\n\ntest(angles=ang)\n\nprint(\"x               : \", x)\nprint(\"angles          : \", ang)\nprint(\"amplitude vector: \", np.real(dev._state))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the ``default.qubit`` simulator provides a shortcut to\n``statepreparation`` with the command\n``qml.QubitStateVector(x, wires=[0, 1])``. However, some devices may not\nsupport an arbitrary state-preparation routine.\n\nSince we are working with only 2 qubits now, we need to update the layer\nfunction as well.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def layer(W):\n    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n    qml.Rot(W[1, 0], W[1, 1], W[1, 2], wires=1)\n    qml.CNOT(wires=[0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variational classifier model and its cost remain essentially the\nsame, but we have to reload them with the new state preparation and\nlayer functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev)\ndef circuit(weights, angles=None):\n    statepreparation(angles)\n\n    for W in weights:\n        layer(W)\n\n    return qml.expval(qml.PauliZ(0))\n\n\ndef variational_classifier(var, angles=None):\n    weights = var[0]\n    bias = var[1]\n    return circuit(weights, angles=angles) + bias\n\n\ndef cost(weights, features, labels):\n    predictions = [variational_classifier(weights, angles=f) for f in features]\n    return square_loss(labels, predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data\n~~~~\n\nWe then load the Iris data set. There is a bit of preprocessing to do in\norder to encode the inputs into the amplitudes of a quantum state. In\nthe last preprocessing step, we translate the inputs x to rotation\nangles using the ``get_angles`` function we defined above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = np.loadtxt(\"data/iris_classes1and2_scaled.txt\")\nX = data[:, 0:2]\nprint(\"First X sample (original)  :\", X[0])\n\n# pad the vectors to size 2^2 with constant values\npadding = 0.3 * np.ones((len(X), 1))\nX_pad = np.c_[np.c_[X, padding], np.zeros((len(X), 1))]\nprint(\"First X sample (padded)    :\", X_pad[0])\n\n# normalize each input\nnormalization = np.sqrt(np.sum(X_pad ** 2, -1))\nX_norm = (X_pad.T / normalization).T\nprint(\"First X sample (normalized):\", X_norm[0])\n\n# angles for state preparation are new features\nfeatures = np.array([get_angles(x) for x in X_norm])\nprint(\"First features sample      :\", features[0])\n\nY = data[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These angles are our new features, which is why we have renamed X to\n\u201cfeatures\u201d above. Let\u2019s plot the stages of preprocessing and play around\nwith the dimensions (dim1, dim2). Some of them still separate the\nclasses well, while others are less informative.\n\n*Note: To run the following code you need the matplotlib library.*\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure()\nplt.scatter(X[:, 0][Y == 1], X[:, 1][Y == 1], c=\"r\", marker=\"o\", edgecolors=\"k\")\nplt.scatter(X[:, 0][Y == -1], X[:, 1][Y == -1], c=\"b\", marker=\"o\", edgecolors=\"k\")\nplt.title(\"Original data\")\nplt.show()\n\nplt.figure()\ndim1 = 0\ndim2 = 1\nplt.scatter(X_norm[:, dim1][Y == 1], X_norm[:, dim2][Y == 1], c=\"r\", marker=\"o\", edgecolors=\"k\")\nplt.scatter(X_norm[:, dim1][Y == -1], X_norm[:, dim2][Y == -1], c=\"b\", marker=\"o\", edgecolors=\"k\")\nplt.title(\"Padded and normalised data (dims {} and {})\".format(dim1, dim2))\nplt.show()\n\nplt.figure()\ndim1 = 0\ndim2 = 3\nplt.scatter(features[:, dim1][Y == 1], features[:, dim2][Y == 1], c=\"r\", marker=\"o\", edgecolors=\"k\")\nplt.scatter(\n    features[:, dim1][Y == -1], features[:, dim2][Y == -1], c=\"b\", marker=\"o\", edgecolors=\"k\"\n)\nplt.title(\"Feature vectors (dims {} and {})\".format(dim1, dim2))\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time we want to generalize from the data samples. To monitor the\ngeneralization performance, the data is split into training and\nvalidation set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\nnum_data = len(Y)\nnum_train = int(0.75 * num_data)\nindex = np.random.permutation(range(num_data))\nfeats_train = features[index[:num_train]]\nY_train = Y[index[:num_train]]\nfeats_val = features[index[num_train:]]\nY_val = Y[index[num_train:]]\n\n# We need these later for plotting\nX_train = X[index[:num_train]]\nX_val = X[index[num_train:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization\n~~~~~~~~~~~~\n\nFirst we initialize the variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_qubits = 2\nnum_layers = 6\nvar_init = (0.01 * np.random.randn(num_layers, num_qubits, 3), 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we optimize the cost. This may take a little patience.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = NesterovMomentumOptimizer(0.01)\nbatch_size = 5\n\n# train the variational classifier\nvar = var_init\nfor it in range(60):\n\n    # Update the weights by one optimizer step\n    batch_index = np.random.randint(0, num_train, (batch_size,))\n    feats_train_batch = feats_train[batch_index]\n    Y_train_batch = Y_train[batch_index]\n    var = opt.step(lambda v: cost(v, feats_train_batch, Y_train_batch), var)\n\n    # Compute predictions on train and validation set\n    predictions_train = [np.sign(variational_classifier(var, angles=f)) for f in feats_train]\n    predictions_val = [np.sign(variational_classifier(var, angles=f)) for f in feats_val]\n\n    # Compute accuracy on train and validation set\n    acc_train = accuracy(Y_train, predictions_train)\n    acc_val = accuracy(Y_val, predictions_val)\n\n    print(\n        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n        \"\".format(it + 1, cost(var, features, Y), acc_train, acc_val)\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot the continuous output of the variational classifier for the\nfirst two dimensions of the Iris data set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\ncm = plt.cm.RdBu\n\n# make data for decision regions\nxx, yy = np.meshgrid(np.linspace(0.0, 1.5, 20), np.linspace(0.0, 1.5, 20))\nX_grid = [np.array([x, y]) for x, y in zip(xx.flatten(), yy.flatten())]\n\n# preprocess grid points like data inputs above\npadding = 0.3 * np.ones((len(X_grid), 1))\nX_grid = np.c_[np.c_[X_grid, padding], np.zeros((len(X_grid), 1))]  # pad each input\nnormalization = np.sqrt(np.sum(X_grid ** 2, -1))\nX_grid = (X_grid.T / normalization).T  # normalize each input\nfeatures_grid = np.array(\n    [get_angles(x) for x in X_grid]\n)  # angles for state preparation are new features\npredictions_grid = [variational_classifier(var, angles=f) for f in features_grid]\nZ = np.reshape(predictions_grid, xx.shape)\n\n# plot decision regions\ncnt = plt.contourf(xx, yy, Z, levels=np.arange(-1, 1.1, 0.1), cmap=cm, alpha=0.8, extend=\"both\")\nplt.contour(xx, yy, Z, levels=[0.0], colors=(\"black\",), linestyles=(\"--\",), linewidths=(0.8,))\nplt.colorbar(cnt, ticks=[-1, 0, 1])\n\n# plot data\nplt.scatter(\n    X_train[:, 0][Y_train == 1],\n    X_train[:, 1][Y_train == 1],\n    c=\"b\",\n    marker=\"o\",\n    edgecolors=\"k\",\n    label=\"class 1 train\",\n)\nplt.scatter(\n    X_val[:, 0][Y_val == 1],\n    X_val[:, 1][Y_val == 1],\n    c=\"b\",\n    marker=\"^\",\n    edgecolors=\"k\",\n    label=\"class 1 validation\",\n)\nplt.scatter(\n    X_train[:, 0][Y_train == -1],\n    X_train[:, 1][Y_train == -1],\n    c=\"r\",\n    marker=\"o\",\n    edgecolors=\"k\",\n    label=\"class -1 train\",\n)\nplt.scatter(\n    X_val[:, 0][Y_val == -1],\n    X_val[:, 1][Y_val == -1],\n    c=\"r\",\n    marker=\"^\",\n    edgecolors=\"k\",\n    label=\"class -1 validation\",\n)\n\nplt.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}