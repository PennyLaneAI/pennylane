OpenQML TODO list
=================


Basic idea
----------

Given a quantum computing backend (simulator or actual hardware) via a plugin,
the library takes an optimization problem consisting of a quantum circuit template :math:`f(\theta_i)`
where :math:`\theta_i` is a vector of optimization parameters, some inputs :math:`x_i` and possibly some outputs :math:`y_i`,
a cost/error function based on the output of the circuit, e.g.

.. math::
  C(\theta_i) = \sum_i |\tr(f(\theta_i)(x_i) Y_i) -y_i|^2

and possibly a penalty function. It then trains the circuit by optimizing the thetas to minimize C.
The inputs :math:`x_i` could be initial state preparation procedures, or just arbitrary gate parameters.
The inputs can contain noise, and when using a hardware backend, expectation values :math:`Y_i` are estimated
by averaging a fixed number of measurements, so there's statistical noise in the result.

The trained circuit is tested (or used) with another sample of :math:`x_i` and :math:`y_i`.
Both training and testing steps rely on having a quantum circuit black box at hand.
The thetas are continuous variables, typically rotation angles or in the CV case, unconstrained real numbers.


Variational Circuit
-------------------

The variational circuit :math:`U(\theta) = U_1,...,U_D ` prepares a state :math:`|\psi(\theta)\rangle =  U(\theta) |0\rangle`. There are three modes in which we can run the circuit:

1. Prepare and sample from :math:`|\psi(\theta)\rangle`. Sampling means to perform a projective measurement in some basis, for example the computational or Fock basis.

2. Prepare the state :math:`|\psi(\theta)\rangle` and estimate the expectation value :math:`\langle \psi(\theta)| O |\psi(\theta)\rangle` of an operator :math:`O`. The estimation can be analytical (classical simulations), or by taking the average of multiple measurements (quantum hardware).

3. Use a ''derivative circuit'' to prepare the state :math:`\partial_{\mu} U(\theta) |0\rangle` with :math:`\mu \in \theta` and estimate the derivative of the expectation of :math:`O`:

.. math::

	\partial_{\mu}\langle \psi(\theta)| O |\psi(\theta)\rangle =  -2 \sum_k \sum_l a_k g_l Im[\langle 0 | U_1..A_k..U_D O_l U_1...U_D |0 \rangle ]

where we have :math:`O = \sum_l g_l O_l` and :math:`\partial_{\mu} U(\mu) = \sum_k a_k A_k`. This incorporates the two cases currently mentioned in the literature:

* UNITARIES FROM GENERATORS: If :math:`U(\mu) = e^{i \mu G}`, we decompose :math:`G = \sum_k a_k P_k` where the :math:`P_k` are unitary, then :math:`U_k = P_k U(\mu)`.
* GATE DERIVATIVES AS LINEAR COMBINATIONS OF UNITARIES: We can often decompose :math:`\partial_{\mu} U(\mu) = \sum_k a_k A_k(\mu)`, where the :math`U_k(\mu)` are parametrized unitary gates that are part of the elementary gate set of the quantum device.


The circuit can also depend on some inputs :math:`(x_1,...,x_N)` of which we do not have to take derivatives, but which can nonetheless change between two calls of the circuit.

.. note::

	How do we deal with batch inputs, i.e. if we need the circuit with the same parameters to be executed for batches of inputs :math:`(x_1,...,x_N), (z_1,...,z_N)...`?

.. note::

	The parameter update for a set of parameters can be parallelised on classical architectures, in other words, updating one parameter is independent of the other updates.



Gradient
--------

The optimization algorithm is a gradient-based one, and it should probably be able to handle noisy data.

* stochastic gradient descent

The gradient can be computed in different ways:

1. user giving us a :math:`\nabla_{\theta_i} f` black box in addition to :math:`f`
2. automatically using numerical differentiation based on :math:`f` evaluations only, `<https://pypi.org/project/Numdifftools/>`_
3. automatically using the analytic method from above, i.e. given a circuit :math:`f` construct a circuit for :math:`\nabla_{\theta_i} f`

For (3) we need to know something about gates, each plugin may have its own set.


Circuit gradient
----------------

Assume we have a gate :math:`U` belonging to a one-parameter unitary group generated by :math:`G`:

.. math:: U_\theta = \exp(-i \theta G).

:math:`U` and :math:`G` commute, and thus

.. math::
   \pde{\theta} \Ad_{U_\theta} \rho = \pde{\theta} (U_\theta \rho U_\theta^\dagger) = -i [G, U_\theta \rho U_\theta^\dagger] = -i \ad_G \Ad_{U_\theta} \rho = -i \Ad_{U_\theta} \ad_G \rho.

Assume now we have a propagator :math:`U = B U_\theta A`, and the initial state of the system is :math:`\rho`.
The expectation values of various observables are obtained as

.. math::
   \expect{x_i}_U = \tr(\Ad_{U}(\rho) x_i)
   = \tr(\rho \Ad_{U^\dagger}(x_i)).

The gradient of the expectation value is given by

.. math::
   \pde{\theta} \expect{x_i}_{B U_\theta A} = \tr(x_i \Ad_B (\pde{\theta} \Ad_{U_\theta}) \Ad_A(\rho))
   = \tr(x_i \Ad_B (-i \ad_G) \Ad_{U_\theta A}(\rho)).


Special case: single-qubit gates
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For single-qubit gates, generated by a linear combination of Pauli matrices :math:`G = \vec{a} \cdot \vec{\sigma}/2` where :math:`|\vec{a}|=1`,
we obtain a closed form expression for the unitary gate :math:`W = U_{\pi/2} = \frac{1}{\sqrt{2}}(\I -2iG)`.
This yields

.. math::
   \Ad_W \rho = W \rho W^\dagger
   = \frac{1}{2}\left((\I-2iG)\rho(\I+2iG)\right)
   = \frac{1}{2}\left(\rho -2i\ad_G(\rho) +4G \rho G \right)

and thus we obtain the convenient formula

.. math::
   \frac{1}{2}(\Ad_W-\Ad_{W^\dagger}) \rho = -i\ad_G(\rho).

This enables us to compute the expectation value gradient as the average of two other expectation values,
obtained using parameter-shifted versions of the gate :math:`U_\theta`:

.. math::
   \pde{\theta} \expect{x_i}_{B U_\theta A}
   = \tr(x_i \Ad_B (-i \ad_G) \Ad_{U_\theta A}(\rho)).
   = \frac{1}{2} \tr(x_i \Ad_B (\Ad_W-\Ad_{W^\dagger}) \Ad_{U_\theta A}(\rho)).
   = \frac{1}{2} \left(\expect{x_i}_{B U_{\theta+\pi/2} A}  -\expect{x_i}_{B U_{\theta-\pi/2} A}\right).



Operator basis approach
~~~~~~~~~~~~~~~~~~~~~~~

Now, given an operator basis :math:`\{x_i\}_i`, we may expand the adjoint representation in it:

.. math::
   \Ad_A x_i &= A x_i A^\dagger = \tilde{A}_{ij} x_j,

   \ad_G x_i &= [G, x_i] = \hat{G}_{ij} x_j.

We have the properties :math:`\widetilde{A^\dagger} = (\tilde{A})^{-1}`
and :math:`\widetilde{BA}_{ik} = \tilde{A}_{ij} \tilde{B}_{jk}`.
Assume now we have a propagator :math:`B U_\theta A`.
The expectation values are given by

.. math::
   \expect{x_i}
   = \tr(\rho \Ad_{(B U_\theta A)^\dagger}(x_i))
   = (\tilde{B})^{-1}_{ij} (\tilde{U_\theta})^{-1}_{jl} (\tilde{A})^{-1}_{lm} \: \tr(\rho x_m)

Their derivatives are given by

.. math::
   \pde{\theta} \expect{x_i}
   &= \tr(\rho \Ad_{A^\dagger} \pde{\theta} \Ad_{U_\theta^\dagger} \Ad_{B^\dagger} (x_i))
   = i \tr(\rho \Ad_{A^\dagger} \ad_G \Ad_{U_\theta^\dagger} \Ad_{B^\dagger} (x_i))
   = i (\tilde{B})^{-1}_{ij} (\tilde{U_\theta})^{-1}_{jk} \hat{G}_{kl} (\tilde{A})^{-1}_{lm} \: \tr(\rho x_m)

   &= (\tilde{B})^{-1}_{ij} \pde{\theta} (\tilde{U_\theta})^{-1}_{jl} (\tilde{A})^{-1}_{lm} \: \tr(\rho x_m)

Consistency requires

.. math::
   \pde{\theta} (\tilde{U_\theta})^{-1}_{jl} = i (\tilde{U_\theta})^{-1}_{jk} \hat{G}_{kl}

The nastiness with the inverses probably results from us not requiring the operator basis :math:`\{x_i\}_i` to be orthonormal.



Optimization problems supported
-------------------------------

* State fitting: No inputs, :math:`y_i` is a target quantum state, :math:`f(\theta_i)` should prepare a state maximally close to the target.
  Note that this cannot be done (efficiently) with a hardware backend.
* Generative model: No inputs, :math:`y_i` are measurement samples (for example expectation values of observables),
  :math:`f(\theta_i)` should prepare a state that produces those samples with maximal likelihood.
* Quantum classifier: :math:`x_i, y_i` are input and output samples, :math:`g(f(\theta_i))` should map the inputs to the outputs.
  :math:`g` is a postprocessing function that maps the circuit state into the required output domain.


Features
--------

* We should be able to tell a plugin to build the given circuit, composed of gates in its library with given parameters, and then
  estimate the :math:`\expect{Y_i}` expectation values to a given accuracy, or using a given number of repeats.
* How do we propose a circuit template, or is the user responsible for it? Maybe each plugin should come with a few default templates.
* If the backend/plugin is responsible for both the gates and the circuit template, maybe the only reason we need to know about them
  is to build the gradient circuit? Otherwise it could just be a black box :math:`f(\theta_i, x_i)` for us.
* Gradient circuit probably requires that the plugins can communicate to us their gate library, in (gate, generator) pairs.
  Alternatively, if the gate derivative can be computed by shifting the parameter, (gate, derivative_par_shift) pairs.
* Should the plugins build and store a circuit graph with explicit parameter dependencies (the Tensorflow approach)
  and evaluate it with different parameter values, or rebuild the circuit anew each time the parameters change?
* Automatic differentiation of classical input/output/parameter transformation functions: use Tensorflow?


Misc. ideas
-----------

* The above approach assumes a fixed circuit/black box with continuous parameters.
  Maybe we could try to optimize the circuit template too, using discrete optimization methods?
* What about using a quantum device to train a classical model, and use/test it in classical hardware?


Data-flow graph
---------------

Much like in TensorFlow or other graph-based automatic differentiation frameworks,
we may model our computational system using a directed acyclic graph (DAG).
In our case

* The edges represent classical variables (scalar or vector) and the nodes represent functions transforming inputs to outputs.
* The nodes can be either classical (classical neural nets, transformation functions) or
  quantum (quantum circuits with classical inputs (gate parameters, initializations) and outputs (measurement results, expectation values)).

  * The graph can be coarse-grained by grouping nodes together into a single node.
    A node that contains any quantum information processing becomes a quantum node.
  * We may normally assume that classical nodes are easy to compute, and quantum nodes are hard to compute unless
    we have access to quantum hardware. OpenQML computes the classical nodes on its own, and relegates the quantum nodes to whatever plugin is in use.
  * We have 3 special nodes, PAR and DATA (with only outgoing edges, representing input) and OUT (with only incoming edges, representing output).

* In this model, the edges are always classical. Quantum information never enters or leaves a node.
  In principle the model could be extended to handle quantum edges as well,
  in which case we could zoom into a quantum node, revealing e.g. a quantum circuit with quantum wires in it.
  This would result in a superset of the quantum circuit notation.

* The edges could maybe be divided into two types, data (D) and optimization parameters/weights (P).
  We now have the following basic rules:

  * Data only comes from data: D output requires a D input (exception: DATA node)
  * Parameters are not affected by data: P output requires that P is the only input type (exception: PAR node)
  * The only sink (node that returns nothing) is OUT
  * The only sources (nodes that take no inputs) are DATA and PAR

* This leaves us with the following intermediate node types:

  * D -> D: data transformation
  * P -> P: parameter transformation
  * D,P -> D: parametrized data processing

* If we relax the "don't mix parameters and data" rule, we could have D,P -> D,P nodes as well.
* As a special case we have a linear graph which can represent e.g. layered neural nets or classical/quantum sandwich structures.


We may use standard AD methods to compute the gradients of classical nodes, and implement our own system
for computing the gradients of the quantum nodes by sending the plugin a modified quantum circuit to execute.
