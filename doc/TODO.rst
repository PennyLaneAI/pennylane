OpenQML TODO list
=================


Basic idea
----------

Given a quantum computing backend (simulator or actual hardware) via a plugin,
the library takes an optimization problem consisting of a quantum circuit template :math:`f(\theta_i)`
where :math:`\theta_i` is a vector of optimization parameters, some inputs :math:`x_i` and possibly some outputs :math:`y_i`,
a cost/error function based on the output of the circuit, e.g.

.. math::
  C(\theta_i) = \sum_i |\tr(f(\theta_i)(x_i) Y_i) -y_i|^2

and possibly a penalty function. It then trains the circuit by optimizing the thetas to minimize C.
The inputs :math:`x_i` could be initial state preparation procedures, or just arbitrary gate parameters.
The inputs can contain noise, and when using a hardware backend, expectation values :math:`Y_i` are estimated
by averaging a fixed number of measurements, so there's statistical noise in the result.

The trained circuit is tested (or used) with another sample of :math:`x_i` and :math:`y_i`.
Both training and testing steps rely on having a quantum circuit black box at hand.
The thetas are continuous variables, typically rotation angles or in the CV case, unconstrained real numbers.


Variational Circuit
-------------------

The variational circuit :math:`U(\theta) = U_1,...,U_D ` prepares a state :math:`|\psi(\theta)\rangle =  U(\theta) |0\rangle`. There are three modes in which we can run the circuit:

1. Prepare and sample from :math:`|\psi(\theta)\rangle`. Sampling means to perform a projective measurement in some basis, for example the computational or Fock basis.

2. Prepare the state :math:`|\psi(\theta)\rangle` and estimate the expectation value :math:`\langle \psi(\theta)| O |\psi(\theta)\rangle` of an operator :math:`O`. The estimation can be analytical (classical simulations), or by taking the average of multiple measurements (quantum hardware).

3. Use a ''derivative circuit'' to prepare the state :math:`\partial_{\mu} U(\theta) |0\rangle` with :math:`\mu \in \theta` and estimate the derivative of the expectation of :math:`O`:

.. math::
		
	\partial_{\mu}\langle \psi(\theta)| O |\psi(\theta)\rangle =  -2 \sum_k \sum_l a_k g_l Im[\langle 0 | U_1..A_k..U_D O_l U_1...U_D |0 \rangle ]

where we have :math:`O = \sum_l g_l O_l` and :math:`\partial_{\mu} U(\mu) = \sum_k a_k A_k`. This incorporates the two cases currently mentioned in the literature:

* UNITARIES FROM GENERATORS: If :math:`U(\mu) = e^{i \mu G}`, we decompose :math:`G = \sum_k a_k P_k` where the :math:`P_k` are unitary, then :math:`U_k = P_k U(\mu)`. 
* GATE DERIVATIVES AS LINEAR COMBINATIONS OF UNITARIES: We can often decompose :math:`\partial_{\mu} U(\mu) = \sum_k a_k A_k(\mu)`, where the :math`U_k(\mu)` are parametrized unitary gates that are part of the elementary gate set of the quantum device.
	

The circuit can also depend on some inputs :math:`(x_1,...,x_N)` of which we do not have to take derivatives, but which can nonetheless change between two calls of the circuit. 

.. note::

	How do we deal with batch inputs, i.e. if we need the circuit with the same parameters to be executed for batches of inputs :math:`(x_1,...,x_N), (z_1,...,z_N)...`? 

.. note::

	The parameter update for a set of parameters can be parallelised on classical architectures, in other words, updating one parameter is independent of the other updates.



Gradient
--------

The optimization algorithm is a gradient-based one, and it should probably be able to handle noisy data.

* stochastic gradient descent

The gradient can be computed in different ways:

1. user giving us a :math:`\nabla_{\theta_i} f` black box in addition to :math:`f`
2. automatically using numerical differentiation based on :math:`f` evaluations only, `<https://pypi.org/project/Numdifftools/>`_
3. automatically using the analytic method from above, i.e. given a circuit :math:`f` construct a circuit for :math:`\nabla_{\theta_i} f`

For (3) we need to know something about gates, each plugin may have its own set.


Circuit gradient
----------------

Assume we have a gate :math:`U` belonging to a one-parameter unitary group generated by :math:`G`:

.. math:: U_\theta = \exp(i \theta G).

:math:`U` and :math:`G` commute, and thus

.. math::
   \pde{\theta} \Ad_{U_\theta} \rho = \pde{\theta} (U_\theta \rho U_\theta^\dagger) = i [G, U_\theta \rho U_\theta^\dagger] = i \ad_G \Ad_{U_\theta} \rho.


Now, given an operator basis :math:`\{x_i\}_i`, we may expand the adjoint representation in it:

.. math::
   \Ad_A x_i &= A x_i A^\dagger = \tilde{A}_{ij} x_j,

   \ad_G x_i &= [G, x_i] = \hat{G}_{ij} x_j.

We have the properties :math:`\widetilde{A^\dagger} = (\tilde{A})^{-1}`
and :math:`\widetilde{BA}_{ik} = \tilde{A}_{ij} \tilde{B}_{jk}`.
Assume now we have a propagator :math:`B U_\theta A`.
The expectation values of various observables are obtained as

.. math::
   \expect{x_i} = \tr(\Ad_{B U_\theta A}(\rho) x_i)
   = \tr(\rho \Ad_{(B U_\theta A)^\dagger}(x_i))
   = (\tilde{B})^{-1}_{ij} (\tilde{U_\theta})^{-1}_{jl} (\tilde{A})^{-1}_{lm} \: \tr(\rho x_m)

Their derivatives are given by

.. math::
   \pde{\theta} \expect{x_i}
   &= \tr(\rho \Ad_{A^\dagger} \pde{\theta} \Ad_{U_\theta^\dagger} \Ad_{B^\dagger} (x_i))
   = -i \tr(\rho \Ad_{A^\dagger} \ad_G \Ad_{U_\theta^\dagger} \Ad_{B^\dagger} (x_i))
   = -i (\tilde{B})^{-1}_{ij} (\tilde{U_\theta})^{-1}_{jk} \hat{G}_{kl} (\tilde{A})^{-1}_{lm} \: \tr(\rho x_m)

   &= (\tilde{B})^{-1}_{ij} \pde{\theta} (\tilde{U_\theta})^{-1}_{jl} (\tilde{A})^{-1}_{lm} \: \tr(\rho x_m)

Consistency requires

.. math::
   \pde{\theta} (\tilde{U_\theta})^{-1}_{jl} = -i (\tilde{U_\theta})^{-1}_{jk} \hat{G}_{kl}

The nastiness with the inverses probably results from us not requiring the operator basis :math:`\{x_i\}_i` to be orthonormal.



Optimization problems supported
-------------------------------

* State fitting: No inputs, :math:`y_i` is a target quantum state, :math:`f(\theta_i)` should prepare a state maximally close to the target.
  Note that this cannot be done (efficiently) with a hardware backend.
* Generative model: No inputs, :math:`y_i` are measurement samples (for example expectation values of observables),
  :math:`f(\theta_i)` should prepare a state that produces those samples with maximal likelihood.
* Quantum classifier: :math:`x_i, y_i` are input and output samples, :math:`g(f(\theta_i))` should map the inputs to the outputs.
  :math:`g` is a postprocessing function that maps the circuit state into the required output domain.


Features
--------

* We should be able to tell a plugin to build the given circuit, composed of gates in its library with given parameters, and then
  estimate the :math:`\expect{Y_i}` expectation values to a given accuracy, or using a given number of repeats.
* How do we propose a circuit template, or is the user responsible for it? Maybe each plugin should come with a few default templates.
* If the backend/plugin is responsible for both the gates and the circuit template, maybe the only reason we need to know about them
  is to build the gradient circuit? Otherwise it could just be a black box :math:`f(\theta_i, x_i)` for us.
* Gradient circuit probably requires that the plugins can communicate to us their gate library, in (gate, generator) pairs.
  Alternatively, if the gate derivative can be computed by shifting the parameter, (gate, derivative_par_shift) pairs.
* Should the plugins build and store a circuit graph with explicit parameter dependencies (the Tensorflow approach)
  and evaluate it with different parameter values, or rebuild the circuit anew each time the parameters change?
* Automatic differentiation of classical input/output/parameter transformation functions: use Tensorflow?


Misc. ideas
-----------

* The above approach assumes a fixed circuit/black box with continuous parameters.
  Maybe we could try to optimize the circuit template too, using discrete optimization methods?
* What about using a quantum device to train a classical model, and use/test it in classical hardware?
