{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1 - Photon redirection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the very basic working principles of PennyLane for continuous-variable backends. ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import openqml, as well as openqml's version of numpy. This allows us to automatically compute gradients for functions that manipulate numpy arrays, including quantum functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openqml as qm\n",
    "from openqml import numpy as np\n",
    "from openqml._optimize import GradientDescentOptimizer, AdagradOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a \"device\" to run the quantum node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev1 = qm.device('default.qubit', wires=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum function\n",
    "\n",
    "We define a quantum function called \"circuit\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qm.qfunc(dev1)\n",
    "def circuit(weights):\n",
    "    \n",
    "    qm.RX(weights[0], [0])\n",
    "    qm.RY(weights[1], [0])\n",
    "    \n",
    "    return qm.expectation.PauliZ(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses openqml to run the following quantum circuit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rotation circuit](figures/rotation_circuit.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a qubit in the ground state, \n",
    "\n",
    "$$ |0\\rangle = \\begin{pmatrix}1 \\\\ 0 \\end{pmatrix}, $$\n",
    "\n",
    "we first rotate the qubit around the x-axis by \n",
    "$$R_x(w_0) = e^{-iw_0 X /2} = \n",
    "\\begin{pmatrix} \\cos \\frac{w_0}{2} &  -i \\sin \\frac{w_0}{2} \\\\  \n",
    "                -i \\sin \\frac{w_0}{2} &  \\cos \\frac{w_0}{2} \n",
    "\\end{pmatrix}, $$ \n",
    "               \n",
    "and then around the y-axis by \n",
    "$$ R_y(w_1) = e^{-i w_1 Y/2} = \n",
    "\\begin{pmatrix} \\cos \\frac{w_1}{2} &  - \\sin \\frac{w_1}{2} \\\\  \n",
    "                \\sin \\frac{w_1}{2} &  \\cos \\frac{w_1}{2} \n",
    "\\end{pmatrix}. $$ \n",
    "\n",
    "After these operations the qubit is in the state\n",
    "\n",
    "$$ | \\psi \\rangle = R_y(w_0) R_x(w_1) | 0 \\rangle $$\n",
    "\n",
    "Finally, we measure the expectation $ \\langle \\psi | Z | \\psi \\rangle $ of the Pauli-Z operator \n",
    "$$Z = \n",
    "\\begin{pmatrix} 1 &  0 \\\\  \n",
    "                0 & -1 \n",
    "\\end{pmatrix}. $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the circuit parameters $w_0$ and $w_1$, the output expectation lies between $1$ (if $| \\psi \\rangle = | 0  \\rangle $) and $-1$ (if $| \\psi \\rangle = | 1  \\rangle $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a cost. Here, the cost is directly the expectation of the PauliZ measurement, so that the cost is trivially the output of the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(weights):\n",
    "    return circuit(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this objective, the optimization procedure is supposed to find the weights that rotate the qubit from the ground state \n",
    "\n",
    " <img src=\"figures/bloch_before.png\" width=\"250\"> \n",
    " \n",
    " to the excited state\n",
    " \n",
    " <img src=\"figures/bloch_after.png\" width=\"250\">\n",
    " \n",
    " The rotation gates give the optimization landscape a trigonometric shape with four global minima and five global maxima.\n",
    " \n",
    " <img src=\"figures/optlandscape.png\" width=\"450\">\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values of the x- and y-rotation parameters are set to near-zero. This corresponds to identity gates, in other words, the circuit leaves the qubit in the ground state. *Note that at zero exactly the gradient is zero and the optimization algorithm will not descent from the maximum.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rotation angles: [0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "weights0 = np.array([0.01, 0.01])\n",
    "print('Initial rotation angles:', weights0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a simple Gradient Descent Optimizer and update the weights for 10 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after step 5: 0.9942561248007442\n",
      "Cost after step 10: 0.7312843497373493\n",
      "Cost after step 15: 0.008145934071862121\n",
      "Cost after step 20: 8.09318002101267e-06\n",
      "Cost after step 25: 7.903630999672728e-09\n",
      "Cost after step 30: 7.718409245072166e-12\n",
      "Cost after step 35: 7.66053886991358e-15\n",
      "Cost after step 40: 3.3306690738754696e-16\n",
      "Cost after step 45: 1.6653345369377348e-16\n",
      "Cost after step 50: -1.3877787807814457e-16\n",
      "Cost after step 55: -8.798517470154366e-15\n",
      "Cost after step 60: -5.035138972431241e-13\n",
      "Cost after step 65: -2.903546847399241e-11\n",
      "Cost after step 70: -1.6743259834139934e-09\n",
      "Cost after step 75: -9.655006341646732e-08\n",
      "Cost after step 80: -5.567542194023778e-06\n",
      "Cost after step 85: -0.0003209827748970606\n",
      "Cost after step 90: -0.01827993232467756\n",
      "Cost after step 95: -0.5894463605133992\n",
      "Cost after step 100: -0.9988743067653387\n",
      "\n",
      "Optimized rotation angles: [-0.03355765  3.108035  ]\n"
     ]
    }
   ],
   "source": [
    "o = GradientDescentOptimizer(0.5)\n",
    "\n",
    "weights = weights0\n",
    "for step in np.arange(1, 101):\n",
    "    weights = o.step(objective, weights)\n",
    "    if step%5==0:\n",
    "        print('Cost after step {}: {}'.format(step, cost(weights)) )\n",
    "\n",
    "print()\n",
    "print('Optimized rotation angles:', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting at a different offset, we train another optimizer called Adagrad, which improves on gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rotation angles: [-0.01  0.01]\n",
      "Cost after step 5: 0.00013306617779057506\n",
      "Cost after step 10: 6.098715976676772e-11\n",
      "Cost after step 15: 5.551115123125783e-17\n",
      "Cost after step 20: 1.1102230246251565e-16\n",
      "Cost after step 25: 1.6653345369377348e-16\n",
      "Cost after step 30: -1.6653345369377348e-16\n",
      "Cost after step 35: -2.7755575615628914e-16\n",
      "Cost after step 40: -7.119305145408816e-14\n",
      "Cost after step 45: -2.120639774894073e-11\n",
      "Cost after step 50: -6.3168203168206816e-09\n",
      "Cost after step 55: -1.881604829112593e-06\n",
      "Cost after step 60: -0.000560067861131136\n",
      "Cost after step 65: -0.13763076512457606\n",
      "Cost after step 70: -0.9702268349223584\n",
      "Cost after step 75: -0.9998837262003906\n",
      "Cost after step 80: -0.9999995647190025\n",
      "Cost after step 85: -0.9999999983707548\n",
      "Cost after step 90: -0.9999999999939022\n",
      "Cost after step 95: -0.9999999999999776\n",
      "Cost after step 100: -1.0\n",
      "\n",
      "Optimized rotation angles: [-9.24310272e-09  3.14159264e+00]\n"
     ]
    }
   ],
   "source": [
    "weights0 = np.array([-0.01, 0.01])\n",
    "print('Initial rotation angles:', weights0)\n",
    "\n",
    "o = AdagradOptimizer(0.5)\n",
    "\n",
    "weights = weights0\n",
    "for step in np.arange(1, 101):\n",
    "    weights = o.step(cost, weights)\n",
    "    if step%5==0:\n",
    "        print('Cost after step {}: {}'.format(step, cost(weights)) )\n",
    "\n",
    "print()\n",
    "print('Optimized rotation angles:', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Adagrad and gradient descent find the same minimum, and, since neither has information on second order derivatives, both take a detour through a saddle point. However, Adagrad takes considerably fewer steps.\n",
    " \n",
    " <img src=\"figures/gd_vs_adag.png\" width=\"450\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
