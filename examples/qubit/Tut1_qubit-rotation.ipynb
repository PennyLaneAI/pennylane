{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1 - Qubit rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the very basic working principles of PennyLane for qubit-based backends. We only look at a single quantum function consisting of a single qubit circuit. The task is to optimize two rotation gates in order to flip the qubit from state $|0\\rangle$ to state $|1\\rangle $. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import openqml, as well as openqml's version of numpy. This allows us to automatically compute gradients for functions that manipulate numpy arrays, including quantum functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openqml as qm\n",
    "from openqml import numpy as np\n",
    "from openqml._optimize import GradientDescentOptimizer, AdagradOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a \"device\" to run the quantum node. We only need a single quantum wire. This example uses the default qubit simulator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev1 = qm.device('default.qubit', wires=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum function\n",
    "\n",
    "We define a quantum function called \"circuit\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qm.qfunc(dev1)\n",
    "def circuit(weights):\n",
    "    \n",
    "    qm.RX(weights[0], [0])\n",
    "    qm.RY(weights[1], [0])\n",
    "    \n",
    "    return qm.expectation.PauliZ(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses openqml to run the following quantum circuit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/rotation_circuit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a qubit in the ground state, \n",
    "\n",
    "$$ |0\\rangle = \\begin{pmatrix}1 \\\\ 0 \\end{pmatrix}, $$\n",
    "\n",
    "we first rotate the qubit around the x-axis by \n",
    "$$R_x(w_0) = e^{-iw_0 X /2} = \n",
    "\\begin{pmatrix} \\cos \\frac{w_0}{2} &  -i \\sin \\frac{w_0}{2} \\\\  \n",
    "                -i \\sin \\frac{w_0}{2} &  \\cos \\frac{w_0}{2} \n",
    "\\end{pmatrix}, $$ \n",
    "               \n",
    "and then around the y-axis by \n",
    "$$ R_y(w_1) = e^{-i w_1 Y/2} = \n",
    "\\begin{pmatrix} \\cos \\frac{w_1}{2} &  - \\sin \\frac{w_1}{2} \\\\  \n",
    "                \\sin \\frac{w_1}{2} &  \\cos \\frac{w_1}{2} \n",
    "\\end{pmatrix}. $$ \n",
    "\n",
    "After these operations the qubit is in the state\n",
    "\n",
    "$$ | \\psi \\rangle = R_y(w_0) R_x(w_1) | 0 \\rangle $$\n",
    "\n",
    "Finally, we measure the expectation $ \\langle \\psi | Z | \\psi \\rangle $ of the Pauli-Z operator \n",
    "$$Z = \n",
    "\\begin{pmatrix} 1 &  0 \\\\  \n",
    "                0 & -1 \n",
    "\\end{pmatrix}. $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the circuit parameters $w_0$ and $w_1$, the output expectation lies between $1$ (if $| \\psi \\rangle = | 0  \\rangle $) and $-1$ (if $| \\psi \\rangle = | 1  \\rangle $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a cost. Here, the cost is directly the expectation of the PauliZ measurement, so that the cost is trivially the output of the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(weights):\n",
    "    return circuit(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this objective, the optimization procedure is supposed to find the weights that rotate the qubit from the ground state \n",
    "\n",
    " <img src=\"figures/bloch_before.png\" width=\"250\"> \n",
    " \n",
    " to the excited state\n",
    " \n",
    " <img src=\"figures/bloch_after.png\" width=\"250\">\n",
    " \n",
    " The rotation gates give the optimization landscape a trigonometric shape with four global minima and five global maxima.\n",
    " \n",
    " <img src=\"figures/optlandscape.png\" width=\"450\">\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values of the x- and y-rotation parameters are set to near-zero. This corresponds to identity gates, in other words, the circuit leaves the qubit in the ground state. *Note that at zero exactly the gradient is zero and the optimization algorithm will not descent from the maximum.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rotation angles: [0.01 0.01]\n"
     ]
    }
   ],
   "source": [
    "weights0 = np.array([0.01, 0.01])\n",
    "print('Initial rotation angles:', weights0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a simple Gradient Descent Optimizer and update the weights for 10 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GradientDescentOptimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c93be2339156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GradientDescentOptimizer' is not defined"
     ]
    }
   ],
   "source": [
    "o = GradientDescentOptimizer(0.5)\n",
    "\n",
    "weights = weights0\n",
    "for step in np.arange(1, 101):\n",
    "    weights = o.step(objective, weights)\n",
    "    if step%5==0:\n",
    "        print('Objective after step {:5d}: {:.7f}'.format(step, objective(weights)) )\n",
    "\n",
    "print()\n",
    "print('Optimized rotation angles:', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting at a different offset, we train another optimizer called Adagrad, which improves on gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rotation angles: [-0.01  0.01]\n",
      "Cost after step 5: 0.00013306617779057506\n",
      "Cost after step 10: 6.098715976676772e-11\n",
      "Cost after step 15: 5.551115123125783e-17\n",
      "Cost after step 20: 1.1102230246251565e-16\n",
      "Cost after step 25: 1.6653345369377348e-16\n",
      "Cost after step 30: -1.6653345369377348e-16\n",
      "Cost after step 35: -2.7755575615628914e-16\n",
      "Cost after step 40: -7.119305145408816e-14\n",
      "Cost after step 45: -2.120639774894073e-11\n",
      "Cost after step 50: -6.3168203168206816e-09\n",
      "Cost after step 55: -1.881604829112593e-06\n",
      "Cost after step 60: -0.000560067861131136\n",
      "Cost after step 65: -0.13763076512457606\n",
      "Cost after step 70: -0.9702268349223584\n",
      "Cost after step 75: -0.9998837262003906\n",
      "Cost after step 80: -0.9999995647190025\n",
      "Cost after step 85: -0.9999999983707548\n",
      "Cost after step 90: -0.9999999999939022\n",
      "Cost after step 95: -0.9999999999999776\n",
      "Cost after step 100: -1.0\n",
      "\n",
      "Optimized rotation angles: [-9.24310272e-09  3.14159264e+00]\n"
     ]
    }
   ],
   "source": [
    "weights0 = np.array([-0.01, 0.01])\n",
    "print('Initial rotation angles:', weights0)\n",
    "\n",
    "o = AdagradOptimizer(0.5)\n",
    "\n",
    "weights = weights0\n",
    "for step in np.arange(1, 101):\n",
    "    weights = o.step(cost, weights)\n",
    "    if step%5==0:\n",
    "        print('Objective after step {:5d}: {:.7f}'.format(step, objective(weights)) )\n",
    "\n",
    "print()\n",
    "print('Optimized rotation angles:', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Adagrad and gradient descent find the same minimum, and, since neither has information on second order derivatives, both take a detour through a saddle point. However, Adagrad takes considerably fewer steps.\n",
    " \n",
    " <img src=\"figures/gd_vs_adag.png\" width=\"450\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
