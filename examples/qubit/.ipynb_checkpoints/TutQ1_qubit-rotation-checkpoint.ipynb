{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Q1 - Qubit rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates the very basic working principles of openqml for qubit-based backends. We only look at a single quantum function consisting of a single-qubit circuit. The task is to optimize two rotation gates in order to flip the qubit from state $|0\\rangle$ to state $|1\\rangle $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import openqml, as well as openqml's version of numpy. This allows us to automatically compute gradients for functions that manipulate numpy arrays, including quantum functions. We call this numpy version `onp` in case we need it alongside the original version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openqml as qm\n",
    "from openqml import numpy as onp\n",
    "from openqml._optimize import GradientDescentOptimizer, AdagradOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a \"device\" to run the quantum node. We only need a single quantum wire. This example uses the default qubit simulator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev1 = qm.device('default.qubit', wires=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum function\n",
    "\n",
    "We define a quantum function called \"circuit\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qm.qfunc(dev1)\n",
    "def circuit(weights):\n",
    "    \n",
    "    qm.RX(weights[0], [0])\n",
    "    qm.RY(weights[1], [0])\n",
    "    \n",
    "    return qm.expectation.PauliZ(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function uses openqml to run the following quantum circuit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/rotation_circuit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a qubit in the ground state, \n",
    "\n",
    "$$ |0\\rangle = \\begin{pmatrix}1 \\\\ 0 \\end{pmatrix}, $$\n",
    "\n",
    "we first rotate the qubit around the x-axis by \n",
    "$$R_x(w_0) = e^{-iw_0 X /2} = \n",
    "\\begin{pmatrix} \\cos \\frac{w_0}{2} &  -i \\sin \\frac{w_0}{2} \\\\  \n",
    "                -i \\sin \\frac{w_0}{2} &  \\cos \\frac{w_0}{2} \n",
    "\\end{pmatrix}, $$ \n",
    "               \n",
    "and then around the y-axis by \n",
    "$$ R_y(w_1) = e^{-i w_1 Y/2} = \n",
    "\\begin{pmatrix} \\cos \\frac{w_1}{2} &  - \\sin \\frac{w_1}{2} \\\\  \n",
    "                \\sin \\frac{w_1}{2} &  \\cos \\frac{w_1}{2} \n",
    "\\end{pmatrix}. $$ \n",
    "\n",
    "After these operations the qubit is in the state\n",
    "\n",
    "$$ | \\psi \\rangle = R_y(w_0) R_x(w_1) | 0 \\rangle $$\n",
    "\n",
    "Finally, we measure the expectation $ \\langle \\psi | Z | \\psi \\rangle $ of the Pauli-Z operator \n",
    "$$Z = \n",
    "\\begin{pmatrix} 1 &  0 \\\\  \n",
    "                0 & -1 \n",
    "\\end{pmatrix}. $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the circuit parameters $w_1$ and $w_2$, the output expectation lies between $1$ (if $| \\psi \\rangle = | 0  \\rangle $) and $-1$ (if $| \\psi \\rangle = | 1  \\rangle $)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a cost. Here, the cost is directly the expectation of the PauliZ measurement, so that the cost is trivially the output of the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(weights):\n",
    "    return circuit(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this objective, the optimization procedure is supposed to find the weights that rotate the qubit from the ground state \n",
    "\n",
    " <img src=\"figures/bloch_before.png\" width=\"250\"> \n",
    " \n",
    " to the excited state\n",
    " \n",
    " <img src=\"figures/bloch_after.png\" width=\"250\">\n",
    " \n",
    " The rotation gates give the optimization landscape a trigonometric shape with four global minima and five global maxima.\n",
    " \n",
    " <img src=\"figures/optlandscape.png\" width=\"450\">\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values of the x- and y-rotation parameters $w_1, w_2$ are set to near-zero. This corresponds to identity gates, in other words, the circuit leaves the qubit in the ground state. *Note that at zero exactly the gradient vanishes and the optimization algorithm will not descent from the maximum.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights0 = np.array([0.01, 0.01])\n",
    "\n",
    "weights0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the objective at the initial point is close to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999000033332889"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective(weights0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a simple Gradient Descent Optimizer and update the weights for 10 steps. The final parameters correspond to a $Z$ expectation of nearly $-1$, which means that the qubit is flipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective after step     5:  0.9942561\n",
      "Objective after step    10:  0.7312843\n",
      "Objective after step    15:  0.0081459\n",
      "Objective after step    20:  0.0000081\n",
      "Objective after step    25:  0.0000000\n",
      "Objective after step    30:  0.0000000\n",
      "Objective after step    35:  0.0000000\n",
      "Objective after step    40:  0.0000000\n",
      "Objective after step    45:  0.0000000\n",
      "Objective after step    50: -0.0000000\n",
      "Objective after step    55: -0.0000000\n",
      "Objective after step    60: -0.0000000\n",
      "Objective after step    65: -0.0000000\n",
      "Objective after step    70: -0.0000000\n",
      "Objective after step    75: -0.0000001\n",
      "Objective after step    80: -0.0000056\n",
      "Objective after step    85: -0.0003210\n",
      "Objective after step    90: -0.0182799\n",
      "Objective after step    95: -0.5894464\n",
      "Objective after step   100: -0.9988743\n",
      "\n",
      "Optimized rotation angles: [0.03355765 3.108035  ]\n"
     ]
    }
   ],
   "source": [
    "o = GradientDescentOptimizer(0.5)\n",
    "\n",
    "weights = weights0\n",
    "for step in np.arange(1, 101):\n",
    "    weights = o.step(objective, weights)\n",
    "    if step%5==0:\n",
    "        print('Objective after step {:5d}: {: .7f}'.format(step, objective(weights)) )\n",
    "\n",
    "print()\n",
    "print('Optimized rotation angles:', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting at a different offset, we train another optimizer called Adagrad, which improves on gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rotation angles: [-0.01  0.01]\n",
      "Objective after step     5:  0.0001331\n",
      "Objective after step    10:  0.0000000\n",
      "Objective after step    15:  0.0000000\n",
      "Objective after step    20:  0.0000000\n",
      "Objective after step    25:  0.0000000\n",
      "Objective after step    30: -0.0000000\n",
      "Objective after step    35: -0.0000000\n",
      "Objective after step    40: -0.0000000\n",
      "Objective after step    45: -0.0000000\n",
      "Objective after step    50: -0.0000000\n",
      "Objective after step    55: -0.0000019\n",
      "Objective after step    60: -0.0005601\n",
      "Objective after step    65: -0.1376308\n",
      "Objective after step    70: -0.9702268\n",
      "Objective after step    75: -0.9998837\n",
      "Objective after step    80: -0.9999996\n",
      "Objective after step    85: -1.0000000\n",
      "Objective after step    90: -1.0000000\n",
      "Objective after step    95: -1.0000000\n",
      "Objective after step   100: -1.0000000\n",
      "\n",
      "Optimized rotation angles: [-9.24310272e-09  3.14159264e+00]\n"
     ]
    }
   ],
   "source": [
    "weights0 = np.array([-0.01, 0.01])\n",
    "print('Initial rotation angles:', weights0)\n",
    "\n",
    "o = AdagradOptimizer(0.5)\n",
    "\n",
    "weights = weights0\n",
    "for step in np.arange(1, 101):\n",
    "    weights = o.step(objective, weights)\n",
    "    if step%5==0:\n",
    "        print('Objective after step {:5d}: {: .7f}'.format(step, objective(weights)) )\n",
    "\n",
    "print()\n",
    "print('Optimized rotation angles:', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Adagrad and gradient descent find the same minimum, and, since neither has information on second order derivatives, both take a detour through a saddle point. However, Adagrad takes considerably fewer steps.\n",
    " \n",
    " <img src=\"figures/gd_vs_adag.png\" width=\"450\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
