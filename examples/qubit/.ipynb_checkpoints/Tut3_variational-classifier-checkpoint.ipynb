{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3 - Variational classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tutorial 3 we considerably switch gears and show how to use openqml to implement variational classifiers - quantum circuits that can be trained from labelled data how to classify new data samples. \n",
    "\n",
    "Inspired by Farhi & Neven (2018), we will first train a variational classifier that encodes bitstring-inputs as basis states (*basis-encoding*) to learn the parity function\n",
    "\n",
    "$$ f: x \\in \\{0,1\\}^{\\otimes n} \\rightarrow y = \\begin{cases} 1 \\text{  if uneven number of ones in } x \\\\ 0 \\text{ else} \\end{cases}.$$\n",
    "\n",
    "We then train a variational classifier that encodes real vectors as amplitude vectors (*amplitude encoding*) after Schuld, Bocharov, Wiebe and Svore (2018) on the first two classes of flowers in the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Learning the parity function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:29:57 WARNING No OpenQML configuration file found.\n"
     ]
    }
   ],
   "source": [
    "import openqml as qm\n",
    "from openqml import numpy as onp\n",
    "import numpy as np\n",
    "from openqml._optimize import GradientDescentOptimizer\n",
    "\n",
    "from math import isclose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a quantum device with four \"wires\" or qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev = qm.device('projectq.simulator', wires=2)\n",
    "dev = qm.device('default.qubit', wires=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational classifiers usually define a \"layer\" or \"block\", which is an elementary circuit architecture that gets repeated to build the variational circuit.\n",
    "\n",
    "<IMage>\n",
    "\n",
    "Our circuit layer consists of an arbitrary rotation on every qubit, as well as CNOTs that entangle each qubit with its neighbour.\n",
    "\n",
    "<IMage>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(W):\n",
    "\n",
    "    qm.Rot(W[0, 0], W[0, 1], W[0, 2], [0])\n",
    "    qm.Rot(W[1, 0], W[1, 1], W[1, 2], [1])\n",
    "    qm.Rot(W[2, 0], W[2, 1], W[2, 2], [2])\n",
    "    qm.Rot(W[3, 0], W[3, 1], W[3, 2], [3])\n",
    "\n",
    "    qm.CNOT([0, 1])\n",
    "    qm.CNOT([1, 2])\n",
    "    qm.CNOT([2, 3])\n",
    "    qm.CNOT([3, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to encode data inputs $x$ into the circuit, so that the measured output depends on the inputs. In this first example, the inputs are bitstrings, which we encode into the state of the qubits. The quantum state $|\\psi \\rangle $ after state preparation is a computational basis state that has 1's where $x$ has 1s, for example\n",
    "\n",
    "$$ x = 0101 \\rightarrow |\\psi \\rangle = |0101 \\rangle . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statepreparation(x):\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 1:\n",
    "            qm.PauliX([i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can elegantly define the quantum function as a state preparation routine, followed by a repitition of the layer structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qm.qfunc(dev)\n",
    "def variational_classifier(weights, x=None):\n",
    "\n",
    "    statepreparation(x)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qm.expectation.PauliZ(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different from previous tutorials, the quantum function takes the data as a keyword argument `x=None`. A quantum function interprets all positional arguments as trainable variables, while keyword arguments stay fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective or cost in supervised learning is the sum of a loss and a regularizer. We use the standard square-loss that measures the distance between target labels and model predictions, as well as a standard l2 regularizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss += (l-p)**2\n",
    "    loss = loss/len(labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def regularizer(weights):\n",
    "    w_flat = weights.flatten()\n",
    "    return onp.abs(onp.inner(w_flat, w_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor how many inputs the current classifier predicted correctly, we also define the accuracy given target labels and model predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if isclose(l, p, abs_tol=1e-5):\n",
    "            loss += 1\n",
    "    loss = loss/len(labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning tasks, the cost depends on the data - here the features and labels considered in the iteration of the optimization routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(weights, features, labels):\n",
    "\n",
    "    predictions = [variational_classifier(weights, x=x) for x in features]\n",
    "\n",
    "    return square_loss(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load and preprocess some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [0. 0. 0. 0.], Y = -1.0\n",
      "X = [0. 0. 0. 1.], Y = 1.0\n",
      "X = [0. 0. 1. 0.], Y = 1.0\n",
      "X = [0. 0. 1. 1.], Y = -1.0\n",
      "X = [0. 1. 0. 0.], Y = 1.0\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"parity.txt\")\n",
    "X = data[:, :-1]\n",
    "Y = data[:, -1]\n",
    "Y = Y*2 - np.ones(len(Y)) # shift label from {0, 1} to {-1, 1}\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"X = {}, Y = {}\".format(X[i], Y[i]))\n",
    "print(\"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = len(X)\n",
    "num_train = int(0.75*num_data)\n",
    "index = np.random.permutation(range(num_data))\n",
    "X_train = X[index[: num_train]]\n",
    "Y_train = Y[index[: num_train]]\n",
    "X_val = X[index[num_train: ]]\n",
    "Y_val = Y[index[num_train: ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize the weights randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15794402,  0.15587718,  0.59409424,  0.89763754],\n",
       "       [-1.18172679,  0.95780576, -2.28003724, -1.02185815],\n",
       "       [-0.5345913 ,  2.49280043, -1.10889838, -0.04057916],\n",
       "       [ 0.72451273,  0.43064806,  0.49004388, -0.22403556]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_qubits = 4\n",
    "num_layers = 5\n",
    "weights0 = [np.random.randn(num_qubits, num_qubits)] * num_layers\n",
    "\n",
    "weights0[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an optimizer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = GradientDescentOptimizer(0.01)\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and train the optimizer. We track the accuracy - the share of correctly classified data samples - on the training and validation set. For this we compute the outputs of the variational classifier and turn them into predictions in $\\{-1,1\\}$ by taking the sign of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.0070080 | Acc train: 0.3333333 | Acc validation: 1.0000000 \n",
      "Iter:     2 | Cost: 1.0371616 | Acc train: 0.3333333 | Acc validation: 1.0000000 \n",
      "Iter:     3 | Cost: 1.0662509 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n",
      "Iter:     4 | Cost: 1.0082665 | Acc train: 0.3333333 | Acc validation: 1.0000000 \n",
      "Iter:     5 | Cost: 1.0279679 | Acc train: 0.3333333 | Acc validation: 1.0000000 \n",
      "Iter:     6 | Cost: 1.0192874 | Acc train: 0.3333333 | Acc validation: 1.0000000 \n",
      "Iter:     7 | Cost: 1.0019231 | Acc train: 0.3333333 | Acc validation: 1.0000000 \n",
      "Iter:     8 | Cost: 1.0193524 | Acc train: 0.3333333 | Acc validation: 1.0000000 \n",
      "Iter:     9 | Cost: 1.0537765 | Acc train: 0.3333333 | Acc validation: 1.0000000 \n",
      "Iter:    10 | Cost: 1.0000134 | Acc train: 0.3333333 | Acc validation: 1.0000000 \n"
     ]
    }
   ],
   "source": [
    "weights = np.array(weights0)\n",
    "for iteration in range(10):\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size, ))\n",
    "    X_train_batch = X_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights = o.step(lambda w: cost(w, X_train_batch, Y_train_batch), weights)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = [np.sign(variational_classifier(weights, x=x)) for x in X_train]\n",
    "    predictions_val = [np.sign(variational_classifier(weights, x=x)) for x in X_val]\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "\n",
    "    print(\"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "          \"\".format(iteration+1, cost(weights, X, Y), acc_train, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Iris classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change from bit-string inputs to real-valued vectors, we simply overwrite the statepreparation function. Since we use four qubits, the quantum state has 16 amplitudes, and we encode $x$ into the first two qubits and again in the last two qubits. \n",
    "\n",
    "To save simulating a lengthy state preparation routine we use a hack: Compute $x \\otimes x$ manually and set the amplitude vector to this value. *Note: Since we normalized $x$ earlier, $x \\otimes x$ is also normalized!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statepreparation(x):\n",
    "\n",
    "    qm.QubitStateVector(onp.kron(x, x), wires=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the Iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X= [0.80377277 0.55160877 0.22064351 0.0315205 ], Y = -1.0\n",
      "X= [0.82813287 0.50702013 0.23660939 0.03380134], Y = -1.0\n",
      "X= [0.80533308 0.54831188 0.2227517  0.03426949], Y = -1.0\n",
      "X= [0.80003025 0.53915082 0.26087943 0.03478392], Y = -1.0\n",
      "X= [0.790965  0.5694948 0.2214702 0.0316386], Y = -1.0\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"iris.txt\")\n",
    "X = data[:, :-1]\n",
    "normalization = np.sqrt(np.sum(X ** 2, -1))\n",
    "X = (X.T / normalization).T  # normalize each feature vector\n",
    "Y = data[:, -1]\n",
    "Y = Y*2 - np.ones(len(Y))\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"X= {}, Y = {}\".format(X[i], Y[i]))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we optimize the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 24.5937983 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n",
      "Iter:     2 | Cost: 1.8612966 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n",
      "Iter:     3 | Cost: 1.0032951 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n",
      "Iter:     4 | Cost: 2.7619727 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n",
      "Iter:     5 | Cost: 7.8998817 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n",
      "Iter:     6 | Cost: 1.0000000 | Acc train: 0.0000000 | Acc validation: 0.0000000 \n",
      "Iter:     7 | Cost: 1.4898075 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n",
      "Iter:     8 | Cost: 1.1322511 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n",
      "Iter:     9 | Cost: 47.7720743 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n",
      "Iter:    10 | Cost: 3.5544347 | Acc train: 0.6666667 | Acc validation: 0.0000000 \n"
     ]
    }
   ],
   "source": [
    "o = GradientDescentOptimizer(0.01)\n",
    "batch_size = 3\n",
    "\n",
    "weights = np.array(weights0)\n",
    "for iteration in range(10):\n",
    "\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size, ))\n",
    "    X_train_batch = X_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights = o.step(lambda w: cost(w, X_train_batch, Y_train_batch), weights)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = [np.sign(variational_classifier(weights, x=x)) for x in X_train]\n",
    "    predictions_val = [np.sign(variational_classifier(weights, x=x)) for x in X_val]\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "\n",
    "    print(\"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "          \"\".format(iteration+1, cost(weights, X, Y), acc_train, acc_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Plot decision landscape!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
