
1. Are you using variational circuits or could you imagine using them for your work? 
If yes:
2. What is the task you'd use the circuits for?
3. What does the circuit look like (i.e., Xanadu layers and homodyne measurement)? Do you classically postprocess the result in any way (i.e., threshold a value)?
4. What is the function you want to optimise?
5. What values would you like to force to stay in a certain regime with a penalty? (i.e. squeezing should be low, trace should be 1...)
6. Which values would you want to monitor during learning? (I.e. the trace of the state, the loss,...)
7. What optimisation methods are interesting for you? (i.e. Nelder-Mead and numeric mehtods, autodiff with tensorflow, other...)


Josh:

1. Yes
2. The task I am currently looking at is using variational circuits to learn the cubic phase gate. I've also worked a bit with Krishna, looking at learning a state used in the preparation of the cubic phase gate

3. In the case of the cubic phase unitary, I looked at optimizing the unitary matrix over the ladder operators, that corresponds to exp(i\gamma x^3). This was done using 10 Nathan layers. In the case of Krishna's circuit, he was restricted by design to a specific architecture: 2 input squeeze states and a Gaussian state, two beamsplitters, and then two post-selected homodyne measurements on the first two modes.
4. *Cubic phase gate:* The unitary I would like to learn is <n|exp(i\gamma x^3)|m>, where n and m are number states in the range 0<=n,m<=cutoff
*Krishna:* in Krishna's case, we were attempting to learn the state |0>+a_0i\sqrt{2/3} |3>
5. *Cubic phase gate*: Trace should be low, squeezing should be low etc. This is due to the additional numeric error when using the Fock backends.
*Krishna*: In his case, the issue we ran into the most was that we required all states |n>, n!=0,3 to have a amplitude of 0. In effect, we found that adding this to the cost function with a high weight enabled us to get this result, but reduced the fidelity regarding the coefficients of |0> and |3> we wanted (a_0 should be of the order 0.01, but was often of higher amplitude than |0> after normalisation)
6. The values I would like to monitor during learning: norm of the unitary operator, trace of the system, coefficients of the state in the number basis. Things that would be cool but are not currently implemented include learning based on the wigner function (the current wigner function would have to be rewritten for symbolic tensorflow support).
I actually was speaking to Nathan about this a couple of months ago - at the moment, the tf backend in strawberry fields is very well suited for state learning, but a bit harder for unitary learning (from my experience). It would be great to have a method that 'extracts' the unitary applied by the backend for analysis/optimization purposes. At the moment, I am using batch mode to calculate each row of the unitary.
7. I'm too new to optimization and tensorflow to be able to answer this question in depth!
oh
another thing that crossed my mind
it would be super nice to have the tf.variable/tf.placeholder/optimization steps/parameter regularization etc. abstracted away by OpenQML, in the same way OpenFermion abstracts away a lot of the Hamiltonian/qubit transformations

-------------------------------------------
Tom:

1. Are you using variational circuits or could you imagine using them for your work?
Yes!

If yes:
2. What is the task you'd use the circuits for?
I have been doing a generative model and a classifier for MNIST. It would be great in future if we can easily apply to all the other well-established datasets without too much overhead. Would be cool to be able to pick an encoding (e.g. fock basis, x-displacement etc). I am also working with JM on state/gate/measurement synthesis (which is more fitting than ML) - actually maybe we should send you that code. It would be great to just be able to list a state/gate in the fock basis and the fitting is automated.

3. What does the circuit look like (i.e., Xanadu layers and homodyne measurement)? Do you classically postprocess the result in any way (i.e., threshold a value)?
All my circuits have many ~Nath~ Xanadu layers. The MNIST stuff encodes in a 2 mode fock basis: for the generative we have random displacements at input while for the classifier we have a hack to prepare an MNIST state.

4. What is the function you want to optimise?
MNIST generative: We are maximising the log likelihood (explained more in the shared overleaf)
MNIST classifier: In my first encoding I was using the photon number in the first mode to output the class, and so was minimising the difference from the simulated circuit photon number to the label photon number. This wasn't working so well, so now I am looking at the average total photon number.
State synthesis: Actually this is JM's stuff!
Gate synthesis: We have a unitary defined in the Fock basis up to a cut off, and fit the output of multiple batch runs of the circuit to each column of the unitary.

5. What values would you like to force to stay in a certain regime with a penalty? (i.e. squeezing should be low, trace should be 1...)
For the MNIST stuff, I am ensuring that the output is always normalised.
For the synthesis stuff, we don't need to worry about normalisation as we are fitting the circuit output to be normalised anyway.
I always clip the values of any parameters in photon number generating gates
When I initialise variables for photon number generating gates, I do it randomly with zero mean and a small standard deviation. Is the SD is too small, the cost function plateaus, while is the SD is large then we struggle with regularisation at first.

6. Which values would you want to monitor during learning? (I.e. the trace of the state, the loss,...)
Generally I like to monitor the cost, the trace, and the optimisation function (this way we can see whether the decreasing cost is due to optimising the trace or the function itself). For the generative, I monitor all the images for some fixed displacements. For the classifier I monitor the classification performance on some sample training data.

7. What optimisation methods are interesting for you? (i.e. Nelder-Mead and numeric mehtods, autodiff with tensorflow, other...)
This is a bit where I need to learn more I guess! I am just doing gradient descent with the Adam optimiser. 

_____________________________________________

Nic

1. I have used them in the past for a very simple task that was more of a simple teach myself something rather than research. The task was the following: Given a fixed input ket |\psi> and a beam splitter BS(\theta, \phi =0 ) what is the optimal ket |\gamma> that I can send in the other arm of the beam splitter to minimize the entanglement (as measured by the linear entropy) of the output two mode state.

so as for number:
2. Find the optimal ket for the task specified in point 1.
3. Initial random state preparation + Beam splitter with another fixed mode
4. The entanglement of the output modes
5. for this example, the norm of the ket at each optimization step and also I wanted to make the mean displacement of the to-be-optimized ket equal to zero
6. The entanglement measure
7. Gradien based methods. Would also like to couple gradients with random search a la basin hopping.

________________________________________

Pierre-Luc

We have to nail all the details but it will be some kind of sequence of layers of classical and quantum networks (GBS to be precise), so closer to "super-classical" networks than fully quantum networks. We expect it should give us some advantage over classical neural nets even if we don't have the complete toolbox of universal qc (with cubic phase gates). 

----------------------------------------

Krishna

1 - Yes 
2- mainly state preparation but I have two applications - one is conditional and one is not
3 - one is xanadu layer, but i have a few other 'layers' i would like to try out, and also with photon counting measurements
4 - mainly fidelity with target state but usually with few other qualifications like specific fock states to be supressed and so on, because fidelity sometimes seems to be too broad
5 - squeezing, kerr strength are most important i have used for my current application
6 - mainly fidelity for now
7 - not familiar with this, i am currently using codes written by miguel and tom. i am new to this space.





