
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  
  <!-- Licensed under the Apache 2.0 License -->
  <link rel="stylesheet" type="text/css" href="../../_static/fonts/open-sans/stylesheet.css" />
  <!-- Licensed under the SIL Open Font License -->
  <link rel="stylesheet" type="text/css" href="../../_static/fonts/source-serif-pro/source-serif-pro.css" />
  <link href="https://fonts.googleapis.com/css?family=Noto+Serif" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="../../_static/css/bootstrap.min.css" />
  <link rel="stylesheet" type="text/css" href="../../_static/css/bootstrap-theme.min.css" />
  <link rel="stylesheet" type="text/css" href="../../_static/css/nanoscroller.css" />
  <script defer src="https://use.fontawesome.com/releases/v5.0.2/js/all.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
       TeX: {
         Macros: {
           pr : ['|\#1\\rangle\\langle\#1|',1],
           ket: ['\\left| \#1\\right\\rangle',1],
           bra: ['\\left\\langle \#1\\right|',1],
           xket: ['\\left| \#1\\right\\rangle_x',1],
           xbra: ['\\left\\langle \#1\\right|_x',1],
           braket: ['\\langle \#1 \\rangle',1],
           braketD: ['\\langle \#1 \\mid \#2 \\rangle',2],
           braketT: ['\\langle \#1 \\mid \#2 \\mid \#3 \\rangle',3],
           ketbra: ['| #1 \\rangle \\langle #2 |',2],
           hc: ['\\text{h.c.}',0],
           cc: ['\\text{c.c.}',0],
           h: ['\\hat',0],
           nn: ['\\nonumber',0],
           di: ['\\frac{d}{d \#1}',1],
           uu: ['\\mathcal{U}',0],
           inn: ['\\text{in}',0],
           out: ['\\text{out}',0],
           vac: ['\\text{vac}',0],
           I: ['I',0],
           x: ['\\hat{x}',0],
           p: ['\\hat{p}',0],
           a: ['\\hat{a}',0],
           ad: ['\\hat{a}^\\dagger',0],
           n: ['\\hat{n}',0],
           nbar: ['\\overline{n}',0],
           sech: ['\\mathrm{sech~}',0],
           tanh: ['\\mathrm{tanh~}',0],
           re: ['\\text{Re}',0],
           im: ['\\text{Im}',0],
           tr: ['\\mathrm{Tr} #1',1],
           sign: ['\\text{sign}',0],
           overlr: ['\\overset\\leftrightarrow{\#1}',1],
           overl: ['\\overset\leftarrow{\#1}',1],
           overr: ['\\overset\rightarrow{\#1}',1],
           avg: ['\\left< \#1 \\right>',1],
           slashed: ['\\cancel{\#1}',1],
           bold: ['\\boldsymbol{\#1}',1],
           d: ['\\mathrm d',0],
           expect: ["\\langle #1 \\rangle",1],
           pde: ["\\frac{\\partial}{\\partial \#1}",1],
           R: ["\\mathbb{R}",0],
           C: ["\\mathbb{C}",0],
           Ad: ["\\text{Ad}",0],
           Var: ["\\text{Var}",0],
           bx: ["\\mathbf{x}", 0],
           bm: ["\\boldsymbol{\#1}",1]
         }
       }
     });
     </script>
  
    <title>openqml.core &#8212; OpenQML 0.1.0 documentation</title>
    <link rel="stylesheet" href="../../_static/xanadu.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  
  <link rel="apple-touch-icon" href="../../_static/logo_new.png" />
  
   

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenQML 0.1.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" accesskey="U">Module code</a> &#187;</li> 
      </ul>
    </div>
    <div class="container-wrapper">

      <div id="mobile-toggle">
        <a href="#"><span class="glyphicon glyphicon-align-justify" aria-hidden="true"></span></a>
      </div>
  <div id="left-column" class="nano css-transitions-only-after-page-load">
    <div class="sphinxsidebar nano-content"><a href="http://xanadu.ai" class="text-logo">
	<img src="../../_static/logo_new.png" class="logo active" width=80%></img>
	<img src="../../_static/logo_new_small.png" class="logo-small" width=80%></img>
	<!-- <span id="project-name">OpenQML</span> -->
</a>
<div class="sidebar-block search-block">
  <div class="sidebar-wrapper">
  <!-- <div id="project-name">OpenQML</div> -->
    <a href="
    ../../index.html">
      <div id="project-name">
        <!-- <img src="../../_static/strawberry_fields.png" width=100%></img> -->
        <p style="font-size:28px;margin:0;padding:0;line-height:1;text-transform:none!important">OpenQML</p>
      </div>
    </a>
    <div id="main-search">
      <form class="form-inline" action="../../search.html" method="GET" role="form">
        <div class="input-group">
          <input name="q" type="text" class="form-control" placeholder="Search...">
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div>
<div class="sidebar-block">
<!--   <div class="sidebar-wrapper">
    <h2>Table Of Contents</h2>
  </div> -->
  <div class="sidebar-toc">
    
    
      <p class="caption"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installing.html">Installation and downloads</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../research.html">Research and contribution</a></li>
</ul>
<p class="caption"><span class="caption-text">Concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autograd_quantum.html">Automatic differentiation of quantum functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../conventions.html">Conventions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references.html">References and further reading</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../core.html">Core classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../circuit.html">Quantum circuits</a></li>
</ul>
<p class="caption"><span class="caption-text">Plugins</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../plugins.html">Plugin system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../plugins/included_plugins.html">Included plugins</a></li>
</ul>

    
  </div>
</div>
      
    </div>
  </div>
        <div id="content">
          <div id="right-column">
            
            <div role="navigation" aria-label="breadcrumbs navigation">
              <ol class="breadcrumb">
                <li><a href="../../index.html">Docs</a></li>
                
                  <li><a href="../index.html">Module code</a></li>
                
                <li>openqml.core</li>
              </ol>
            </div>
            
            <div class="document clearer body">
              
  <h1>Source code for openqml.core</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2018 Xanadu Quantum Technologies Inc.</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Core classes</span>
<span class="sd">============</span>

<span class="sd">**Module name:** :mod:`openqml.core`</span>

<span class="sd">.. currentmodule:: openqml.core</span>


<span class="sd">The :class:`Optimizer` class is based on MLtoolbox by Maria Schuld.</span>



<span class="sd">Classes</span>
<span class="sd">-------</span>

<span class="sd">.. autosummary::</span>
<span class="sd">   Optimizer</span>


<span class="sd">Optimizer methods</span>
<span class="sd">-----------------</span>

<span class="sd">.. currentmodule:: openqml.core.Optimizer</span>

<span class="sd">.. autosummary::</span>
<span class="sd">   set_hp</span>
<span class="sd">   weights</span>
<span class="sd">   train</span>


<span class="sd">Optimizer private methods</span>
<span class="sd">-------------------------</span>

<span class="sd">.. autosummary::</span>
<span class="sd">   _optimize_SGD</span>
<span class="sd">   _reg_cost_L2</span>

<span class="sd">----</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">signal</span>
<span class="kn">import</span> <span class="nn">logging</span> <span class="k">as</span> <span class="nn">log</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="k">import</span> <span class="n">minimize</span><span class="p">,</span> <span class="n">OptimizeResult</span>


<span class="c1"># optimization parameters</span>
<span class="c1">#Par = namedtuple(&#39;Par&#39;, &#39;name, init, regul&#39;)</span>
<span class="c1">#Par.__new__.__defaults__ = (&#39;&#39;, 0, False)</span>


<div class="viewcode-block" id="StopOptimization"><a class="viewcode-back" href="../../core.html#openqml.core.StopOptimization">[docs]</a><span class="k">class</span> <span class="nc">StopOptimization</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="s2">&quot;Exception for stopping the optimization.&quot;</span>
    <span class="k">pass</span></div>


<span class="n">OPTIMIZER_NAMES</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;SGD&quot;</span><span class="p">,</span> <span class="s2">&quot;Nelder-Mead&quot;</span><span class="p">,</span> <span class="s2">&quot;Powell&quot;</span><span class="p">,</span> <span class="s2">&quot;CG&quot;</span><span class="p">,</span> <span class="s2">&quot;BFGS&quot;</span><span class="p">,</span> <span class="s2">&quot;Newton-CG&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;L-BFGS-B&quot;</span><span class="p">,</span> <span class="s2">&quot;TNC&quot;</span><span class="p">,</span> <span class="s2">&quot;COBYLA&quot;</span><span class="p">,</span> <span class="s2">&quot;SLSQP&quot;</span><span class="p">,</span> <span class="s2">&quot;dogleg&quot;</span><span class="p">,</span> <span class="s2">&quot;trust-ncg&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;trust-exact&quot;</span><span class="p">,</span> <span class="s2">&quot;trust-krylov&quot;</span><span class="p">]</span>


<div class="viewcode-block" id="Optimizer"><a class="viewcode-back" href="../../core.html#openqml.core.Optimizer">[docs]</a><span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Quantum circuit optimizer.</span>

<span class="sd">    Optimization hyperparameters are given as keyword arguments.</span>

<span class="sd">    Args:</span>
<span class="sd">      cost_func (callable): Cost/error function. Typically involves the evaluation of one or more :class:`~openqml.circuit.QNode` instances</span>
<span class="sd">        representing variational quantum circuits. Takes two arguments: weights (array[float]) and optionally a batch of data item indices (Sequence[int]).</span>
<span class="sd">      cost_grad (callable): Gradient of the cost/error function with respect to the weights. Takes the same argumets as ``cost_func``.</span>
<span class="sd">        Typically obtained using autograd as :code:`cost_grad = autograd.grad(cost_func, 0)`.</span>
<span class="sd">      weights (array[float]): initial values for the weights/optimization parameters</span>
<span class="sd">      n_data (int): total number of data samples to be used in training</span>

<span class="sd">    Keyword Args:</span>
<span class="sd">      optimizer (str): &#39;SGD&#39; or any optimizer not requiring a Hessian, compatible with :func:`scipy.optimize.minimize`: &#39;Nelder-Mead&#39;, &#39;Powell&#39;, &#39;CG&#39;, &#39;BFGS&#39;, &#39;L-BFGS-B&#39;, &#39;TNC&#39;, &#39;SLSQP&#39;</span>
<span class="sd">      lambda (float): regularization strength</span>
<span class="sd">      regularizer (None, callable):  None, &#39;L2&#39;, or a custom function mapping Sequence[float] to float.</span>
<span class="sd">      init_learning_rate (float): SGD only: initial learning rate, usually around 0.1</span>
<span class="sd">      decay  (float): SGD only: decay rate for the learning rate</span>
<span class="sd">      batch_size (None, int): SGD only: How many randomly chosen data samples to include in computing the cost function each iteration. None means all of them.</span>
<span class="sd">      print_every (int): add a status entry into the log every print_every iterations</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;String representation.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cost_func</span><span class="p">,</span> <span class="n">cost_grad</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">n_data</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_cost_func</span> <span class="o">=</span> <span class="n">cost_func</span>  <span class="c1">#: callable: scalar function to be minimized</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cost_grad</span> <span class="o">=</span> <span class="n">cost_grad</span>  <span class="c1">#: callable: gradient of _cost_func</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_n_data</span> <span class="o">=</span> <span class="n">n_data</span>        <span class="c1">#: int: total number of data samples to be used in training</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop</span> <span class="o">=</span> <span class="kc">False</span>            <span class="c1">#: bool: flag, stop optimization</span>

        <span class="c1"># default hyperparameters</span>
        <span class="n">default_hp</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="s1">&#39;SGD&#39;</span><span class="p">,</span>
                      <span class="s1">&#39;init_learning_rate&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
                      <span class="s1">&#39;decay&#39;</span><span class="p">:</span> <span class="mf">0.03</span><span class="p">,</span>
                      <span class="s1">&#39;lambda&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
                      <span class="s1">&#39;regularizer&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                      <span class="s1">&#39;batch_size&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                      <span class="s1">&#39;print_every&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span> <span class="o">=</span> <span class="n">default_hp</span>    <span class="c1">#: dict[str-&gt;*]: hyperparameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># update with user-given hyperparameters</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;HYPERPARAMETERS:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">):</span>
            <span class="n">temp</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span> <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="s1">&#39; (default)&#39;</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:20s}{!s:10s}{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">temp</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">()</span>

        <span class="n">temp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">callable</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">temp</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">OPTIMIZER_NAMES</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The optimizer has to be either a callable or in the list of allowed optimizers, </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">OPTIMIZER_NAMES</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">temp</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;Nelder-Mead&#39;</span><span class="p">,</span> <span class="s1">&#39;Powell&#39;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">cost_grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> does not use a gradient function.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">temp</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;The weights must be given as a 1d array.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">weights</span>  <span class="c1">#: array[float]: optimization parameters</span>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s2">&quot;Current weights.&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span>


<div class="viewcode-block" id="Optimizer.set_hp"><a class="viewcode-back" href="../../core.html#openqml.core.Optimizer.set_hp">[docs]</a>    <span class="k">def</span> <span class="nf">set_hp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set hyperparameter values.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="Optimizer._reg_cost_L2"><a class="viewcode-back" href="../../core.html#openqml.core.Optimizer._reg_cost_L2">[docs]</a>    <span class="k">def</span> <span class="nf">_reg_cost_L2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;L2 regularization cost.</span>

<span class="sd">        Args:</span>
<span class="sd">          weights (array[float]): optimization parameters</span>
<span class="sd">          grad (bool): should we return the gradient of the regularization cost instead?</span>

<span class="sd">        Returns:</span>
<span class="sd">          float, array[float]: regularization cost or its gradient</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">grad</span><span class="p">:</span>
            <span class="c1"># gradient with respect to each weight</span>
            <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;lambda&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">weights</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;lambda&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span></div>


<div class="viewcode-block" id="Optimizer._optimize_SGD"><a class="viewcode-back" href="../../core.html#openqml.core.Optimizer._optimize_SGD">[docs]</a>    <span class="k">def</span> <span class="nf">_optimize_SGD</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Stochastic Gradient Descent optimization.</span>

<span class="sd">        Args:</span>
<span class="sd">          x0 (array[float]): initial values for the optimization parameters</span>
<span class="sd">          max_steps (int): maximum number of iterations for the algorithm</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;init_learning_rate&#39;</span><span class="p">]</span>
        <span class="n">decay</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;decay&#39;</span><span class="p">]</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;batch_size&#39;</span><span class="p">]</span>
        <span class="n">print_every</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;print_every&#39;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">batch_size</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">_n_data</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Batch size cannot be larger than the total number of data samples.&#39;</span><span class="p">)</span>

        <span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
        <span class="n">success</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;Requested number of iterations finished.&#39;</span>

        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Global step       Cost  Learn. rate&#39;</span><span class="p">)</span>
        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;-----------------------------------&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">global_step</span><span class="p">,</span> <span class="n">global_step</span> <span class="o">+</span> <span class="n">max_steps</span><span class="p">):</span>
            <span class="c1"># generate a random batch of data samples</span>
            <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_data</span><span class="p">)</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">perm</span><span class="p">[:</span><span class="n">batch_size</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="c1"># take a step against the gradient  TODO does not ensure that the cost goes down, should it?</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">err_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
            <span class="n">decayed_lr</span> <span class="o">=</span> <span class="n">init_lr</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span><span class="n">decay</span><span class="o">*</span><span class="n">step</span><span class="p">)</span>
            <span class="n">x</span> <span class="o">-=</span> <span class="n">decayed_lr</span> <span class="o">*</span> <span class="n">grad</span>
            <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">err_func</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

            <span class="c1">#self._weights = x  # store the current weights</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:11d}</span><span class="s1"> </span><span class="si">{:10.6g}</span><span class="s1"> </span><span class="si">{:12.6g}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">cost</span><span class="p">,</span> <span class="n">decayed_lr</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">:</span>
                <span class="n">success</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;User stop.&#39;</span>
                <span class="k">break</span>

        <span class="k">return</span> <span class="n">OptimizeResult</span><span class="p">({</span><span class="s1">&#39;success&#39;</span><span class="p">:</span> <span class="n">success</span><span class="p">,</span>
                               <span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
                               <span class="s1">&#39;nit&#39;</span><span class="p">:</span> <span class="n">step</span><span class="o">-</span><span class="n">global_step</span><span class="p">,</span>
                               <span class="s1">&#39;message&#39;</span><span class="p">:</span> <span class="n">msg</span><span class="p">})</span></div>


<div class="viewcode-block" id="Optimizer.train"><a class="viewcode-back" href="../../core.html#openqml.core.Optimizer.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Optimize the system.</span>

<span class="sd">        Args:</span>
<span class="sd">          max_steps (int): maximum number of steps for the algorithm</span>
<span class="sd">        Returns:</span>
<span class="sd">          float: final cost function value</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;regularizer&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">err_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">err_grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">err_func</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost_func</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span> <span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">_reg_cost_L2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">err_grad</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="o">=</span><span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span> <span class="o">+</span><span class="bp">self</span><span class="o">.</span><span class="n">_reg_cost_L2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cost_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">err_grad</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">x0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span>  <span class="c1"># initial weights</span>
        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Initial cost: </span><span class="si">{:.6g}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">err_func</span><span class="p">(</span><span class="n">x0</span><span class="p">)))</span>

        <span class="k">def</span> <span class="nf">signal_handler</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span> <span class="n">frame</span><span class="p">):</span>
            <span class="s2">&quot;Called when SIGINT is received, for example when the user presses ctrl-c.&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stop</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="c1"># catch ctrl-c gracefully</span>
        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGINT</span><span class="p">,</span> <span class="n">signal_handler</span><span class="p">)</span>

        <span class="n">optimizer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;optimizer&#39;</span><span class="p">]</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;SGD&#39;</span><span class="p">:</span>   <span class="c1"># stochastic gradient descent</span>
                <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimize_SGD</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">max_steps</span><span class="p">)</span>

            <span class="k">elif</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">OPTIMIZER_NAMES</span><span class="p">:</span>
                <span class="n">print_every</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hp</span><span class="p">[</span><span class="s1">&#39;print_every&#39;</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">nit</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1">#: int: number of iterations performed</span>
                <span class="k">def</span> <span class="nf">callback</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">x</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nit</span> <span class="o">%</span> <span class="n">print_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:9d}</span><span class="s1"> </span><span class="si">{:10.6g}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nit</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">err_func</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">nit</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">StopOptimization</span><span class="p">(</span><span class="s1">&#39;User stop.&#39;</span><span class="p">)</span>

                <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Iteration       Cost&#39;</span><span class="p">)</span>
                <span class="n">log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;--------------------&#39;</span><span class="p">)</span>
                <span class="n">opt</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">err_func</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">err_grad</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span>
                               <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;maxiter&#39;</span><span class="p">:</span> <span class="n">max_steps</span><span class="p">,</span> <span class="s1">&#39;disp&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Unknown optimisation method &#39;</span><span class="si">{}</span><span class="s2">&#39;.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>

        <span class="k">except</span> <span class="n">StopOptimization</span> <span class="k">as</span> <span class="n">exc</span><span class="p">:</span>
            <span class="c1"># TODO the callback should maybe store more optimization information than just the last x</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Optimisation successful: False&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of iterations performed: &quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">nit</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reason for termination: &quot;</span><span class="p">,</span> <span class="n">exc</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_weights</span> <span class="o">=</span> <span class="n">opt</span><span class="o">.</span><span class="n">x</span>

            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Optimisation successful: &quot;</span><span class="p">,</span> <span class="n">opt</span><span class="o">.</span><span class="n">success</span><span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of iterations performed: &quot;</span><span class="p">,</span> <span class="n">opt</span><span class="o">.</span><span class="n">nit</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of iterations performed: Not applicable to solver.&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Final parameters: &quot;</span><span class="p">,</span> <span class="n">opt</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reason for termination: &quot;</span><span class="p">,</span> <span class="n">opt</span><span class="o">.</span><span class="n">message</span><span class="p">)</span>

        <span class="n">cost</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">err_func</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_weights</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Final cost: </span><span class="si">{:.6g}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>

        <span class="c1"># restore default handler</span>
        <span class="n">signal</span><span class="o">.</span><span class="n">signal</span><span class="p">(</span><span class="n">signal</span><span class="o">.</span><span class="n">SIGINT</span><span class="p">,</span> <span class="n">signal</span><span class="o">.</span><span class="n">SIG_DFL</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">cost</span></div></div>
</pre></div>

            </div>
              
          </div>
          <div class="clearfix"></div>
        </div>


    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenQML 0.1.0 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" >Module code</a> &#187;</li> 
      </ul>
    </div>
<script type="text/javascript">
  $("#mobile-toggle a").click(function () {
    $("#left-column").toggle();
  });
</script>
<script type="text/javascript" src="../../_static/js/bootstrap.js"></script>
<script type="text/javascript" src="../../_static/js/nanoscroller.min.js"></script>
<script type="text/javascript">
    $(".nano").nanoScroller();
</script>
<script type="text/javascript">
    if ($('.current').length) {
        var target = $('.current')[0]
        var rect = target.getBoundingClientRect();
        if (rect.bottom > window.innerHeight) {
            $('.text-logo').addClass('active');
            $('.logo').removeClass('active');
            $('.logo-small').addClass('active');
            $(".nano").nanoScroller({ scrollTo: $('.current') });
        } else {
            // $('.text-logo').addClass('active');
            // $('.logo').removeClass('active');
            // $('.logo-small').addClass('active');
            $(".nano").nanoScroller({ scrollTop: 93 });
        }
    }
    if ($('#search-documentation').length) {} else {
        $(".nano").on("update", function(event, values){
            $('.text-logo').toggleClass('active', $('.sphinxsidebar').scrollTop() > 0);
            var act = $('.text-logo').hasClass('active');
            if(act){
                $('.logo').removeClass('active');
                $('.logo-small').addClass('active');
            }
        });
    }
    $(".nano").bind("scrolltop", function(e){
        $('.logo-small').removeClass('active');
        $('.logo').addClass('active');
    });
    $(document).ready(function () {
        $(".css-transitions-only-after-page-load").each(function (index, element) {
            setTimeout(function () { $(element).removeClass("css-transitions-only-after-page-load") }, 10);
        });
    });
</script>
  <div class="footer">
    Copyright 2018, Xanadu Inc. Documentation created using <a href="http://sphinx.pocoo.org/">Sphinx</a>.
  </div>
  </body>
</html>