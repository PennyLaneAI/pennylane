# This workflow is designed to run unit tests for all interfaces of the PennyLane library on GitHub Actions. 
# It allows for customization of various parameters such as the branch to checkout, the job name prefix and suffix, 
# the PyTest coverage flags, additional PyTest arguments, and the option to run a lightened version of the CI. 
# The workflow includes multiple jobs that run tests for different interfaces such as Torch, Autograd, TensorFlow, JAX, and more. 
# It also includes a job to upload the coverage report to Codecov. 
# The workflow utilizes matrix strategies to parallelize the test runs and supports skipping specific jobs on a lightened CI run.

name: Unit Test - All Interfaces
on:
  workflow_call:
    secrets:
      codecov_token:
        description: The codecov token to use when uploading coverage files
        required: true
      test_report_server_endpoint_url:
        description: The endpoint URL for the test report collection server
        required: false
      test_report_server_api_key:
        description: The API key for the test results collection server
        required: false
    inputs:
      branch:
        description: The PennyLane branch to checkout and run unit tests for
        required: true
        type: string
      job_name_prefix:
        description: A prefix to attach to all jobs that are spawned by this workflow
        required: false
        type: string
        default: ''
      job_name_suffix:
        description: A suffix to attach to all jobs that are spawned by this workflow
        required: false
        type: string
        default: ''
      pytest_coverage_flags:
        description: PyTest Coverage flags to pass to all jobs
        required: false
        type: string
        default: --cov-config=.coveragerc --cov=pennylane --cov-append --cov-report=term-missing --cov-report=xml --no-flaky-report --tb=native
      pytest_additional_args:
        description: Additional arguments to pass to pytest
        required: false
        type: string
        default: ''
      run_lightened_ci:
        description: |
          Indicate if a lightened version of the CI should be run instead of the entire suite.

          The lightened version of the CI includes the following changes:
          - Only Python 3.11 is tested against, instead of 3.11, 3.12, 3.13
        required: false
        type: boolean
        default: false
      skip_ci_test_jobs:
        description: |
          Names of jobs (comma separated) that should be skipped on a lightened CI run.
          The value of this variable is only used IF 'run_lightened_ci' is `true`.
          For a full test-suite run, all jobs are triggered.
        required: false
        type: string
        default: ''
      additional_python_packages:
        description: Additional Python packages to install separated by a space
        required: false
        type: string
        default: ''
      upload_to_codecov:
        description: Indicate if the coverage report should be uploaded to codecov
        required: false
        type: boolean
        default: true
      use_large_runner:
        description: |
          Indicate if the large runner should be used for the job.
          If this is true, large runner is used for the build regardless of the context.
        required: false
        type: boolean
        default: false
      python_warning_level:
        description: Sets the default Python warning level as defined by https://docs.python.org/3/using/cmdline.html#envvar-PYTHONWARNINGS
        required: false
        type: string
        default: 'default'
env:
  ACTIONS_RUNNER_DEBUG: true
  ACTIONS_STEP_DEBUG: true

jobs:
  determine_runner:
    if: github.event.pull_request.draft == false
    name: Determine runner type to use
    uses: ./.github/workflows/determine-workflow-runner.yml
    with:
      default_runner: ubuntu-24.04
      force_large_runner: ${{ inputs.use_large_runner }}

  warnings-as-errors-setup:
    needs:
      - determine_runner
    
    runs-on: ${{ needs.determine_runner.outputs.runner_group }}
    steps:
      - name: Set fail-fast for WAE
        id: mat_fail_fast
        run: |
          echo "fail_fast=${{ contains(inputs.python_warning_level, 'default') && 'default' || 'error' }}" >> $GITHUB_OUTPUT

      - name: Set pytest arguments for setting warnings level
        id: pytest_warning_flags
        env:
          PYTEST_WARNING_ARGS: -W "${{ inputs.python_warning_level }}" --continue-on-collection-errors "${{ inputs.pytest_additional_args }}"
        run: |
          if [ "${{ inputs.python_warning_level }}" != "default" ]; then
            echo "Setting pytest warning flags"
            echo "pytest_warning_args=$PYTEST_WARNING_ARGS" >> $GITHUB_OUTPUT
          else
            echo "No pytest warning flags needed"
            echo "pytest_warning_args=" >> $GITHUB_OUTPUT
          fi

    outputs:
      fail_fast: ${{ steps.mat_fail_fast.outputs.fail_fast }}
      pytest_warning_args: ${{ steps.pytest_warning_flags.outputs.pytest_warning_args }}

  setup-ci-load:
    needs:
      - determine_runner
    
    runs-on: ${{ needs.determine_runner.outputs.runner_group }}

    steps:
      - name: Setup Python Versions
        id: python_versions

        # All jobs will use the 'default' python versions listed in the dictionary below.
        # Unless the job name exists as a key as well, in which case the python versions listed for the job itself will be used instead.
        run: |
          if [ "${{ inputs.run_lightened_ci }}" == "true" ];
          then
            cat >python_versions.json <<-EOF
          {
            "default": ["3.11"]
          }
          EOF
          elif [ "${{ inputs.python_warning_level }}" == "error" ];
          then
            cat >python_versions.json <<-EOF
          {
            "default": ["3.11"]
          }
          EOF
          else
            cat >python_versions.json <<-EOF
          {
            "default": ["3.11", "3.12", "3.13", "3.14"],
            "torch-tests": ["3.11", "3.14"],
            "jax-tests": ["3.11", "3.14"],
            "capture-jax-tests": ["3.11", "3.14"],
            "all-interfaces-tests": ["3.11"],
            "external-libraries-tests": ["3.11"],
            "qcut-tests": ["3.11"],
            "qchem-tests": ["3.11"],
            "gradients-tests": ["3.11"],
            "data-tests": ["3.11", "3.14"],
            "device-tests": ["3.11"]
          }
          EOF
          fi

          jq . python_versions.json
          echo "python_versions=$(jq -r tostring python_versions.json)" >> $GITHUB_OUTPUT

      - name: Setup Matrix Max Parallel
        id: max_parallel
        run: |
          if [ "${{ inputs.run_lightened_ci }}" == "true" ];
          then
            cat >matrix_max_parallel.json <<-EOF
          {
            "default": 1,
            "core-tests": 6,
            "gradients-tests": 2,
            "jax-tests": 4,
            "device-tests": 1
          }
          EOF
          else
            cat >matrix_max_parallel.json <<-EOF
          {
            "default": 1,
            "core-tests": 6,
            "jax-tests": 8,
            "torch-tests": 2,
            "device-tests": 1
          }
          EOF
          fi

          jq . matrix_max_parallel.json
          echo "matrix_max_parallel=$(jq -r tostring matrix_max_parallel.json)" >> $GITHUB_OUTPUT

      - name: Setup Job to Skip
        id: jobs_to_skip
        env:
          JOBS_TO_SKIP: ${{ inputs.skip_ci_test_jobs }}
        run: |
          if [ "${{ inputs.run_lightened_ci }}" == "true" ];
          then
            skipped_jobs=$(echo -n "$JOBS_TO_SKIP" | python -c 'import re, json, sys; print(json.dumps(list(map(lambda job: job.strip(), filter(None, re.split(",|\n|\s", sys.stdin.read()))))))')
            echo "The following jobs will be skipped: $skipped_jobs"
            echo "jobs_to_skip=$skipped_jobs" >> $GITHUB_OUTPUT
          else
            echo 'jobs_to_skip=[]' >> $GITHUB_OUTPUT
          fi

    outputs:
      matrix-max-parallel: ${{ steps.max_parallel.outputs.matrix_max_parallel }}
      python-version: ${{ steps.python_versions.outputs.python_versions }}
      jobs-to-skip: ${{ steps.jobs_to_skip.outputs.jobs_to_skip }}
  
  # This job is the source of truth for the default versions of PyTorch, TensorFlow, and JAX.
  # Individual jobs can use these values or override at a per job level.
  default-dependency-versions:
    needs:
      - determine_runner
    uses: ./.github/workflows/interface-dependency-versions.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}

  torch-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).torch-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).torch-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'torch-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}torch-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: core-interfaces-coverage-torch-${{ matrix.python-version }}
      python_version: ${{ matrix.python-version }}
      additional_pip_packages: |
        ${{ needs.default-dependency-versions.outputs.pytorch-version }}
        ${{ inputs.additional_python_packages }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_additional_args: ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}torch-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      pytest_markers: torch and not qcut and not finite-diff and not param-shift
      requirements_file: ${{ github.event_name == 'schedule' && strategy.job-index == 0 && 'torch.txt' || '' }}


  autograd-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).autograd-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).autograd-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'autograd-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}autograd-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: core-interfaces-coverage-autograd-${{ matrix.python-version }}
      python_version: ${{ matrix.python-version }}
      additional_pip_packages: ${{ inputs.additional_python_packages }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_additional_args: ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}autograd-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      pytest_markers: autograd and not qcut and not finite-diff and not param-shift

  jax-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).jax-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        group: [1, 2, 3, 4]
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).jax-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'jax-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}jax-tests (${{ matrix.group }}, ${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: core-interfaces-coverage-jax-${{ matrix.python-version }}-${{ matrix.group }}
      python_version: ${{ matrix.python-version }}
      additional_pip_packages: |
        ${{ needs.default-dependency-versions.outputs.jax-version }}
        ${{ inputs.additional_python_packages }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_markers: jax and not qcut and not finite-diff and not param-shift and not capture
      pytest_additional_args: --dist=loadscope --splits 4 --group ${{ matrix.group }} ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_durations_file_path: '.github/durations/jax_tests_durations.json'
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}jax-tests (${{ matrix.group }}, ${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      requirements_file: ${{ github.event_name == 'schedule' && strategy.job-index == 0 && 'jax.txt' || '' }}

  capture-jax-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).capture-jax-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).capture-jax-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'capture-jax-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}capture-jax-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: core-interfaces-coverage-capture-jax-${{ matrix.python-version }}-${{ matrix.group }}
      python_version: ${{ matrix.python-version }}
      additional_pip_packages: |
        ${{ needs.default-dependency-versions.outputs.catalyst-jax-version }}
        ${{ inputs.additional_python_packages }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_markers: capture and not qcut and not finite-diff and not param-shift
      pytest_additional_args: --dist=loadscope ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_durations_file_path: '.github/durations/capture_jax_tests_durations.json'
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}capture-jax-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      requirements_file: ${{ github.event_name == 'schedule' && strategy.job-index == 0 && 'capture_jax.txt' || '' }}




  core-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).core-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        group: [1, 2, 3, 4, 5, 6]
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).core-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'core-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}core-tests (${{ matrix.group }}, ${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: core-interfaces-coverage-core-${{ matrix.python-version }}-${{ matrix.group }}
      python_version: ${{ matrix.python-version }}
      additional_pip_packages: ${{ inputs.additional_python_packages }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_markers: core and not qcut and not finite-diff and not param-shift
      pytest_additional_args: --splits 6 --group ${{ matrix.group }} ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_durations_file_path: '.github/durations/core_tests_durations.json'
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}core-tests (${{ matrix.group }}, ${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      requirements_file: ${{ github.event_name == 'schedule' && strategy.job-index == 0 && 'core.txt' || '' }}


  all-interfaces-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).all-interfaces-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).all-interfaces-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'all-interfaces-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}all-interfaces-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: all-interfaces-coverage
      python_version: ${{ matrix.python-version }}
      additional_pip_packages: |
        ${{ needs.default-dependency-versions.outputs.jax-version }}
        ${{ needs.default-dependency-versions.outputs.pytorch-version }}
        ${{ inputs.additional_python_packages }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_markers: all_interfaces
      pytest_additional_args: ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}all-interfaces-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      requirements_file: ${{ github.event_name == 'schedule' && strategy.job-index == 0 && 'all_interfaces.txt' || '' }}


  external-libraries-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).external-libraries-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).external-libraries-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'external-libraries-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}external-libraries-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: external-libraries-tests-coverage
      python_version: ${{ matrix.python-version }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_markers: external
      pytest_additional_args: ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}external-libraries-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      additional_pip_packages: |
        pyzx matplotlib stim quimb mitiq>=0.45.1 optax scipy-openblas32>=0.3.26 qualtran openqasm3 antlr4_python3_runtime
        ${{ needs.default-dependency-versions.outputs.jax-version }}
        git+https://github.com/PennyLaneAI/pennylane-qiskit.git@master
        ${{ needs.default-dependency-versions.outputs.catalyst-nightly }}
        ${{ inputs.additional_python_packages }}
      # TODO: Remove once Numba supports NumPy>2.3 
      additional_pip_packages_post: |
        numpy>=2.0,<=2.3

      requirements_file: ${{ github.event_name == 'schedule' && strategy.job-index == 0 && 'external.txt' || '' }}


  qcut-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).qcut-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).qcut-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'qcut-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}qcut-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: qcut-coverage
      python_version: ${{ matrix.python-version }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_markers: qcut
      pytest_additional_args: ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}qcut-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      additional_pip_packages: |
        kahypar==1.3.5 
        opt_einsum
        ${{ needs.default-dependency-versions.outputs.jax-version }}
        ${{ needs.default-dependency-versions.outputs.pytorch-version }}
        ${{ inputs.additional_python_packages }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}


  qchem-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).qchem-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).qchem-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'qchem-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}qchem-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: qchem-coverage
      python_version: ${{ matrix.python-version }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_markers: qchem
      pytest_additional_args: ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}qchem-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      # pyscf is not compatible with numpy 2.4 yet
      additional_pip_packages: |
        openfermionpyscf basis-set-exchange geometric scikit-learn numpy<2.4
        ${{ inputs.additional_python_packages }}

  gradients-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).gradients-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        config:
          - suite: finite-diff
          - suite: param-shift
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).gradients-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'gradients-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}gradients-tests (${{ matrix.config.suite }}, ${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: gradients-${{ matrix.config.suite }}-coverage
      python_version: ${{ matrix.python-version }}
      pytest_additional_args: ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}gradients-tests (${{ matrix.config.suite }}, ${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      additional_pip_packages: |
        ${{ needs.default-dependency-versions.outputs.jax-version }}
        ${{ needs.default-dependency-versions.outputs.pytorch-version }}
        ${{ inputs.additional_python_packages }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_markers: ${{ matrix.config.suite }}


  data-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).data-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).data-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'data-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}data-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: data-coverage-${{ matrix.python-version }}
      python_version: ${{ matrix.python-version }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_additional_args: ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_markers: data
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}data-tests (${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
      additional_pip_packages: |
        h5py
        ${{ inputs.additional_python_packages }}


  device-tests:
    needs:
      - setup-ci-load
      - determine_runner
      - default-dependency-versions
      - warnings-as-errors-setup
    strategy:
      fail-fast: ${{ needs.warnings-as-errors-setup.outputs.fail_fast == 'default' }}
      max-parallel: >-
        ${{
           fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).device-tests
           || fromJSON(needs.setup-ci-load.outputs.matrix-max-parallel).default
         }}
      matrix:
        config:
          - device: default.qubit
            shots: None
          - device: default.qubit
            shots: 10000
          - device: default.mixed
            shots: None
        python-version: >-
          ${{
            fromJSON(needs.setup-ci-load.outputs.python-version).device-tests
            || fromJSON(needs.setup-ci-load.outputs.python-version).default
           }}
    if: ${{ !contains(fromJSON(needs.setup-ci-load.outputs.jobs-to-skip), 'device-tests') }}
    uses: ./.github/workflows/unit-test.yml
    with:
      job_runner_name: ${{ needs.determine_runner.outputs.runner_group }}
      job_name: ${{ inputs.job_name_prefix }}device-tests (${{ matrix.config.device }}, ${{ matrix.config.shots }}, ${{ matrix.python-version }})${{ inputs.job_name_suffix }}
      branch: ${{ inputs.branch }}
      coverage_artifact_name: devices-coverage-${{ matrix.config.device }}-${{ matrix.config.shots }}
      python_version: ${{ matrix.python-version }}
      additional_pip_packages: |
        ${{ !contains(matrix.config.skip_interface, 'jax') && needs.default-dependency-versions.outputs.jax-version || '' }}
        ${{ !contains(matrix.config.skip_interface, 'torch') && needs.default-dependency-versions.outputs.pytorch-version || '' }}
        ${{ inputs.additional_python_packages }}
      additional_pip_packages_post: ${{ needs.default-dependency-versions.outputs.pennylane-lightning-latest }}
      pytest_test_directory: pennylane/devices/tests
      pytest_coverage_flags: ${{ inputs.pytest_coverage_flags }}
      pytest_additional_args: --device=${{ matrix.config.device }} --shots=${{ matrix.config.shots }} ${{ needs.warnings-as-errors-setup.outputs.pytest_warning_args }}
      pytest_xml_file_path: '${{ inputs.job_name_prefix }}device-tests (${{ matrix.config.device }}, ${{ matrix.config.shots }}, ${{ matrix.python-version }})${{ inputs.job_name_suffix }}.xml'
  
  all-tests-passed:
    runs-on: ${{ needs.determine_runner.outputs.runner_group }}
    needs:
      - determine_runner
      - torch-tests
      - autograd-tests
      - jax-tests
      - capture-jax-tests
      - core-tests
      - all-interfaces-tests
      - external-libraries-tests
      - qcut-tests
      - qchem-tests
      - gradients-tests
      - data-tests
      - device-tests
    
    # Run this job even if any dependencies were skipped
    # but do not run if any dependencies failed or were cancelled
    if: >-
      ${{
        always() &&
        !contains(needs.*.result, 'failure') &&
        !contains(needs.*.result, 'cancelled')
       }}

    steps:
      - name: Block until all test jobs are complete
        run: echo "All test jobs completed successfully."

  upload-to-codecov:
    runs-on: ${{ needs.determine_runner.outputs.runner_group }}
    needs: 
      - all-tests-passed
      - determine_runner

    # Run this if and only if the "all-test-passed" job succeeded
    if: >-
      ${{
        always() &&
        inputs.upload_to_codecov == true &&
        needs.all-tests-passed.result == 'success'
       }}

    steps:
      # Checkout repo so Codecov action is able to resolve git HEAD reference
      - name: Checkout
        uses: actions/checkout@v6
        with:
          ref: ${{ inputs.branch }}

      - name: Download Coverage Artifacts
        uses: actions/download-artifact@v4

      - name: Upload to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.codecov_token }}
          fail_ci_if_error: true  # upload errors should be caught early

  upload-reports:
    needs:
      - setup-ci-load
      - determine_runner
      - torch-tests
      - autograd-tests
      - jax-tests
      - capture-jax-tests
      - core-tests
      - all-interfaces-tests
      - external-libraries-tests
      - qcut-tests
      - qchem-tests
      - gradients-tests
      - data-tests
      - device-tests

    # Run this if any of the test jobs failed or were cancelled
    if: >-
      ${{
        always() &&
        (contains(join(needs.*.result, ','), 'failure') ||
         contains(join(needs.*.conclusion, ','), 'failure') ||
         contains(join(needs.*.status, ','), 'failure') ||
         contains(join(needs.*.result, ','), 'cancelled') ||
         contains(join(needs.*.conclusion, ','), 'cancelled') ||
         contains(join(needs.*.status, ','), 'cancelled'))
      }}

    runs-on: ${{ needs.determine_runner.outputs.runner_group }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ inputs.branch }}

      - name: Download Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: test-report-*
          path: .github/test-reports
          merge-multiple: true

      - name: Upload Test Failure Reports
        env:
          PENNYLANE_OSS_SERVER_ENDPOINT_URL: ${{ secrets.test_report_server_endpoint_url }}
          PENNYLANE_OSS_SERVER_API_KEY: ${{ secrets.test_report_server_api_key }}
        run: |
          python -m pip install --upgrade pip
          python -m pip install --group upload-reports

          python .github/workflows/scripts/upload_reports/upload.py \
            --commit-sha "${{ github.sha }}" \
            --branch "${{ inputs.branch }}" \
            --workflow-id "${{ github.run_id }}" \
            --workspace-path "${{ github.workspace }}"
