name: Unit Test
on:
  workflow_call:
    inputs:
      job_name:
        description: The name of the Job as it would appear on GitHub Actions UI
        required: true
        type: string
      branch:
        description: The PennyLane branch to checkout and run unit tests for
        required: true
        type: string
      coverage_artifact_name:
        description: Name of the artifact file that will contain the coverage file for codevoc
        required: true
        type: string
      checkout_fetch_depth:
        description: How many commits to checkout from HEAD of branch passed
        required: false
        type: number
        default: 1
      python_version:
        description: The version of Python to use in order to run unit tests
        required: false
        type: string
        default: '3.9'
      pipeline_mode:
        description: The pipeline mode can be unit-tests, benchmarks, or reference-benchmark
        required: false
        type: string
        default: 'unit-tests'
      install_jax:
        description: Indicate if JAX should be installed or not
        required: false
        type: boolean
        default: true
      jax_version:
        description: The version of JAX to install. Leave empty to install latest version.
        required: false
        type: string
        default: ''
      install_tensorflow:
        description: Indicate if TensorFlow should be installed or not
        required: false
        type: boolean
        default: true
      tensorflow_version:
        description: The version of TensorFlow to install. Leave empty to install latest version.
        required: false
        type: string
        default: ''
      install_pytorch:
        description: Indicate if PyTorch should be installed or not
        required: false
        type: boolean
        default: true
      pytorch_version:
        description: The version of PyTorch to install. Leave empty to install latest version.
        required: false
        type: string
        default: ''
      install_pennylane_lightning_master:
        description: Indicate if PennyLane-Lightning should be installed from the master branch
        required: false
        type: boolean
        default: true
      pytest_test_directory:
        description: The directory where the PennyLane tests are that should be run by PyTest
        required: false
        type: string
        default: tests
      pytest_coverage_flags:
        description: Coverage flags for PyTest
        required: false
        type: string
        default: ''
      pytest_markers:
        description: Custom mark string to pass to PyTest
        required: false
        type: string
        default: ''
      pytest_additional_args:
        description: Additional arguments to pass to PyTest
        required: false
        type: string
        default: ''
      additional_pip_packages:
        description: Additional packages to install. Values will be passed to pip install {value}
        required: false
        type: string
        default: ''
      requirements_file:
        description: File name to store stable version of requirements for a test group
        required: false
        type: string
        default: ''

jobs:
  test:
    name: ${{ inputs.job_name }}
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v3
        with:
          ref: ${{ inputs.branch }}
          fetch-depth: ${{ inputs.checkout_fetch_depth }}
          repository: PennyLaneAI/pennylane

      - name: Determine benchmark name
        if: ${{ inputs.pipeline_mode != 'unit-tests' }}
        id: benchmark_name
        run: |
          job_name="${{ inputs.job_name }}"
          _benchmark_name=${job_name//[(,)]/""}
          _benchmark_name=${_benchmark_name//[(" ")]/"-"}
          echo "benchmark_name=$_benchmark_name" >> $GITHUB_OUTPUT

      - name: Cache reference benchmarks
        if: ${{ inputs.pipeline_mode != 'unit-tests' }}
        id: benchmark-cache
        uses: actions/cache@v3
        with:
          path: benchmark_reference
          key: ${{ steps.benchmark_name.outputs.benchmark_name }}-benchmarks_references

      - name: Check if the shared part of the job will continue
        id: continue
        run: >-
          echo "confirm=${{
            contains(fromJSON('["unit-tests", "benchmarks"]'), inputs.pipeline_mode)
            || (inputs.pipeline_mode == 'reference-benchmarks'
            && steps.benchmark-cache.outputs.cache-hit != 'true' )}}" >> $GITHUB_OUTPUT

      - name: Install PennyLane and dependencies
        if: steps.continue.outputs.confirm == 'true'
        uses: ./.github/workflows/install_deps
        with:
          python_version: ${{ inputs.python_version }}
          install_pytorch: ${{ inputs.install_pytorch }}
          install_tensorflow: ${{ inputs.install_tensorflow }}
          install_jax: ${{ inputs.install_jax }}
          additional_pip_packages: ${{ inputs.additional_pip_packages }}
          install_pennylane_lightning_master: ${{ inputs.install_pennylane_lightning_master }}
          requirements_file: ${{ inputs.requirements_file }}

      - name: Set PyTest Args
        if: steps.continue.outputs.confirm == 'true'
        id: pytest_args
        env:
          PIPELINE_MODE: ${{ inputs.pipeline_mode }}
          PYTEST_COVERAGE_ARGS: ${{ inputs.pytest_coverage_flags }}
          PYTEST_PARALLELISE_ARGS: -n auto
          PYTEST_BENCHMARKS_ARGS: --benchmark-enable --benchmark-only --benchmark-json=benchmarks.json
          PYTEST_ADDITIONAL_ARGS: ${{ inputs.pytest_additional_args }}
        run: |
          if [[ "$PIPELINE_MODE" =~ .*"benchmarks".* ]]; then
            echo "args=$PYTEST_BENCHMARKS_ARGS $PYTEST_ADDITIONAL_ARGS" >> $GITHUB_OUTPUT
          else
            echo "args=$PYTEST_COVERAGE_ARGS $PYTEST_PARALLELISE_ARGS $PYTEST_ADDITIONAL_ARGS" >> $GITHUB_OUTPUT
          fi

      - name: Run PennyLane Unit Tests
        if: steps.continue.outputs.confirm == 'true'
        env:
          PYTEST_MARKER: ${{ inputs.pytest_markers != '' && format('-m "{0}"', inputs.pytest_markers) || '' }}
          COV_CORE_SOURCE: pennylane
          COV_CORE_CONFIG: .coveragerc
          COV_CORE_DATAFILE: .coverage.eager
          TF_USE_LEGACY_KERAS: "1"  # sets to use tf-keras (Keras2) instead of keras (Keras3) when running TF tests
        # Calling PyTest by invoking Python first as that adds the current directory to sys.path
        run: python -m pytest ${{ inputs.pytest_test_directory }} ${{ steps.pytest_args.outputs.args }} ${{ env.PYTEST_MARKER }}

      - name: Adjust coverage file for Codecov
        if: inputs.pipeline_mode == 'unit-tests'
        run: bash <(sed -i 's/filename=\"/filename=\"pennylane\//g' coverage.xml)

      - name: Upload Coverage File
        if: inputs.pipeline_mode == 'unit-tests'
        uses: actions/upload-artifact@v3
        with:
          name: ${{ inputs.coverage_artifact_name }}
          path: coverage.xml

      # If this is a reference benchmark move file to the cached directory, and we have a non-empty reference benchmark file.
      - name: Move reference pytest benchmark file to cache
        if: inputs.pipeline_mode == 'reference-benchmarks' && hashFiles('benchmarks.json') != ''
        run: mkdir -p ${{ github.workspace }}/benchmark_reference && cp benchmarks.json "$_"

      - name: Convert pytest benchmark JSON files to XUBM-JSON
        if: inputs.pipeline_mode == 'benchmarks' && hashFiles('benchmarks.json') != '' && hashFiles('benchmark_reference/benchmarks.json') != ''
        run: |
          mkdir ${{ github.workspace }}/benchmark_results
          python .github/workflows/scripts/benchmarks/convert_pytest_JSON_to_XUBM.py --filename_XUBM benchmark_results/benchmarks_xubm.json --author ${{ github.event.pull_request.user.login }} --github_reference ${{ github.ref }}
          python .github/workflows/scripts/benchmarks/convert_pytest_JSON_to_XUBM.py --filename benchmark_reference/benchmarks.json --filename_XUBM benchmark_reference/benchmarks_xubm.json --author ${{ github.event.pull_request.user.login }}

      - name: Plotting benchmark graphs
        if: inputs.pipeline_mode == 'benchmarks' && hashFiles('benchmark_results/benchmarks_xubm.json') != '' && hashFiles('benchmark_reference/benchmarks_xubm.json') != ''
        run: |
          python .github/workflows/scripts/benchmarks/plot_benchmarks.py --graph_name ${{ steps.benchmark_name.outputs.benchmark_name }}

      # Merge benchmarks data in a single JSON. Create a CSV file with runtimes. Move data to the artifact directory.
      - name: Post-process benchmark data
        if: inputs.pipeline_mode == 'benchmarks' && hashFiles('benchmark_results/benchmarks_xubm.json') != '' && hashFiles('benchmark_reference/benchmarks_xubm.json') != ''
        run: |
          python .github/workflows/scripts/benchmarks/export_benchmarks_data.py

      # If this is an assessment benchmark, upload the data as an artifact.
      - name: Upload pytest benchmark results
        if: inputs.pipeline_mode == 'benchmarks'
        uses: actions/upload-artifact@v3
        with:
          name: ${{ steps.benchmark_name.outputs.benchmark_name }}-benchmarks
          path: ${{ github.workspace }}/benchmark_results
