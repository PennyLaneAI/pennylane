\documentclass[aps,pra,10pt,twocolumn,groupedaddress,nofootinbib]{revtex4-1}

\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}% Include figure files
\usepackage{bm,bbm}% bold math
\usepackage{hyperref}% add hypertext capabilities
\usepackage[margin=2cm]{geometry}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{qcircuit}
\usepackage{multirow}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}%[section]


\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\Ad}{Ad}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator{\diag}{diag}  % diagonal vector of the given matrix / diagonal matrix with the given vector as diagonal
\DeclareMathOperator{\Ker}{Ker}    % kernel
\DeclareMathOperator{\Imag}{Imag}  % image
\DeclareMathOperator{\spec}{spec}  % eigenvalue spectrum
\DeclareMathOperator{\lcm}{lcm}    % least common multiple

\newcommand{\isom}{\cong} % isomorphic to
\newcommand{\conj}[1]{\overline{#1}} % complex conjugate
\newcommand{\mnorm}[1]{\ensuremath{\left\| #1 \right\|}} % matrix norm

\newcommand{\de}[2]{\frac{d #1}{d #2}}   % derivative
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}}  % partial derivative
\newcommand{\hc}{\text{h.c.}}  % hermitian conjugate

\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bpm}{\begin{pmatrix}}
\newcommand{\epm}{\end{pmatrix}}

\newcommand{\naturals}{\ensuremath{\mathbb N}}
\newcommand{\Z}{\ensuremath{\mathbb Z}}  % integer numbers
\newcommand{\Q}{\ensuremath{\mathbb Q}}  % rational numbers
\newcommand{\R}{\ensuremath{\mathbb R}}  % real numbers
\newcommand{\C}{\ensuremath{\mathbb C}}  % complex numbers
\newcommand{\K}{\ensuremath{\mathbb K}}  % any field
\newcommand{\I}{\mathbbm{1}} % vector space identity op

\newcommand{\SL}{\text{SL}} % special linear group
\newcommand{\GL}{\text{GL}} % general linear group
\newcommand{\OO}{\text{O}}  % orthogonal group
\newcommand{\SO}{\text{SO}} % special orthogonal group
\newcommand{\U}{\text{U}}   % unitary group
\newcommand{\SU}{\text{SU}} % special unitary group
\newcommand{\Sp}{\text{Sp}} % symplectic group


\newcommand{\CNOT}{\text{CNOT}}

\newcommand{\comm}[2]{\ensuremath{\left[#1, #2\right]}}             % commutator
\newcommand{\acomm}[2]{\ensuremath{\left\{#1, #2\right\}}}          % anticommutator
\newcommand{\ket}[1]{\ensuremath{\left| #1 \right \rangle}}
\newcommand{\bra}[1]{\ensuremath{\left \langle #1 \right |}}
\newcommand{\braket}[2]{\ensuremath{\left\langle #1\left|#2 \right.\right\rangle}}
\newcommand{\ketbra}[2]{\ket{#1}\bra{#2}}
%\newcommand{\vect}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\inprod}[2]{\ensuremath{\left\langle #1, #2 \right\rangle}}  % inner product
\newcommand{\lieprod}[2]{\ensuremath{\left[#1, #2\right]}}          % Lie product
\newcommand{\liealg}[1]{\ensuremath{\mathfrak{#1}}}                 % Lie algebra
\newcommand{\expect}[1]{\ensuremath{\left\langle #1 \right\rangle}} % expectation value
\newcommand{\tracep}[1]{\ensuremath{\trace\left( #1 \right)}}


\renewcommand{\a}{\hat{a}}
\newcommand{\adag}{\hat{a}^{\dagger}}
\newcommand{\x}{\hat{x}}
\newcommand{\p}{\hat{p}}
\renewcommand{\c}{\hat{c}}
\renewcommand{\d}{\hat{d}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\bad}{\bm{\hat a}^\dagger}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\cR}{\mathcal{R}}

\newcommand{\sidenote}[1]{\marginpar{\footnotesize{\textcolor{red}{-#1}}}}

\newcommand{\nathan}[1]{\textcolor{blue}{Nathan: #1}}
\newcommand{\maria}[1]{\textcolor{orange}{Maria: #1}}
\newcommand{\ville}[1]{\textcolor{purple}{Ville: #1}}
\newcommand{\cg}[1]{\textcolor{cyan!80!black}{Christian G.: #1}}

%quantum and classical node stypes
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{shapes.multipart}
\definecolor{quantum1}{HTML}{8EDBCE}
\definecolor{quantum2}{HTML}{3F605B}
\definecolor{exp1}{HTML}{9AB9ED}
\definecolor{exp2}{HTML}{204177}
\definecolor{classical}{HTML}{EBBA92}
\tikzset{input node/.style={}}
\tikzset{quantum node/.style={draw, align=center, anchor=west, inner sep=5pt,rounded corners=4pt, rectangle split, rectangle split horizontal, rectangle split parts=2, rectangle split part fill={quantum1,quantum2}, every two node part/.style={text = white}}}
\tikzset{classical node/.style={draw, rectangle,align=center, anchor=west, thin, fill=classical, inner sep=5pt}}
%\tikzset{expectation node/.style={draw, align=center, anchor=west, inner sep=5pt, rectangle split, rectangle split horizontal, rectangle split parts=2, rectangle split part fill={exp1,exp2}, every two node part/.style={text = white}}}
\tikzset{output node/.style={}}
\tikzset{out label/.style={midway, above}}
\tikzset{connector/.style={anchor=center, opacity=0.}}
\tikzset{samples label/.style={at start, below, xshift=4pt}}


\begin{document}

\title{OpenQML - Technical manuscript}
\author{Maria Schuld}
\author{Nathan Killoran}
\email{nathan@xanadu.ai}
\author{Ville Bergholm}
\affiliation{Xanadu Inc., 372 Richmond St W, Toronto, M5V 1X6, Canada}


\date{\today}

\begin{abstract}
Compilation of formulas and conventions for the OpenQML framework.
\end{abstract}

\maketitle


\section{Model structure}

We consider an abstract \textbf{model} $f(x, \theta)$
that maps a set of \textbf{model inputs} $x$ (which may be empty) and a set of \textbf{model parameters} $\theta$ to a \textbf{model output} $y$. \\

Each set of model parameters $\theta$ is associated with a scalar \textbf{cost} $C(\theta, \mathcal{D})$ of the model, which also depends on a (possibly empty) dataset $\mathcal{D}$.\\

The models we are interested in are hybrid, since they are combinations of quantum and classical computations. Such a model can be depicted as a directed graph of \textbf{nodes} which we call a \textbf{hybrid model graph}. There are two types of nodes: \textbf{Classical nodes} map node inputs $x_c$ and node parameters $\theta_c$ to outputs $y_c$ via a computation executed by a classical computer. \textbf{Quantum nodes} map node inputs $x_q$ and node parameters $\theta_q$ to the estimate of an expectation $y_q$.\\

Each node defines a map from \textbf{node inputs} to \textbf{node outputs}, where the node inputs are the node outputs from the parent nodes. Root nodes (those without parents) take a subset of the model inputs $x$ as node inputs, and the collection of leaf nodes (those without children) return the model output as their node outputs.

\subsection{Quantum nodes}
A quantum node always consists of a \textbf{variational circuit} $U(\gamma)$, where $U$ is a \textbf{quantum circuit} with a set of tunable \textit{circuit parameters} $\gamma$. The circuit acts on an \textbf{initial state} $|\psi_0 \rangle$ which is usually the vacuum state $\ket{0}$.\\

The circuit parameters $\gamma$ can be of three different types: \textbf{trainable parameters} that are associated with the trainable model parameters  $\theta_q$, \textbf{fixed parameters} which are always set to a specific value during training, or \textbf{placeholders} for inputs $x_q$ from preceding classical nodes (if there are any). After applying the variational circuit, we get a final quantum state $\ket{\psi (x_q, \theta_q) }$ from which we draw sample observations with regards to an observable $O$ (i.e., a compuational basis measurement returns samples of bit strings, and a photon number resolution measurement returns strings of integers). The samples are used to estimate the expectation value
\[\gamma \mapsto \bra{\psi_0} U(\gamma)^{\dagger}  \hat{O} U(\gamma) \ket{\psi_0}.\]
The estimation is the output $y_q$ of the quantum node. It is associated with an error
\[ \epsilon = \frac{\langle O \rangle}{\sqrt{R}}, \]
that depends on the variance of the operator $\langle O \rangle$, as well as the number of samples $R$ that we draw. $R$ is a hyperparameter of the quantum node. We draw quantum nodes as:\\
\begin{figure}[h]
\centering
\begin{tikzpicture}
\node[quantum node] (c) at (0,0) {$U(x_c, \theta_c) $ \nodepart{two} $O $};
\draw[->] (c) -- (3.5,0)  node[out label] {$y_{q}$} node[samples label] {$R$};
\end{tikzpicture}
\end{figure}


The variational circuit can be composed of \textbf{layers}
\[ U (\gamma) = \L_{N_L} \cdots \L_{1}\]
which repeat a fixed architecture, which possibly depends on hyperparameters. If there is no layer structure, we set $N_L=1$. A layer $\L_l$, $l=1,...,N_L$, consists of a series of \textbf{gates}
\[\L_l = \G^l_{N_G}(\gamma^l_{N_G}) \cdots \G^l_1(\gamma^l_1). \]
Each gate is parametrised by a set of circuit parameters
$\gamma^i_j$, $i = 1,...,N_L$, $j=1,...,N_G$. The set can be empty if the gate is constant.\\

We propose to use the following \textbf{elementary gate sets} from which the gates $\mathcal{G}$ are taken.

\subsubsection{Qubit architectures}

For qubit architectures we define three parametrized gates,
the elementary rotations around the $x$, $y$ and~$z$ axes:
\begin{eqnarray*}
        R_x(\alpha) &=& \e^{-i\alpha \sigma_x/2} =
        \begin{pmatrix}
          \cos \frac{\alpha}{2} & -i \sin \frac{\alpha}{2}\\
          -i \sin \frac{\alpha}{2} & \cos \frac{\alpha}{2}
        \end{pmatrix}\\
        R_y(\beta) &=& \e^{-i\beta \sigma_y/2} =
        \begin{pmatrix}
          \cos \frac{\beta}{2} & -\sin \frac{\beta}{2}\\
          \sin \frac{\beta}{2} & \cos \frac{\beta}{2}
        \end{pmatrix}\\
        R_z(\gamma) &=& \e^{-i\gamma \sigma_z/2}=
        \begin{pmatrix}
          \e^{-i \frac{\gamma}{2}} & 0\\
          0 & \e^{i \frac{\gamma}{2}}
        \end{pmatrix}\\
\end{eqnarray*}
Any $\SO(3)$ rotation can be expressed using three Euler angles, that is, three rotations around two orthogonal axes. A standard axis choice is~$zyz$. Due to the isomorphism $\SU(2)/\Z_2 \isom \SO(3)$ we obtain a similar decomposition
for arbitrary single-qubit gates $S$:
\begin{align}
\label{eq:app:euler}
\notag
S &= R_z(\gamma) R_y(\beta) R_z(\alpha)\\
&=
\bpm
e^{-i(\alpha+\gamma)/2} \cos(\beta/2) & -e^{i(\alpha-\gamma)/2} \sin(\beta/2)\\
e^{-i(\alpha-\gamma)/2} \sin(\beta/2) &  e^{i(\alpha+\gamma)/2} \cos(\beta/2)
\epm.
\end{align}

Together with the $\CNOT$ gate this gate set is universal, i.e. any $\SU(2^n)$
gate can be expanded into a finite sequence of $\SU(2)$ and $\CNOT$ gates~\cite{barenco1995}.

\subsubsection{CV architectures}

Most gates in CV quantum computing are naturally parametrised. We consider a universal gate set consisting of a universal Gaussian gate set and a nonlinear gate. The following gates are universal for Gaussian transformations,
\begin{eqnarray}
  	R(\phi) & =& \exp\left(i \phi \adag \a \right), \\
  	D(\alpha) & =& \exp(r (e^{i\phi} \adag -e^{-i\phi} \a)), \\
  	S(r) & =& \exp \left(\frac{r}{2} \left(e^{-i\phi} \a^2 -e^{i\phi}  (\adag)^2 \right) \right), \\
  	BS(\theta) & =& \exp\left(\eta (e^{i \phi} \adag_1 \a_2 -e^{-i \phi}\a_1 \adag_2) \right).
  \label{Eq:gaussiangates}
\end{eqnarray}
As the nonlinear gate we consider a general transformation
\begin{equation}
	\mathcal{N}(\phi) = \exp(i \phi \; \varphi(\a, \adag))
	\label{Eq:nonlineargate}
\end{equation}
where $\varphi$ is at least of third order in the quadrature operators.
\cg{A pedantic comment: Not every non-linear gate always promotes linear optics to universality \cite{Oszmaniec2017}.}

\subsection{Classical nodes}

A classical node can be any differentiable function $y_c = h(x_c, \theta_c)$ that maps inputs $x_c$ and model parameters $\theta_c$ to real \maria{any need for complex outputs?} output vectors or scalars $y_c$.\\

We depict classical nodes by the following symbol:\\
\begin{figure}[h]
\centering
\begin{tikzpicture}
\node[classical node] (c) at (0,0) {$h(x_c, \theta_c) $};
\draw[->] (c) -- (2.5,0)  node[out label] {$y_{c}$};
\end{tikzpicture}
\end{figure}


\begin{figure*}[t]
\begin{flushleft}
\begin{tikzpicture}
\node[align=left, anchor=west] at (-1,1) {(a)};
\node[input node] (i1) at (-0.5,0) {$\emptyset$};
\node[input node] (iK) at (-0.5,-1.5) {$\emptyset$};
\node[quantum node] (q1) at (0,0) {$U(\theta_1) $ \nodepart{two} $\sigma_x$};
\node[]  at (1,-0.75) {$\vdots $};
\node[quantum node] (qK) at (0,-1.5) {$U(\theta_i)$ \nodepart{two} $\sigma_x \otimes \sigma_y$};
\node[]  at (1,-2.25) {$\vdots $};
\node[classical node] (c) at (4.25,-1) {$\sum_{i=1}^K a_i y_{q_i} $};
\node[output node] (o) at (7,-1) {$y$};
\draw[->] (i1) -- (q1);
\draw[->] (iK) -- (qK) ;
\draw[] (q1.east) -- (3.7,0)  node[out label] {$y_{q_1}$} node[samples label] {$R$};
\draw[->] (3.7,0) -- (3.7,-1)-- (c);
\draw[] (qK.east) -- (3.7,-1.5) node[out label] {$y_{q_i}$} node[samples label] {$R$};
\draw[->] (3.7,-1.5) -- (3.7,-1)-- (c);
\draw[->] (c) -- (o);
%\draw[red] (q1) -| (mid) |- (c.west);
\end{tikzpicture}
 ~~~~~~~~~~~~~~~ \begin{tikzpicture}
\node[align=left, anchor=west] at (-1,0) {(b) Training:};
\node[input node] (i) at (-0.5,-0.75) {$\emptyset$};
\node[quantum node] (q) at (0,-0.75) {$ U(\theta) $ \nodepart{two} $\Lambda_{\mathcal{D}}$};
\node[output node] (o) at (2.75,-0.75) {$y$};
\draw[->] (i) -- (q);
\draw[->] (q) -- (o) node[samples label] {$R$};
\node[align=left, anchor=west] at (-0.5,-2) {Data generation:};
\node[input node] (i) at (-0.5,-2.75) {$\emptyset$};
\node[quantum node] (q) at (0,-2.75) {$ U(\theta) $ \nodepart{two} $\sigma_z \otimes \hdots \otimes\sigma_z$};
\node[output node] (o) at (4.25,-2.75) {$y$};
\draw[->] (i) -- (q);
\draw[->] (q) -- (o) node[samples label] {$1$};
\end{tikzpicture}\\ \bigbreak

\begin{tikzpicture}
\node[align=left, anchor=west] at (-1,1) {(c)};
\node[input node] (i) at (-0.5,0) {$\bx$};
\node[classical node] (c1) at (0,0) {$h_1(\bx, \theta_{c_1}) $};
\node[] (dots) at (3,0) {$\hdots $};
\node[classical node] (cK) at (4.5,0) {$h_K(\mathbf{y}_{c_{K-1}}, \theta_{c_K}) $};
\node[quantum node] (q) at (8.5,0) {$ U(\mathbf{y}_c, \theta_q) $ \nodepart{two} $\sigma_z$};
\node[output node] (o) at (11.75,0) {$y$};
\draw[->] (i) -- (c1);
\draw[->] (c1) -- (dots) node[out label] {$\mathbf{y}_{c_1}$};
\draw[->] (dots) -- (cK) node[out label] {$\mathbf{y}_{c_{K-1}}$};
\draw[->] (cK) -- (q) node[out label] {$\mathbf{y}_{c_K}$};
\draw[->] (q) -- (o);
\end{tikzpicture}\\ \bigbreak


\end{flushleft}
\caption{Examples of model graphs. Turqois nodes depict quantum nodes, and orange nodes depict classical nodes. (a) Model graph of a variational quantum eigensolver, in which expectation values of local Pauli operators are combined by a classical layer to an expectation value of a global Hamiltonian. (b)  A model, here a generative model) can even have different architectures for training and data generation. While the model is trained maximising the expectation of a quantum operator $\Lambda_{\mathcal{D}}$ that projects onto a `training set subspace' of Hilbert space, it can be used to sample data by performing a single computational basis measurement (i.e., $R=1$). (c) Model graph of a classical neural network with a final quantum layer that computes the scalar output.}
\label{Fig:example_modelgraphs}
\end{figure*}


From the building blocks of classical and quantum nodes, more complicated hybrid structures evolve. Examples of hybrid model graphs are given in Figure \ref{Fig:example_modelgraphs}. Figure \ref{Fig:example_modelgraphs} (a) shows a variational quantum eigensolver, where the model input is an empty set, and the output of the model is the weighed sum of expectation values. Figure \ref{Fig:example_modelgraphs} (b) shows a generative quantum model. Finally, Figure \ref{Fig:example_modelgraphs} (c) shows a classical neural network with one final quantum layer, whose output is the expectation value of a $\sigma_z$ operator applied to a predefined qubit.

\section{Optimization}


\subsection{Cost function}
We consider three different tasks for OpenQML: optimization, unsupervised learning and supervised learning. The tasks depend on whether and what type of data is used for training (see Table \ref{Tbl:tasks}).\\

\begin{table}[t]
\begin{tabular}{lll}
\hline \hline
Task & Inputs & Targets\\
\hline
Optimisation & No & No \\
Unsupervised & Yes & No\\
Supervised & Yes & Yes\\\hline \hline
\end{tabular}
\caption{Definition of the three tasks depending on whether and which type of data is used to train the circuit.}
\label{Tbl:tasks}
\end{table}


The cost function has a different structure for different tasks of training. We will revisit the examples in Figure \ref{Fig:example_modelgraphs} and present some potential cost function for these models. \\

%\begin{figure}[t]
%\includegraphics[scale=0.08]{tasks.pdf}
%\caption{Three tasks for OpenQML and the ob}
%\label{Fig:tasks}
%\end{figure}

\paragraph{Expectation maximisation cost for a variational eigensolver}

The cost function of a variational eigensolver is the direct model output $y$ shown in Figure \ref{Fig:example_modelgraphs} (a). $y$ is a weighed sum of local expectation values, which classically computes the expectation value of a global observable which is decomposed into a sum of local observables. Hence, the cost function reads
\[ C(\theta) = \sum_i a_i \bra{0}U(\theta)^{\dagger} O_i U(\theta)\ket{0}, \]
where the $O_i$ are Pauli operators acting on a few qubits only.


\paragraph{Cost for a generative model}
Assume we are given some unlabelled data $\mathcal{D}$ that can be associated with the basis states of a quantum state. Our goal is to prepare a quantum state $\ket{\psi}$ using the variational circuit, so that the likelihood of measuring a basis state $\ket{x^m}$ that represents an input $x^m \in \mathcal{D}$ is large. If we denote the projection onto the basis states that denote the data set as $\Lambda_{\mathcal{D}}$ as in Figure \ref{Fig:example_modelgraphs} (b), the goal is to maximise the expectation of this operator. The loss is hence given by
\[ C(\theta, \mathcal{D}) = \bra{0}U(\theta)^{\dagger} \Lambda_{\mathcal{D}} U(\theta)\ket{0}.\]
\maria{This is not maximum likelihood. Would that be better?}

\paragraph{Square loss cost for supervised learning}
Subfigure (c) in Figure \ref{Fig:example_modelgraphs} is an example of a supervised model. We are given a labelled dataset $\mathcal{D}$. The output $y^m$ of the model for an input $\bx^m$ is defined as the expectation of a specified $\sigma_z$ operator. The square loss compares this expectation with the target value $t^m$ for all training data $m=1,...,M$,
\[ C(\theta, \mathcal{D}) = \sum_{m} |\bra{0}U(\theta, \bx^m)^{\dagger} \sigma_z U(\theta, \bx^m)\ket{0}-t^m|^2. \]

There are of course many other loss functions one could think of, each taking the output of the current model and possibly some data and scoring its parameters with a scalar value.

\subsection{Optimization methods}

There are three methods to optimize models built from classical and quantum nodes:
\begin{enumerate}
\item Gradient-free optimization (i.e., Nelder-Mead, genetic algorithms, particle swarm optimization...)
\item Gradient-based optimization with numerical computation of gradients
\item Gradient-based optimization with analytical computation of the gradients
\end{enumerate}
Note that when we speak of gradients here, we actually refer to estimates of gradients that result from estimates of expectation values. Only in simulations of the quantum circuits can these estimates be exact deterministic values. In the items above, ``analytical computation'' therefore means that if the results of the quantum nodes were exact expectation values (i.e. values derived from analytical computation or estimated by an amount of measurements that goes to infinity), the gradients would be exact. Numerically computed gradients are approximations of the true gradients, even if the quantum nodes were giving exact expectations.

\subsubsection{Gradient-free optimization}
Gradient-free optimization is straight-forward from an algorithmic perspective, since we only need function evaluations that the quantum and classical devices can provide together.

\subsubsection{Gradient-based numeric optimization}

Also numerical methods require only black-box evaluations of the model. The value of the cost function $C(\theta)$ is estimated at two positions close to the current circuit parameter $\mu \in \theta$ which we want to update. The approximation of the gradient is given by
\[ \partial_{\mu} C(\mu) \approx \frac{C(\mu ) - C(\mu + \Delta \mu)}{\Delta \mu} \]
for the \textbf{forward finite-differences} method, and by
\[ \partial_{\mu} C(\mu) \approx \frac{C(\mu - \frac{1}{2}\Delta \mu ) - C(\mu + \frac{1}{2} \Delta \mu)}{\Delta \mu} \]
for the \textbf{centered finite-differences} method. The parameter shift $\Delta \mu$ has to be as small as possible for good approximations, but still large enough to make the finite difference larger than the error in estimating $C(\mu)$ with quantum devices, which makes numerical gradients useless for some applications.

\subsubsection{Gradient-based analytic optimization}

The third method, gradient-based optimization with analytical gradients, takes some more theoretical foundations, which we develop in the following section. The most common algorithm is (stochastic) gradient descent. In gradient descent, to minimize the cost in every \textbf{step} the parameters are updated according to the following simple loop:\\

\begin{algorithmic}[1]
\Procedure{Gradient Descent step}{}
\For {$\mu \in \theta $}
\State $\mu^{(t+1)} = \mu^{(t)} - \eta^{(t)} \partial_{\mu} C(\theta[, \mathcal{D}]) + S$
\EndFor
\EndProcedure
\end{algorithmic}

Here $\mathcal{D}$ is the dataset associated with the cost function for learning problems. If $C$ depends on a dataset $\mathcal{D}$, stochastic gradient descent refers to a gradient descent algorithm where in each step $\mathcal{D}$ is a random subsample of the full training data set. $S$ is an extra term for variations of gradient descent that take momentum (i.e., former gradients) into account. \\

The learning rate $\eta^{(t)}$ can be adapted in each step, either
depending on the gradient or on the step number. $S$ is a potential
additional term that can add a momentum to the update. The gradient
$\partial_{\mu} C(\theta[, \mathcal{D}])$ has to be evaluated by
automatic differentiation of the cost function, which is computed
hybridly by the quantum device or simulator and a classical
computer. We therefore need to define a hybrid automatic
differentiation scheme.




\section{Computing analytical gradients of quantum nodes}

The central feature of OpenQML is that it can compute automatic gradients of expectation values. In this section we therefore explore issues related to computing analytic gradients of models $f(x, \theta)$ consisting of quantum and classical nodes with regards to a specific model parameter $\mu \in \theta$.

\subsection{Gradients of hybrid model graphs}

\subsubsection{Backpropagating throught the graph}

Assume the model $f$ is described by a hybrid model graph $\mathcal{M}$. We want to take the derivative of an element $y_s$ of $f$'s output $y$ with respect to a model parameter $\mu$. Assume that $\mu$ is in the subset $\theta^*$ of the parameters $\theta$, and the subset parametrizes the node $n^*$. For now we assume that the subsets are disjoint, so that there is no parameter tie between nodes. Let $y_s \circ n^{(p)}_1 \circ ... \circ (n^*)^{(p)}$ be the chain of (quantum or classical nodes) that emerges from following the output $y_s$ in the opposite direction of the directed edges until we reach node $n^*$. Since there may be $N_p > 1$ of those paths, we used a superscript to denote the path index. All branches that do not lead back to $\theta_t$ are independent of $\mu$ and can be thought of as constants. We can formally derive a `backpropagation' mechanism for this model.\\

The chain rule prescribes that the gradient with respect to the model parameter $\mu \in \theta_t$ is given by
\[ \partial_{\mu \in \theta_t} y_s = \sum_{p=1}^{N_p} \frac{\partial y}{\partial n_1^{(p)}} \frac{ \partial n^{(p)}_1}{\partial n^{(p)}_{2}}  \cdots \frac{\partial n^*}{\partial \mu}. \]
\begin{figure}[t]
\begin{tikzpicture}
\node[] at (-0.75,1.5) {Path p=1};
\node[input node] (i) at (-0.75,0) {$\emptyset$};
\node[classical node, fill=lightgray, anchor=center] (c1) at (0,0) {\phantom{.}};
\node[quantum node, anchor=center] (q1) at (1,0) {\phantom{.} \nodepart{two} \phantom{.}};
\node[quantum node, fill=lightgray, anchor=center] (q2) at (1,0.75) {\phantom{.} \nodepart{two} \phantom{.}};
\node[classical node, anchor=center] (c2) at (2,0) {\phantom{.} };
\node[anchor=center] (mid1up) at (2.5,0.325) {};
\node[anchor=center] (mid1down) at (2.5,-0.325) {};
\node[quantum node, anchor=center] (q3) at (3.5,0.75) {\phantom{.} \nodepart{two} \phantom{.}};
\node[classical node, fill=lightgray, anchor=center] (c3) at (3,-0.75) {\phantom{.} };
\node[classical node, fill=lightgray, anchor=center] (c4) at (4,-0.75) {\phantom{.} };
\node[anchor=center] (mid2up) at (4.5,0.325) {};
\node[anchor=center] (mid2down) at (4.5,-0.325) {};
\node[classical node, anchor=center] (c5) at (5,0) {\phantom{.} };
\node[anchor=center] (mid3up) at (5.5,0.325) {};
\node[anchor=center] (mid3down) at (5.5,-0.325) {};
\node[output node, anchor=center] (o1) at (6,0.75) {$\textcolor{gray}{y_1}$ };
\node[output node, anchor=center] (o2) at (6,0) {$y_2$ };
\node[output node, anchor=center] (o3) at (6,-0.75) {$\textcolor{gray}{y_3}$ };
\draw[->, gray] (i) -- (c1);
\draw[->, gray] (c1) -- (q1);
\draw[->] (q1) -- (c2);
\draw[->, gray] (q2) -- (q3);
\draw[->, gray] (c2) -| (mid1down)|- (c3);
\draw[->] (c2) -| (mid1up) |- (q3);
\draw[->] (c3) -- (c4);
\draw[->, gray] (c4) -| (mid2up) |- (c5);
\draw[->] (q3) -| (mid2down) |- (c5);
\draw[->, gray] (c5)++(0,4pt) -| (mid3up) |- (o1);
\draw[->] (c5) -- (o2);
\draw[->, gray] (c5)++(0,-4pt) -| (mid3down) |- (o3);
\node[classical node, anchor=center] (c5) at (5,0) {\phantom{.} };

\end{tikzpicture}\\
\begin{tikzpicture}
\node[] at (-0.75,1.5) {Path p=2};
\node[input node] (i) at (-0.75,0) {$\emptyset$};
\node[classical node, fill=lightgray, anchor=center] (c1) at (0,0) {\phantom{.}};
\node[quantum node, anchor=center] (q1) at (1,0) {\phantom{.} \nodepart{two} \phantom{.}};
\node[quantum node, fill=lightgray, anchor=center] (q2) at (1,0.75) {\phantom{.} \nodepart{two} \phantom{.}};
\node[classical node, anchor=center] (c2) at (2,0) {\phantom{.} };
\node[anchor=center] (mid1up) at (2.5,0.325) {};
\node[anchor=center] (mid1down) at (2.5,-0.325) {};
\node[quantum node, fill=lightgray, anchor=center] (q3) at (3.5,0.75) {\phantom{.} \nodepart{two} \phantom{.}};
\node[classical node, anchor=center] (c3) at (3,-0.75) {\phantom{.} };
\node[classical node, anchor=center] (c4) at (4,-0.75) {\phantom{.} };
\node[anchor=center] (mid2up) at (4.5,0.325) {};
\node[anchor=center] (mid2down) at (4.5,-0.325) {};
\node[classical node, anchor=center] (c5) at (5,0) {\phantom{.} };
\node[anchor=center] (mid3up) at (5.5,0.325) {};
\node[anchor=center] (mid3down) at (5.5,-0.325) {};
\node[output node, anchor=center] (o1) at (6,0.75) {$\textcolor{gray}{y_1}$ };
\node[output node, anchor=center] (o2) at (6,0) {$y_2$ };
\node[output node, anchor=center] (o3) at (6,-0.75) {$\textcolor{gray}{y_3}$ };
\draw[->, gray] (i) -- (c1);
\draw[->, gray] (c1) -- (q1);
\draw[->] (q1) -- (c2);
\draw[->, gray] (q2) -- (q3);
\draw[->] (c2) -| (mid1down)|- (c3);
\draw[->, gray] (c2) -| (mid1up) |- (q3);
\draw[->] (c3) -- (c4);
\draw[->] (c4) -| (mid2up) |- (c5);
\draw[->] (q3) -| (mid2down) |- (c5);
\draw[->, gray] (c5)++(0,4pt) -| (mid3up) |- (o1);
\draw[->] (c5) -- (o2);
\draw[->, gray] (c5)++(0,-4pt) -| (mid3down) |- (o3);
\node[classical node, anchor=center] (c5) at (5,0) {\phantom{.} };

\end{tikzpicture}
\caption{Illustration of the two paths that lead from the second output $y_2$ back to the first quantum node, with respect to whose parameter we want to differentiate $y_2$.}
\label{}
\end{figure}

This means that we need to be able to compute two types of gradients for each node: The derivative with respect to the input from a previous node, as well as the derivative for each node parameter. Note that while $\partial_{\mu \in \theta_t} y_s$ is a scalar gradient, the gradients in between may be multi-dimensional tensors.\\

The gradients for classical nodes are straight forward, and can be computed by known methods such as (classical) automatic differentiation. However, for quantum nodes this is not obvious. We therefore show how to do quantum automatic differentiation.

\subsubsection{Gradients of quantum nodes}

Quantum nodes map a set of parameters  $\theta_q \subset \theta$ to an estimate of an expectation value $E = \mathrm{tr}\{ \rho(\theta_q ) O\}$ for an operator $O$ with regards to a state $\rho(\theta_q)$ prepared by the variational circuit. For convenience we will drop the subscript $q$ in this section. We will also neglect inputs $x$ to the variational circuit. Derivatives with respect to the node inputs work exactly as if they were model parameters.\\

Formally, we have
\[ \partial_{\mu} \bra{0}U^{\dagger} \hat{O} U \ket{\psi} =  \bra{0}(\partial_{\mu}U^{\dagger}) \hat{O} U \ket{0} + \bra{0}U^{\dagger} \hat{O} (\partial_{\mu} U) \ket{0}, \]

This expression contains the \textbf{circuit derivative},
$\partial_{\mu}  U(\theta)$. Assume that only the $l$th layer depends
on circuit parameter $\mu$, then
\[
\partial_{\mu}  U(\theta) =  \mathcal{L}_1^{\dagger} \cdots (\partial_{\mu} \mathcal{L}_l^{\dagger})\cdots \mathcal{L}_D^{\dagger}.
\]
We call the expression $\partial_{\mu} \mathcal{L}_l^{\dagger}$  the
\textbf{layer derivative}. The layer is decomposed into different
gates from our universal gate set. Let $\mu$ be the parameter of the
$r$th gate $\G_r(\mu)$ (where $\G$ can also depend on other
parameters). The derivative of a layer with respect to $\mu$ is then the product of all the gates, but with $\G_r$ replaced by $\partial_{\mu} \G_r(\mu)$.
This expression
is in general not a quantum gate, and in particular not necessarily a
member of our elementary gate set.

\subsubsection{Gradients of quantum gates}

\maria[FOR NOW MATERIAL IS IN THE APPENDIX]


\section{Error analysis}

Even for analytic differentiation, the gradients are   associated with errors due to the fact that outcomes from quantum nodes are estimates of expectations. This section analyses the propagation and bounds on this error. It also discusses prospects of noise having desirable side effects such as regularisation (known from dropout and similar techniques) and faster convergence of training (as known from Stochastic Gradient Descent training).

\subsection{Error propagation}








%\bibliographystyle{unsrt}
\bibliography{openqml_formalism}

\appendix




\clearpage
\appendix
\section{Gradient computation (Ville)}

In this section we explore issues related to computing gradients of variational quantum circuits.
\ville{Most things in this section are probably not going to make it to the final version,
I'm just organizing my notes on the gradient computation problem here.}


\subsection{Gradient formula for $n$-parameter gates}
\label{sec:gradient_formula_for_n_parameter_gates}
For unitary gates generated by a linear combination of Hermitian generators
$G = \sum_k \theta_k B_k$,
\be
U_{\vec{\theta}} = \exp(-i \sum_k \theta_k B_k) = \exp(-i G),
\ee
we obtain the following formula~\cite{dynamo_manual}:
\be
\pd{}{\theta_k} U_{\vec{\theta}}
= -i \underbrace{\left(B_k +\frac{-i}{2!}\ad_G B_k +\frac{(-i)^2}{3!}\ad_G^2 B_k +\ldots \right)}_{Q_k}
U_{\vec{\theta}}.
\ee
Thus the derivative of the state~$\rho$ propagated by $U_{\vec{\theta}}$ is given by
\begin{align}
  \notag
  \pd{}{\theta_k} \Ad_{U_{\vec{\theta}}} \rho
  &= \pd{}{\theta_k} (U_{\vec{\theta}} \rho U_{\vec{\theta}}^\dagger)
  = \left(\pd{}{\theta_k} U_{\vec{\theta}}\right) \rho U_{\vec{\theta}}^\dagger +\text{h.c.}\\
  %&= -i Q_k U_{\vec{\theta}} \rho U_{\vec{\theta}}^\dagger +\text{h.c.}
  &= -i \ad_{Q_k} \Ad_{U_{\vec{\theta}}} \rho,
\end{align}
where $\Ad_{A} B = A B A^{-1}$ and $\ad_{A} B = A B - B A$.
Assume now we have a propagator U, and the initial state of the system is~$\rho$.
The expectation value of the observable~$Q$ after the propagation is obtained as
\be
\expect{Q}_U = \tr((\Ad_{U}\rho) Q)
= \tr(\rho \Ad_{U^\dagger}Q).
\ee
With $U=B U_{\vec{\theta}} A$, the derivative of the expectation value is given by
\begin{align}
  \label{eq:d_ev}
  \notag
  \pd{}{\theta_k} \expect{Q}_{B U_{\vec{\theta}} A}
  &= \tr(Q \Ad_B (\pd{}{\theta_k} \Ad_{U_{\vec{\theta}}}) \Ad_A \rho)\\
  &= \tr(Q \Ad_B (-i \ad_{Q_k}) \Ad_{U_{\vec{\theta}} A} \rho).
\end{align}

\ville{The usefulness of this formula depends on whether we can sum the series~$Q_k$ easily enough.
  However, evaluating the $\ad_{Q_k}$ part on a quantum computer is the real problem.
}

\subsubsection{One-parameter gates}

In the case where we only have a single generator (one-parameter subgroup, see below),
$G = \theta_1 B_1$, the commutator $\ad_G B_1 = 0$
and the series terminates at the first term, yielding
$Q_1 = B_1$.

\subsubsection{Displacement}

The displacement gate is given by
\begin{align}
  D(\vec{\theta}) &= D(\re z, \im z) = \exp\left( za^\dagger -z^* a\right)\\
  \notag
&= \exp\left(-i\left(\re(z)(-i)(a-a^\dagger) -\im(z)(a+a^\dagger)\right)\right).
\end{align}
The generators are $\{B_k\}_k = \{-i(a-a^\dagger), -(a+a^\dagger)\}$,
and the corresponding parameters are the $\theta_1 = \re z$ and $\theta_2 = \im z$.
Now we have
\begin{align*}
  -i \ad_G B_1 &= -i\comm{i(za^\dagger -z^* a)}{-i(a-a^\dagger)}
  = -2 \theta_2 \I,\\
  -i\ad_G B_2 &= -i\comm{i(za^\dagger -z^* a)}{-(a+a^\dagger)}
  = 2 \theta_1 \I,
\end{align*}
and thus
\begin{align*}
Q_1 &= B_1 -\theta_{2} \I,\\
Q_2 &= B_2 +\theta_{1} \I.
\end{align*}
The series terminate already at the second term, and furthermore
the identity terms do not matter in a commutator.
We obtain $\ad_{Q_k} = \ad_{B_k}$, and
hence displacement gates can be handled exactly the same way as one-parameter gates.
\cg{I guess, another way of seeing this is that different displacements commute. Once can thus always decompose any displacement gate into two commuting one parameter gates, e.g., $D(\re z, \im z) \propto D(\re z, 0) D(0, \im z)$.}

\subsection{One-parameter subgroups in quantum mechanics}

A one-parameter subgroup of the topological group~$G$ is defined by a continuous group homomorphism
\be
\phi: \R \to G,
\ee
where $\R$ is a group with respect to addition.
The homomorphism property means that
\be
\phi(a)\phi(b) = \phi(a+b) \quad \forall a,b \in \R,
\ee
which immediately yields
$\phi(0) = e$ and $\phi(-a) = \phi(a)^{-1}$,
where $e$~is the identity element of~$G$.
$\Imag \phi$ is always a subgroup of~$G$.

The kernel of the homomorphism is defined as
\be
\Ker \phi = \{a \in \R | \phi(a) = e\}.
\ee

\begin{theorem}
  \label{th:1psg}
One-parameter subgroups are either isomorphic to~$\R$, or their parametrization~$\phi$ is periodic.

\begin{proof}
$\phi$ is an injection iff $\Ker \phi = \{0\}$ (trivial kernel).
In this case the subgroup $\phi(\R) < G$ is isomorphic to~$\R$.
Let us next assume that the kernel is nontrivial, i.e. for some $p \neq 0$ we have
$\phi(p) = e$.
% p \in \Ker \phi \implies n*p \in \Ker \phi
This is equivalent to
\be
\phi(a+p) = \phi(a)\phi(p) = \phi(a) \quad \forall a \in \R,
\ee
and the map~$\phi$ is thus periodic. Let us use $\tilde{p}>0$
to denote the period, i.e.
$\tilde{p} = \min \{|p| \neq 0 | p \in \Ker \phi\}$.
Now the kernel is seen to be $\Ker \phi = \{n \tilde{p} | n \in Z\}$.
\end{proof}
\end{theorem}

% https://en.wikipedia.org/wiki/Stone%27s_theorem_on_one-parameter_unitary_groups
Stone's theorem on one-parameter unitary groups states that
% for every strongly continuous one-parameter family of unitary operators on a Hilbert space there is a unique Hermitian generator, and that
for every Hermitian generator~$G$, the homomorphism
\be
\label{eq:1pug}
\phi(\theta) = U_\theta = \exp(-i \theta G)
\ee
defines a strongly continuous one-parameter unitary group.
This holds in both finite-dimensional and infinite-dimensional cases.
The unitary and its generator commute.

\ville{The reason why I'm looking into the periodicity of the subgroup $U_\theta$ is that I was originally
  overoptimistic and though that periodicity might be sufficient to make the gradient trick~\ref{sec:gradient_trick} work. It probably isn't.
  Might be necessary though.}
\cg{Without having thought about it much, I don't expect the periodicity to be relevant for the gradient trick. The intuition is that whether a subgroup is non-periodic or just has a very long period depends on very fine tuned properties of the $\lambda_i$ and so it only matters at long times, whereas the gradient is about infinitesimal changes (This is not a rigorous argument of course). Due to the quantum version of the Poincar\'{e} recurrence theorem, actually, for any $\epsilon$ and any finite dimensional $G$ there is a time $T$ (possibly very large) such that $\|\exp(-i T G) - \I \|_\infty \leq \epsilon$.}

\subsubsection{Finite-dimensional Hilbert spaces}


Assume we are given a hermitian $n \times n$ generator~$G$, with the eigendecomposition
\be
G = V \diag(\lambda_1, \ldots, \lambda_n) V^\dagger.
\ee
%Tracelessness yields $\sum_k \lambda_k = 0$.
%$G$~generates a one-parameter subgroup of the unitary group~$SU(n)$
Now using Eq.~\eqref{eq:1pug} we obtain
\be
U_\theta
= V \diag(e^{-i\lambda_1 \theta}, \ldots, e^{-i\lambda_n \theta}) V^\dagger,
\ee
where $\theta \in \R$. Using Th.~\ref{th:1psg} we find that
\begin{align}
  & U_\theta \: \text{is periodic with period} \: \tilde{\theta} > 0\\
  \iff \: & \Ker U = \{n \tilde{\theta} | n \in Z\}\\
  \iff \: & \lambda_k \tilde{\theta} = n_k 2 \pi \quad \forall k, \: \text{where} \: n_k \in \Z\\
  \iff \: & \frac{\lambda_i}{\lambda_j} = \frac{n_i}{n_j} \quad \forall i,j
  \: \text{for all nonzero eigenvalues.}
\end{align}

Let us extend the definition of least common multiple to real numbers:
\be
\lcm(a_1, a_2, \ldots) := \min \{q | \forall i \exists n_i \in \Z: a_i n_i = q>0\}.
\ee
Such a number exists iff $a_i$ are rational multiples of each other.
Now
\be
\tilde{\theta} = 2\pi \: \lcm \{\lambda^{-1} \:|\: \lambda \in \spec(G) \setminus \{0\}\}.
\ee

In quantum mechanics the global phase of a propagator has no physical meaning, so we may always add
a multiple of identity to the generator~$G$ without affecting the physics.
This is equivalent to shifting all the eigenvalues by the same amount.
Hence the periodicity conditions are relaxed a bit. In particular, we may always make one eigenvalue zero.


Qubits:
$G$~has only two eigenvalues, so $U(\theta)$ is always periodic (modulo the phase)
with $\tilde{\theta} = 2\pi/|\lambda_2-\lambda_1|$.

Qudits:
Now almost all generators~$G$ have eigenvalues that are not rational multiples of each other.
These~$G$ generate a one-parameter subgroup of $U(d)$ isomorphic to~$\R$.
However, some generators~$G$ always generate a periodic subgroup.


\subsection{Attempts at extending the gradient trick}

Let us assume $G$ is such that there is a value~$\theta$ which yields
\be
\label{eq:U_IG}
U_\theta = a\I +bG.
\ee
Adjusting global phase, we may always choose $a \in \R$.
\begin{align}
\label{eq:AdUtheta}
\notag
\Ad_{U_\theta} \rho &=
|a|^2 \rho +\re(ab^*)\acomm{G}{\rho}
-i\im(ab^*)\comm{G}{\rho} +|b|^2 G\rho G\\
&=
a^2 \rho +a\re(b)\acomm{G}{\rho}
+ia\im(b)\comm{G}{\rho} +|b|^2 G\rho G.
\end{align}
\ville{The idea is to add a few of these expressions together with different values of $\theta$
such that only the $\comm{G}{\rho} = \ad_G \rho$ term remains.}

Unitarity also requires
\begin{align}
\notag
&U_\theta U_\theta^\dagger = a^2\I +2\re(ab^*) G +|b|^2 G^2 = \I\\
\iff \:&
2a\re(b) G +|b|^2 G^2 = (1-a^2)\I,
\end{align}
which limits the form $G$ can take.

Another possible approach:
Quite generally we have
\begin{align}
\notag
\Ad_{U_\theta}
&= \Ad_{\exp(-i \theta G)}
= \exp(-i \theta \ad_{G})\\
&= \I +(-i\theta)\ad_G +\frac{(-i\theta)^2}{2!} \ad_G^2 +\ldots
\end{align}
We may eliminate all even terms in the series by projecting to the odd part:
\begin{align}
\notag
\frac{1}{2}\left(\Ad_{U_\theta} -\Ad_{U_{-\theta}})\right)
&= (-i\theta)\ad_G +\frac{(-i\theta)^3}{3!} \ad_G^3 +\ldots
\end{align}


\ville{Can we express $\ad_G$ (for some relevant $G$) using a linear combination of these series expressions with different values of~$\theta$?}

\ville{In the case where $G$ only has two unique eigenvalues, the above series collapses to $\ad_G$ times a scalar sine series, see below.}

\subsection{Gradient trick}
\label{sec:gradient_trick}

Let us assume that $G'$ has only two unique eigenvalues, $\lambda$ and~$\lambda+2r$.
We may now define $G = G' -(\lambda+r)\I$ with the unique eigenvalues
$\pm r$, and obtain $G^2 = r^2\I$.\footnote{
Alternatively we may define $G = G' -\lambda \I$ with the unique eigenvalues
$\{0, 2r\}$, and obtain $G^2 = 2r G$ (a scaled projector).
This yields the same expression for $U_\theta$ modulo a global phase.}


Now we may write the Taylor series for the exponential
and split it into a cosine and a sine series:
\begin{align*}
U_\theta &= \exp(-iG\theta) = \sum_{k=0}^\infty \frac{(-i\theta)^k G^k}{k!}\\
&=
\sum_{k=0}^\infty \frac{(-i\theta)^{2k} G^{2k}}{(2k)!}
+\sum_{k=0}^\infty \frac{(-i\theta)^{2k+1} G^{2k+1}}{(2k+1)!}\\
&=
\I \sum_{k=0}^\infty \frac{(-1)^{k} (\theta r)^{2k}}{(2k)!}
-i G/r \sum_{k=0}^\infty \frac{(-1)^{k} (\theta r)^{2k+1}}{(2k+1)!}\\
&=
\I \cos(r \theta)
-i G/r \sin(r \theta).
\end{align*}

%For such a scaled idempotent generator we obtain
%\begin{align*}
%U_\theta
%&= \exp(-iG\theta) = \sum_{k=0}^\infty \frac{(-i\theta)^k G^k}{k!}\\
%&= \I +\sum_{k=1}^\infty \frac{(-i\theta)^k (2r)^{k-1}}{k!} G\\
%&= \I +\frac{1}{2r} \sum_{k=1}^\infty \frac{(-i 2r \theta)^k}{k!} G\\
%&= \I +\frac{G}{2r} (\exp(-i 2r \theta)-1).
%\end{align*}


This propagator is of the form presented in Eq.~\eqref{eq:U_IG},
with $a=\cos(r\theta)$ and $b=-i/r \sin(r\theta)$.
All the unwanted terms in Eq.~\eqref{eq:AdUtheta} are seen to be even functions of~$\theta$,
and thus we obtain
\be
\Ad_{U_\theta} -\Ad_{U_{-\theta}}
=
-i r^{-1} \sin(2r\theta) \ad_G.
\ee
The norm of this expression is maximized when $\theta = \pi/(4r)$,
and using the unitary
\be
W := U_{\pi/(4r)} = \frac{1}{\sqrt{2}}(\I -ir^{-1}G)
\ee
we finally obtain the convenient formula
\be
\label{eq:adAdformula}
-i\ad_G = r(\Ad_W-\Ad_{W^\dagger})
\ee
which we may use in Eq.~\eqref{eq:d_ev} to compute the gradient of the expectation value.
Adding a multiple of identity to~$G$ does not affect either~$\ad_G$ or~$\Ad_{U_{\theta}}$,
so this formula works for any~$G$ with two unique eigenvalues (and $r$~is half the difference of these eigenvalues).

\ville{To me, the remarkable part about the trick is that we can compute the exact partial derivative of the circuit at~$\vec{\theta}$,
  representing a local property, by evaluating the circuit at two points a macroscopic distance away from~$\vec{\theta}$.}


Qubits:
This trick works for arbitrary single-qubit gates, generated by a linear combination of Pauli matrices
$G = \vec{a} \cdot \vec{\sigma}/2$
where $|\vec{a}|=1$.
We obtain $G^2 = \frac{1}{4} \I$, and thus $r=1/2$.

Qudits (including the multi-qubit case) and CV modes:
Gates generated by a $G$ that has only two unique eigenvalues are amenable to the gradient trick.



\section{Gradient computation (Christian)}
\cg{Hi! I left some comments also in other parts of this manuscript and hope that is ok. Nathan told me to have a closer look at the gradient problem. I don't have any ground breaking insights (and probably you have thought about some of the things I will write below already), but maybe some of the following observations can be useful:}

\ref{sec:gradient_formula_for_n_parameter_gates}

The gradient trick, as described in \ref{sec:gradient_trick}, works for arbitrary generators that are both Hermitian and unitary (or at least unitary up to a constant factor).
In finite dimensional Hilbert spaces, such operators thus have only two different eigenvalues.
Up to rescaling and addition of identities they can thus be transformed into projectors.
Adding identities to generators doesn't change the expectation value and gradient at all (as the global phase does not matter) and rescaling simply rescales the gradient.
The gradient trick, as explained in Section~\ref{sec:gradient_trick}, can thus be straightforwardly applied to all circuits whose gates are generated by projectors (I am not really saying anything new, just paraphrasing what is already explained in the end of Section~\ref{sec:gradient_trick}).

At least for simgle-parameter gates with $G = \theta_1 B_1$ arbitrary hermitian, one can ``extend'' the gradient trick with the following reasoning (spoiler: this is not very practical):
Any hermitian operator $B_1$ can obviously be decomposed in into its eigendecomposition $B_1 = \sum_j b_1^{(j)} \, B_1^{(j)}$ with $b_1^{(j)}$ real and $B_1^{(j)} = {B_1^{(j)}}^2$ commuting orthogonal a projectors and therefore we can write $U_{\theta_1} = \exp(-i \theta_1 \sum_j b_1^{(j)} \, B_1^{(j)}) = \prod_j \exp(-i \theta_1 b_1^{(j)} \, B_1^{(j)}) = \prod_j U^{(j)}_{\theta_1}$.
One can then try to estimate the gradient $\pd{}{\theta_1} \expect{Q}_{B U_{\theta_1} A}$ by ``pretending that that one has actually more parameters'' from estimates of all the $\pd{}{\theta_1^{(j)}} \expect{Q}_{B \prod_k U^{(k)}_{\theta_1^{(k)}} A}$.
In fact, as $2 B_1^{(j)} - \I$ has the right properties to make the gradient trick work, if one can actually perform all the unitaries
\begin{equation}
  W^{(j)} = \frac{1}{\sqrt{2}}(\I - i (2 B_1^{(j)} - \I)) ,
\end{equation}
one can estimate all of these gradients because the general formula \eqref{eq:d_ev} and that fact that all the $B_1^{(j)}$ and $U^{(j)}$ commute imply that
\begin{align}
    \label{eq:d_ev_eigendecomposed}
  &\pd{}{\theta_1^{(j)}} \expect{Q}_{B \prod_k U^{(k)}_{\theta_1^{(k)}}  A} \\
  = &\tr(Q \Ad_{B \prod_{k<j} U^{(k)}_{\theta_1^{(k)}}} ((-i b_1^{(j)}/2) \ad_{2 B_1^{(j)} - \I}) \times\\ &\Ad_{U_{\theta_1^{(k)}} \prod_{k>j} U^{(k)}_{\theta_1^{(k)}} A} \rho)\\
  = &\tr(Q \Ad_B ((-i b_1^{(j)}/2) \ad_{2 B_1^{(j)} - \I}) \Ad_{\prod_{k} U^{(k)}_{\theta_1^{(k)}} A} \rho).     \label{eq:d_ev_eigendecomposed_end}
\end{align}
The linearity of $\ad_{\sum_k b_1^{(k)} B_1^{(k)}} = \sum_k b_1^{(k)} \ad_{B_1^{(k)}}$ and the fact that $\prod_{k} U^{(k)}_{\theta_1^{(k)}} \big|_{\theta_1^{(k)} = \theta_1} = U_{\theta_1}$ then implies that the gradient $\pd{}{\theta_1} \expect{Q}_{B U_{\theta_1} A}$ one is actually interested in, can be recovered as a linear combinations of the gradients in \eqref{eq:d_ev_eigendecomposed} -- \eqref{eq:d_ev_eigendecomposed_end}.
\cg{Well, as I said, this is not very practical.}
\ville{I implemented this in Python and it seems to work. It's not that terrible either, when you have a small number of unique eigenvalues.
I would put it like this:
Let $G = \theta B$, where the hermitian generator~$B$ is spectrally decomposed as~$B = \sum_j b_k B_k$. Now, since $\ad$ is linear, we have
from Eq.~\eqref{eq:d_ev} and Eq.~\eqref{eq:adAdformula}
\begin{align}
  \pd{}{\theta} \expect{Q}_{F U_{\theta} E}
  &= \tr(Q \Ad_F (-i \ad_{B}) \Ad_{U_{\theta} E} \rho)\\
  \notag
  &= \tr(Q \Ad_F (\sum_j b_j (-i \ad_{B_j})) \Ad_{U_{\theta} E} \rho)\\
  \notag
  &= \tr(Q \Ad_F (\sum_j \frac{b_j}{2} (\Ad_{W_j} -\Ad_{W_j^\dagger})) \Ad_{U_{\theta} E} \rho),
\end{align}
where $W_j = \exp(-i (\pi/2) B_j)$.
When simulating you don't have to evaluate the full circuit many times, even though the sum in the middle requires multiple $\Ad$ operations.
Of course one has to be able to implement all the $W_j$ gates on the quantum device.
}
\cg{Nice! So what I did was not so useless after all :-) On the hardware, implementing the $W_j$ is however completely impossible, I fear.}


\cg{I explored a number of further avenues based on results in (at first sight maybe unrelated fields that I remembered):}
\begin{description}
\item [\cite{Pati2015}] As promised, I read this paper, which is terribly written. It is not so transparent, but the idea explained there in a very involved language seems to be nothing but a strategy to meaure in a post-selected ensemble. In the end this is more or less the same trick as in the Mitarai paper, so nothing exiting here. 
\item [\cite{Garttner2017}] This is just one of several refernces I expored to see if the tools to measaure out of time ordered correlation functions (which are also non-hermition operators) could help us with the gradient estimation. After giving these papers a closer look I eneded up rather disappointed. All methods that I could find work only because of special assmptions on the states, such as them beein eigenstates of one of the observables making up the OTOC. Whithout such constain they do not work. 
\item [Lemma 4.4 from \cite{Aaronson2011}]
  This is quite interesting! This lemma shows that any arbitrary matrix with small norm can be embedded as a submatrix into a large enough unitary matrix.
  I thought that this should imply a general scheme to measure non-hermitian operators via post selection, but it looks like it only sort of does:
  Say that $X$ is a non-unitary operation that one would like to perform.
  With Lemma 4.4. form \cite{Aaronson2011} one can extend the Hilbert space by adding one qubit initialized in $\ket{0}$ and then embed $\hat{Q}$ into the ``upper left corner'' of a unitary $\chi$ that acts on this larger Hilbert space.
  By performing this unitary, measuring the ancilla qubit, and post-selecting on outcome $0$, corresponding the the POVM element $P_0 = \I \otimes \ketbra{0}{0}$, one can perform on any state $\rho$ in the system of interest the operation
  \begin{equation}
    \rho \mapsto  P_0 \chi^\dagger \rho \chi P_0  \propto  X^\dagger \rho X
  \end{equation} 
  In the notation of Section~\ref{sec:gradient_computation_Maria}, and in particular \eqref{Eq:term_to_estimate}, taking $X = A^{\dagger}  \hat{Q} \G$, this allows to estimate for example
  \begin{equation}
    |\bra{0} U^\dagger A^{\dagger}  \hat{Q} \G U \ket{0}|^2
  \end{equation}
  by starting in the vacuum, doing the preparation circuit $U$, performing the measurement of $\chi$ on the system plus ancilla, measuring the ancilla, doing the inverse $U^\dagger$ of the preparation circuit and then estimating the probability of getting vacuum out conditions on the ancilla measurement having resulted in $0$.
  This is, again, not very practical, and on top of that I don't see how it could be used to estimate 
  \begin{equation}
    \bra{0} U^\dagger A^{\dagger}  \hat{Q} \G U \ket{0}
  \end{equation}
  or the real part of that (which is what we are actually interested in) instead of the absolute value squared.
  Maybe someone else has an idea?
\end{description}

\section{Gradient computation (Maria)}\label{sec:gradient_computation_Maria}
\maria{Adding my attempts and notes here. We can merge the two when ready.}

\subsection{Estimating gradients of expectation values with the LCU technique}

Assume we want to take the derivative of the expectation values
\[ y = \bra{0}U^{\dagger} \G^{\dagger}  V^{\dagger} \hat{O} V \G U \ket{0}\]
of a quantum gate $\G = \G(\mu)$ acting on a single subsystem (i.e., mode or qubit), with respect to the parameter $\mu$ of $\G$. Note that $\G$ could depend on other parameters as well. Here, $U,V$ are the circuits applied before and after this gate. To simplify the notation, we absorb $U$ into the quantum state, and $V$ into the operator and compute $ \bra{\psi} \G^{\dagger}  \hat{Q} \G \ket{\psi}$ instead, with $\hat{Q} = V^{\dagger} \hat{O} V$, and $\ket{\psi}= U \ket{0}$.\\

The derivative of the expectation is formally given by
\begin{equation}
	\partial_{\mu} y = \bra{\psi} ( \partial_{\mu}\G^{\dagger})  \hat{Q} \G \ket{\psi} + \bra{\psi} \G^{\dagger}  \hat{Q}  (\partial_{\mu}\G) \ket{\psi}
	\label{Eq:der_of_exp}
\end{equation}

In matrix notation, the expression $\partial_{\mu}\G$ is the entry-wise gradient of $\G$. Since this is in general a complex square matrix, we can always decompose it into the weighted sum of two unitary matrices $A$ and $B$ [ADD PROOF],
\[\partial_{\mu}\G = \alpha A + \beta B \]
with real $\alpha, \beta$. To be more general, for example if there is a convenient decomposition into more or less than $2$ unitaries, let us assume that
\begin{equation} 
	\partial_{\mu}\G = \sum_{k=1}^K \alpha_k A_k, 
	\label{Eq:deriv_decomp}
\end{equation}
for real $\alpha_k$. The gradient becomes
\begin{equation*}
	\partial_{\mu} y  = \sum_{k=1}^K \alpha_k  \left( \bra{\psi} A_k^{\dagger}  \hat{Q} \G \ket{\psi} +  \bra{\psi}\G^{\dagger} \hat{Q} A_k  \ket{\psi} \right).
\end{equation*}
Let us look at one of these terms,
\begin{equation}
	\bra{\psi} A^{\dagger}  \hat{Q} \G \ket{\psi} +  \bra{\psi}\G^{\dagger} \hat{Q} A  \ket{\psi},
\label{Eq:term_to_estimate}
\end{equation}
where here and in the following we drop the subscript $k$ for readability. To estimate this value on a quantum computer, we can instead estimate the two expectations
\begin{equation}
	E_1 = \bra{\psi} (\G + A)^{\dagger} \hat{Q} (\G + A) \ket{\psi},		\label{Eq:E1}
\end{equation}
and
\begin{equation}
	E_2= \bra{\psi}(\G - A)^{\dagger}  \hat{Q} (\G - A) \ket{\psi}
	\label{Eq:E2}
\end{equation}
on the quantum device, and then classically compute $\frac{1}{2}E_1 - \frac{1}{2} E_2$.
\begin{proof}
We have
\begin{eqnarray*}
	E_1 &= &\bra{\psi} \G^{\dagger} \hat{c} \G \ket{\psi} + \bra{\psi} A^{\dagger} \hat{c} A \ket{\psi} \\
	&+& \bra{\psi} \G^{\dagger} \hat{c} A \ket{\psi}+ \bra{\psi} A^{\dagger} \hat{c} \G \ket{\psi}
\end{eqnarray*}
and
\begin{eqnarray*}
	E_2 &= & \bra{\psi} \G^{\dagger} \hat{c} \G \ket{\psi} + \bra{\psi} A^{\dagger} \hat{c} A \ket{\psi} \\
	&-& \bra{\psi} \G^{\dagger} \hat{c} A \ket{\psi}- \bra{\psi} A^{\dagger} \hat{c} \G \ket{\psi}
\end{eqnarray*}
Subtracting the terms cancels the first two expectation values in $E_1$ and $E_2$ and adds the second ones, and the factor of $\frac{1}{2}$ ensures that we have the correct normalization.
\end{proof}

\subsection{Autodiff on a qubit-based quantum computer}

We will now propose a way to estimate $E_1$ and $E_2$ on a qubit-based quantum computer, using a technique called a linear combination of unitaries (LCU). We will first consider the special case of Pauli rotations, in which the procedure is known to simplify considerably [REF MITARAI], and subsequently consider the more general case.\\

\subsubsection{Pauli gates}
Consider the gate $\G(\mu) = \e^{i \mu G}$, where $G$ is hermitian as well as unitary, and therefore a projector, $G^2 = \mathbbm{1}$ [\maria{Tried the more general $G^2 = r \mathbbm{1}$, but ran into trouble with $r$ in the calculation below. Possibly generalise.}]. A prominent example are Pauli operators, and it is known that any single qubit quantum gate can be decomposed into such gates. The gradient of such a gate is given by
\[ \partial_{\mu} \G = i G \e^{i \mu G}.    \]
Then Eq. (\ref{Eq:der_of_exp}) becomes
\[ \partial_{\mu} y =   \bra{\psi} \G^{\dagger} (iG)^{\dagger}  \hat{Q} \mathbbm{1} \G \ket{\psi} + \bra{\psi} \G^{\dagger} \mathbbm{1}  \hat{Q} (iG) \G \ket{\psi} , \]
which has the form of term (\ref{Eq:term_to_estimate}). We can therefore write
\[ \partial_{\mu} y = \frac{1}{2}E_1 - \frac{1}{2} E_2, \]
with
\[E_1 = \bra{\psi} (\mathbbm{1} + iG)^{\dagger} \hat{Q} (\mathbbm{1} + iG) \ket{\psi},\]
and
\[E_2= \bra{\psi}(\mathbbm{1} - iG)^{\dagger}  \hat{Q} (\mathbbm{1} - iG) \ket{\psi}. \]
Since $G$ is a projector, we get
\begin{equation}
	\G(\pm \frac{\pi}{2}) = \frac{1}{\sqrt{2}}(\I \pm iG),
	\label{Eq:Pauli_equality}
\end{equation}

\begin{proof}
We may write the Taylor series for the exponential of $\G(\mu)$
and split it into a cosine and a sine series:
\begin{align*}
\mathcal{G}(\mu) &= \exp(-i\mu G) = \sum_{k=0}^\infty \frac{(-i\mu)^k G^k}{k!}\\
&=
\sum_{k=0}^\infty \frac{(-i\mu)^{2k} G^{2k}}{(2k)!}
+\sum_{k=0}^\infty \frac{(-i\mu)^{2k+1} G^{2k+1}}{(2k+1)!}\\
&=
\I \sum_{k=0}^\infty \frac{(-1)^{k} \mu^{2k}}{(2k)!}
-i G \sum_{k=0}^\infty \frac{(-1)^{k} \mu^{2k+1}}{(2k+1)!}\\
&=
\I \cos(\mu)
-i G \sin(\mu).
\end{align*}
For $\mu = \pm \frac{\pi}{2}$ we get $\G(\pm \frac{\pi}{2}) = \frac{1}{\sqrt{2}}(\I \pm iG)$.
\end{proof}

We conclude that the expectations can be estimated straight from the quantum device, by inserting either a $\G(\frac{\pi}{2})$ or $\G(-\frac{\pi}{2})$ into the original circuit:
\begin{multline*}
	\partial_{\mu} y =   \bra{\psi} \G\left(\mu \right)^{\dagger} \G \left(\frac{\pi}{2} \right)^{\dagger}  \hat{Q} \G\left(\mu \right) \left(\frac{\pi}{2} \right) \G\left(\mu \right) \ket{\psi} \\ + \bra{\psi} \G\left(\mu \right)^{\dagger} \G \left(- \frac{\pi}{2} \right)^{\dagger} \hat{Q} \G \left(-\frac{\pi}{2} \right) \G\left(\mu \right) \ket{\psi}
\end{multline*}

If $G$ is a combination of Pauli operators of the form $\boldsymbol\alpha \boldsymbol\sigma$ where $\boldsymbol\alpha$ is a real $3$-dimensional vector of length $|\boldsymbol\alpha|$ and $\boldsymbol\sigma$ is the vector of Pauli operators, the result of the proof is also known through the famous relation
\[\e^{i \mu (\boldsymbol\alpha \boldsymbol\sigma)} = \mathbbm{1} \cos(|\boldsymbol\alpha| \mu) + i (\vec{n} \vec{\sigma}) \sin(|\boldsymbol\alpha| \mu).\] 


Since most quantum architectures implement rotations about the three Pauli axes, this ``angle gradient trick'' is in many contexts sufficient to build and automatically differentiate variational circuits on quantum computers. It includes other important cases of hardware-specific gates, from which the standard gate set might be originally composed. For example, the Xmon qubits in Google's superconducting chip naturally implement the three gates [REF:Qcirc documentation]
\begin{eqnarray*} 
&\text{ExpW}(\mu, \delta) &= \exp \left(- i \mu \left( \cos (\delta) \sigma_x + \sin(\delta)\sigma_y\right) \right)\\
&\text{ExpZ}(\mu) &= \exp \left(- i \mu \sigma_z \right)\\
&\text{Exp11}(\mu) &= \exp \left(- i \mu \ketbra{11}{11} \right), 
\end{eqnarray*}
which, as (linear combinations of) Pauli gates and projectors, can be handled with the gradient trick. Also Pauli-based $2$-qubit gates, such as the microwave-controlled transmon gate for superconducting architectures\footnote{Here, the time-dependent prefactors are summarised as the gate parameter $\mu$, while $b=\frac{J}{\Delta_{12}}$ represents the quotient of the interaction strength $J$ and detuning $\Delta_{12}$ between the qubits, and $c =m_{12}$ is a cross-talk factor.} \maria{CHECK!} [ REF: https://arxiv.org/pdf/1106.0553.pdf],
\begin{equation*}
\G(\mu) = \exp \left( \mu (\sigma_x \otimes \mathbbm{1} -  b \sigma_z \otimes \sigma_x + c \mathbbm{1} \otimes \sigma_x)  \right),
\end{equation*}
are derivable with the angle-based framework.  

\subsubsection{General case}

\begin{figure}[t]
$$
\Qcircuit @C=1em @R=.7em {
\lstick{\ket{0}} &  \gate{H}  &\ctrlo{1} & \ctrl{1}&  \gate{H} & \meter & \cw & \rstick{c}\\
\lstick{\ket{\psi}} &   \qw &\gate{\G} & \gate{A} & \qw  & \qw & \qw & \rstick{\ket{\psi'}}  \\
}
$$
\caption{LCU for qubits.}
\label{Fig:lcu_qubits}
\end{figure}

In the general case we need a trick to estimate expectations (\ref{Eq:E1}) and (\ref{Eq:E2}). One possibility is a linear combination of unitaries [REF].

Prepare the state
\[ \frac{1}{\sqrt{2}} \left( \ket{0} + \ket{1}  \right)\otimes \ket{\psi}, \]
where the first qubit is an ancilla. Now apply $\G$ from Equation (\ref{Eq:term_to_estimate}) conditioned on the ancilla being in state $0$, and $A$ conditioned on the ancilla being in state $1$,
\[ \frac{1}{\sqrt{2}} \left( \ket{0} \G \ket{\psi} + \ket{1} A \ket{\psi} \right). \]
A second Hadamard prepares
\[ \frac{1}{2} \left( \ket{0} (\G + A) \ket{\psi} + \ket{1} (\G - A) \ket{\psi} \right). \]
A measurement of the ancilla selects one of the two branches and results in state
$\frac{1}{2 \sqrt{p_0}} (\G + A) \ket{\psi}$ with probability
\[p_0  = \frac{1}{4} \bra{\psi} (\G + A)^{\dagger} (\G + A) \ket{\psi},\]
or in state $\frac{1}{2\sqrt{p_1}} (\G - A) \ket{\psi}$ with probability
\[p_1 = \frac{1}{4} \bra{\psi} (\G - A)^{\dagger} (\G - A) \ket{\psi}.   \]
Note that this is not a conditional measurement: Either result contributes to the final estimate. We measure the expectation of $\hat{Q}$ with respect to the final state. If the ancilla was in state $0$, we contribute the result to the estimation of
\[\tilde{E}_0 =  \frac{1}{4 p_0} \bra{\psi} (\G + A)^{\dagger} \hat{Q} (\G + A) \ket{\psi}, \]
and if the ancilla was in state $1$ we contribute the result towards the estimation of
\[\tilde{E}_1 =  \frac{1}{4 p_1} \bra{\psi} (\G - A)^{\dagger} \hat{Q} (\G - A) \ket{\psi}. \]
We can now use our knowledge of $p_0, p_1$ from the measurement together with $E_0 = 4 p_0 \tilde{E}_0$ and $E_1 = 4 p_1 \tilde{E}_1$ to compute the desired estimate (\ref{Eq:term_to_estimate}),
\[\bra{\psi} A^{\dagger}  \hat{Q} \G \ket{\psi} +  \bra{\psi}\G^{\dagger} \hat{Q} A  \ket{\psi} = 2 p_0 \tilde{E}_0 - 2 p_1 \tilde{E}_1.\]



Overall, on top of the ``normal operation mode'' of the device, this routine requires that we can apply the gate as well the unitaries from the derivative decomposition controlled by an ancilla. We need to estimate $K$ expectation values altogether. Furthermore, the decomposition of the derivative gate into (single-subsystem)  unitaries that can be implemented by the device has to be worked out once. We will show below that for interesting cases in qubit and CV architectures, the $A_k$ are represented by a single qubit gate.

\subsubsection{Decomposition of derivatives of single qubit gates}
For the LCU trick to work in general, we need a decomposition of the derivative gate as in Eq. (\ref{Eq:deriv_decomp}). As remarked in \cite{schuld18cc}, for general single qubit gates parameterized as

\begin{equation*} \label{eq:1q:parametrization}
\G (\alpha,\beta, \gamma, \phi ) = \e^{i\phi} \begin{pmatrix} \e^{i\beta} \cos \alpha &  \e^{i\gamma} \sin \alpha\\ -\e^{-i\gamma} \sin \alpha &  \e^{-i\beta} \cos \alpha \end{pmatrix}
\end{equation*}
one can use the ``parameter shift'' rule as
\begin{eqnarray*}
 \partial_{\alpha} \G  &=&  \G(\alpha + \frac{\pi}{2}, \beta, \gamma ),   \\
 \partial_{\beta} \G &=& \frac{1}{2} \G(\alpha, \beta + \frac{\pi}{2} , 0  ) + \frac{1}{2}  \G(\alpha, \beta + \frac{\pi}{2} , \pi ),  \\
\partial_{\gamma} \G &=& \frac{1}{2} \G(\alpha, 0, \gamma + \frac{\pi}{2}  ) + \frac{1}{2}  \G(\alpha, \pi, \gamma + \frac{\pi}{2} ).
\end{eqnarray*}

\subsection{Autodiff on a CV quantum computer}

We now turn to continuous-variable architectures. As before, the task is to compute 
\[\partial_{\mu}y = \partial_{\mu} \bra{0}U^{\dagger} \G^{\dagger} V^{\dagger} \hat{O}V \G U \ket{0} = \partial_{\mu} \bra{\psi} \G^{\dagger} \hat{Q} \G \ket{\psi},  \]
but now  $\ket{0}$ denotes the vacuum state, $U, V$ are CV circuits and $\G$ is a (one or two-mode) CV gate. The original observable $\hat{O}$ that we are interested in is typically a quadrature $\x, \p$ (for homodyne detection) or the number projector $\ketbra{n}{n}$ (for photon number resolution) of a given mode. \maria{Any other measurements to consider?} We will have a look at both these settings.


\subsubsection{Homodyne measurement}

We will show now that for homodyne measurements, the derivative of the quadrature expectation can in many cases of interest be estimated by estimating the expectations of slightly different circuits. For this we do not need the LCU trick from qubits. This can be shown using the Heisenberg picture. \\

-\cg{At fist I was convinced that it would work in general, but now I have the following doubt: We are dealing with two representations here, the one in Hilbert space and the one in terms of creation and annihilation operators. The question is somehow, when is the right moment to switch representation. The Hilbert space representation has the advantage that operations are always linear. This is what allows us to ``pull'' the derivative past the state and the operation $V$ when deriving \eqref{Eq:der_of_exp}, irrespective of what $V$ is. If we use this representation, however, we need to calculate the derivatives in \eqref{Eq:der_of_exp} in Hilbert space. This is difficult for the CV operations and general qubit gates. We can, as you show here, elegantly compute the derivatives of the CV operations in their representation as maps on the creation operators. We would thus like do the whole calculation in this representation. However, the map $V$ is, as you also said, non-linear. If we want to compute the derivative with respect to some gate that is hidden behind such a $V$ in this representation, we thus need to use the chain rule. This problem goes away only if all stats and operations are Gaussian, i.e, linear operations on the creation operators. So, with this I think that the following only really woks for Gaussian circuits. Does that make sense?}\maria{I don't think that is a valid concern because we do not want to compute the derivative of anything inside this nonlinear transform. It is therefore a blackbox for the purpose of derivation. So no chain rule required...My question is only, can we use the Bogoliubov transform on a quadrature vector that has previously been mapped by a nonlinear transform. Don't see why not, but I have never seen it done either.}
\cg{I resurrected this comment, not because I want to be mean, but because I want to fully understand what is going on.
Let's go through this slowly:
We assume that only $\G$ depends on $\mu$.
If we represent everything in Hilbert space, then we have
\begin{equation}
  \begin{split}
    &\partial_{\mu} \bra{0}U^{\dagger} \G^{\dagger} V^{\dagger} \hat{O}V \G U \ket{0} \\
    = &\bra{0}U^{\dagger} \partial_{\mu} (\G^{\dagger} V^{\dagger} \hat{O}V \G) U \ket{0}
  \end{split}
\end{equation}
because of linearity and we can hence use the tricks described before.
Now let's look at the same thing in the quadrature picture.
For concreteness, let us chose $V = \I$, $\hat{O} = \hat{p}^2$, $U$ a a cubit phase gate with parameter $\gamma = 1$ and $\G$ as a phase rotation gate with parameter $\phi = \mu$ (with respect to which we want to differentiate).
By direct calculation we have (if I didn't make any stupid mistakes)
\begin{align}
    &\G^\dagger V^\dagger \hat{O} V G = \G^\dagger \hat{p}^2 G \\
    = &\sin^2(\mu) \hat{x}^2 + \cos^2(\mu) \hat{p}^2 - \sin(\mu) \cos(\mu) (\hat{x} \hat{p} + \hat{p} \hat{x})
\end{align}
and hence
\begin{align}
  &U^\dagger \G^\dagger V^\dagger \hat{O} V G U \nonumber\\
  = &\sin^2(\mu) \hat{x}^2 + \cos^2(\mu) \big( \hat{p}^2 + (\hat{p} \hat{x}^2 + \hat{x}^2 \hat{p}) + \hat{x}^4 \big)\\
  - &\sin(\mu) \cos(\mu) \big( \hat{x} (\hat{p}+\hat{x}^2)+ (\hat{p}+\hat{x}^2) \hat{x}  \big) \nonumber.
\end{align}
If I take the derivative with respect to $\mu$ at $\mu=0$ only the last term survives and I get
\begin{equation}
  \partial_\mu\Big|_{\mu=0} U^\dagger \G^\dagger V^\dagger \hat{O} V G U 
  = - \hat{x}\hat{p} - \hat{p}\hat{x} - 2 \hat{x}^3 .
\end{equation}
Now, let's do the same calculation again, assuming that we can pull the derivative $\partial_\mu$ past the outer $U^\dagger \cdot U$ (despite it being a non-linear gate), so that we can directly apply \eqref{eq:derivative_rhase_rotation}, without having to use a chain rule.
We then get that
\begin{align}
  \partial_\mu &(U^\dagger \G^\dagger V^\dagger \hat{O} V G U) \nonumber\\
  = &U^\dagger (\partial_\mu( R(\mu)^\dagger \hat{p}^2 R(\mu) )) U \\
  = &U^\dagger R(\mu+\frac{\pi}{2})^\dagger \hat{p}^2 R(\mu+\frac{\pi}{2}) U \\
  = &U^\dagger (R(\mu+\frac{\pi}{2})^\dagger \hat{p} R(\mu+\frac{\pi}{2}))^2 U \\
  = &U^\dagger (-\cos(\mu) \hat{x} - \sin(\mu) \hat{p})^2 U \\
  = &U^\dagger (\cos^2(\mu) \hat{x}^2 + \sin^2(\mu) \hat{p}^2 + \cos(\mu) \sin(\mu) (\hat{x} \hat{p} + \hat{p} \hat{x})) U
\end{align}
At $\mu = 0$ only the first term survives and we hence get
\begin{align}
  \partial_\mu &U^\dagger \G^\dagger V^\dagger \hat{O} V G U \nonumber\\
    = &U^\dagger \hat{x}^2 U = \hat{x}^2 ,
\end{align}
which is different from what we had before.
Maybe I am just making a trivial mistake in my calculation? Or maybe I am missing something obvious?
}
To make this claim more precise, denote the quadratures after the evolution with the circuit $V$ as $\c = V^{\dagger} \x V$ and $\d = V^{\dagger} \p V$. If $V$ is a Gaussian circuit, the modified quadratures are a linear combination of the original ones, while non-Gaussian circuits result in $\c, \d$ being polynomials in the original quadratures. \maria{Does $V$ have to be Gaussian in order for the idea below to work? }
Let $\hat{q} = \x, \p$ be either of the two operators. We want to express the derivative of the expectation as a linear combination of expectations,
\begin{equation}
	\partial_{\mu}\bra{\psi} \G(\mu)^{\dagger} \hat{q} \G(\mu) \ket{\psi} = \sum_{k=1}^K \gamma_k \bra{\psi} A_k^{\dagger} \hat{q} A_k \ket{\psi},
	\label{Eq:sumofsympl}
\end{equation}
in which the gate $\G$ has been replaced by another gate (or sequence of gates) $A_k$. \\

%Maybe this little digression shows why this may not be so difficult: Assume that the transformation of the modified quadratures $\hat{q}$ maps them to a new operator that is a polynomial $\mathcal{P}_d$ of order $d$ in the modified quadratures,
%\[ \G(\mu) \hat{q} \G^{\dagger}(\mu)  = \mathcal{P}_d(\c, \d) . \]
%The coefficients with which the different terms appear depend on the parameter $\mu$. Deriving this polynomial for $\mu$ will therefore result in a different polynomial  $\mathcal{P}'_{d'}$ (which may have a different order $d'$). This polynomial might not correspond to a valid transformation, but if it can be decomposed into ``valid polynomials'',
%\[ \mathcal{P}_d(\c, \d) = \sum_k \mathcal{P}^k_d(\c, \d), \]
%we can express the expectation as a sum of other expectations that give rise to $\mathcal{P}^k_d$. For example, if $\G$ is Gaussian,
%\begin{equation}
%	\partial_{\mu} \left( \G(\mu) \hat{q} \G^{\dagger}(\mu)\right)  = \partial_{\mu}a(\mu)  \c + \partial_{\mu}b(\mu) \d + \partial_{\mu}\alpha (\mu), 
%	\label{Eq:derofhomodyne}
%\end{equation}
%we have to try and express the right hand side as a weighed sum of affine symplectic transformations
%\[ \sum_{k=1}^K \gamma_k (\tilde{a}_k(\mu)  \c + \tilde{b}_k(\mu) \d + \tilde{\alpha}_k (\mu)) = \sum_{k=1}^K \gamma_k \; A_k^{\dagger} \hat{q} A_k. \]


We will now show how to do this for the standard Gaussian gates in the StrawberryFields elementary gate set, as well as for the cubic phase gate. In the case of Gaussian gates, we calculate the derivative of the Bogoliubov transform of the quadrature vector, and express it as a linear combination of Bogoliubov transforms, which can be translated back into linear combinations of expectations. For the cubic phase gate we look at the nonlinear transformation of quadratures directly.\\


\paragraph{Phase rotation.}
The Bogoliubov transform $ \mathcal{B}_R(\phi)$ of the phase rotation gate $R(\phi)$ acts as
\[
\begin{pmatrix}
\x'\\
\p'
\end{pmatrix} =  \underbrace{\begin{pmatrix} \cos(\phi) & \sin(\phi) \\
-\sin(\phi) & \cos(\phi) \end{pmatrix}}_{\mathcal{B}_R(\phi)}\begin{pmatrix} \x\\ \p \end{pmatrix}, \]
has the derivative
\[ \partial_{\phi} \; \mathcal{B}_R(\phi) = \begin{pmatrix} -\sin(\phi) & \cos(\phi) \\
-\cos(\phi) & -\sin(\phi) \end{pmatrix} ,\]
and hence
\[ \partial_{\phi} \; \mathcal{B}_R(\phi) = \mathcal{B}_R(\phi + \frac{\pi}{2}).\]
This effectively exchanges the position and momentum quadratures of the mode.\\

We therefore have
\begin{equation} \label{eq:derivative_rhase_rotation}
  \partial_{\phi}\bra{\psi} R(\phi)^{\dagger} \hat{q} R(\phi) \ket{\psi} =  \bra{\psi} R(\phi + \frac{\pi}{2})^{\dagger} \hat{q} R(\phi + \frac{\pi}{2}) \ket{\psi}. 
\end{equation}

\paragraph{Displacement.}

Displacement by a complex number $b = b_{\mathrm{re}} + i b_{\mathrm{im}}$ shifts the quadratures,
\[
\begin{pmatrix}
\x'\\
\p'
\end{pmatrix} = \begin{pmatrix} \x + b_{\mathrm{re}} \\ \p + b_{\mathrm{im}} \end{pmatrix}. \]

To use a similar notation as above, extend the quadrature vector by a constant entry,
\begin{equation}
\begin{pmatrix}
  \x' \\ \p' \\ \mathbbm{1}
\end{pmatrix}
 = 
\underbrace{\begin{pmatrix}
1 & 0 & b_{\mathrm{im}}  \\
0 & 1 & b_{\mathrm{re}} \\
0 & 0 & 1
\end{pmatrix}}_{\mathcal{B}_D(b)}
\begin{pmatrix}
\x \\ \p \\ \mathbbm{1}
\end{pmatrix}.
\end{equation}
The derivative of the Bogoliubov transform $\mathcal{B}_D(b)$ with respect to $b_{\mathrm{im}}$ and $b_{\mathrm{re}}$ is given by 
\begin{eqnarray*}
\partial_{b_{\mathrm{re}}} \mathcal{B}_D(b) &=& \begin{pmatrix}
0 & 0 & 0  \\
0 & 0 & 1 \\
0 & 0 & 0
\end{pmatrix},\\
\partial_{b_{\mathrm{im}}} \mathcal{B}_D(b) &=& \begin{pmatrix}
0 & 0 & 1  \\
0 & 0 & 0 \\
0 & 0 & 0
\end{pmatrix}.
\end{eqnarray*}
The gradient gates are therefore equivalent to the sum of a displacement of $1$ and $-1$ in the variable that we derive for, 
\begin{eqnarray*}
\partial_{b_{\mathrm{re}}} \mathcal{B}_D(b) &=& \frac{1}{2} \mathcal{B}_D(1, 0) - \frac{1}{2} \mathcal{B}_D(-1, 0) \\
\partial_{b_{\mathrm{im}}} \mathcal{B}_D(b) &=& \frac{1}{2} \mathcal{B}_D(0, 1) - \frac{1}{2} \mathcal{B}_D(0, -1)
\end{eqnarray*}
This leads to the final expressions
\begin{multline} 
\partial_{b_{\mathrm{re}}}\bra{\psi} D(b)^{\dagger} \hat{q} D(b) \ket{\psi} =  \frac{1}{2}\bra{\psi} D(1,0)^{\dagger} \hat{q} D(1,0) \ket{\psi} \\
- \frac{1}{2}\bra{\psi} D(1,0)^{\dagger} \hat{q} D(-1,0) \ket{\psi}, 
\end{multline}
and
\begin{multline} 
\partial_{b_{\mathrm{im}}}\bra{\psi} D(b)^{\dagger} \hat{q} D(b) \ket{\psi} =  \frac{1}{2}\bra{\psi} D(0,1)^{\dagger} \hat{q} D(1,0) \ket{\psi} \\ 
- \frac{1}{2}\bra{\psi} D(1,0)^{\dagger} \hat{q} D(0,-1) \ket{\psi} . 
\end{multline}

\paragraph{Squeezing.}
Squeezing a mode by a factor $z = r \e^{i\phi}$ with the squeezing operator $S(r, \phi)$ corresponds to the Bogoliubov transformation
\[ \begin{pmatrix} \x'\\ \p' \end{pmatrix} = \underbrace{\begin{pmatrix} \e^{-r} & 0 \\ 0 & \e^{r} \end{pmatrix} \begin{pmatrix} \cos \phi & \sin \phi \\ - \sin \phi & \cos \phi \end{pmatrix}}_{\mathcal{B}_S(r, \phi)} \begin{pmatrix} \x \\ \p \end{pmatrix}. \]
Then
\begin{eqnarray*} \partial_{r} \mathcal{B}_S(r, \phi) &=& \begin{pmatrix} -1 & 0 \\ 0 & 1 \end{pmatrix}
\mathcal{B}_S(r, \phi) \\
&=& \mathcal{B}_S(r-\log (\sqrt{2}-1), \phi) - \sqrt{2} \mathcal{B}_S(r, \phi),\end{eqnarray*}
and
\begin{eqnarray*} \partial_{\phi} \mathcal{B}_S(r, \phi) &=& \begin{pmatrix} \e^{-r} & 0 \\ 0 & \e^{r} \end{pmatrix} \begin{pmatrix} -\sin \phi & \cos \phi \\ - \cos \phi & -\sin \phi \end{pmatrix}\\
&=& \mathcal{B}_S(r, \phi + \frac{\pi}{2}).\end{eqnarray*}



\paragraph{Beam splitter.}
The beam splitter with $\theta$ and phase $\phi$ (using Strawberry Fields conventions) applied to modes $i$ and $j$ with quadrature vector $\xi = (x_i, p_i, x_j, p_j )^T$ applies the Bogoliubov transformation $B$ on the vector of quadrature operators,
\begin{equation} \label{eq:beamsplitter_as_matrix} \begin{pmatrix} \x'_i \\ \p'_i \\ \x'_j \\ \p'_j \end{pmatrix} =
\underbrace{\begin{pmatrix} \cos(\theta) \mathbbm{1} & -e^{-i\phi}\sin(\theta) \mathbbm{1}\\
e^{i\phi}\sin(\theta) \mathbbm{1} & \cos(\theta) \mathbbm{1} \end{pmatrix}}_{\mathcal{B}_{BS}(\theta, \phi)} \begin{pmatrix} \x_i \\ \p_i \\ \x_j \\ \p_j \end{pmatrix}.
\end{equation}

The derivative of the transformation $ \mathcal{B}_{BS}(\theta, \phi)$ with respect to $\theta$ is given by the matrix
\begin{eqnarray*}
\partial_{\theta} \mathcal{B}_{BS}(\theta, \phi) &=& \begin{pmatrix} -\sin(\theta) \mathbbm{1} & -e^{-i\phi}\cos(\theta) \mathbbm{1}\\
e^{i\phi}\cos(\theta) \mathbbm{1} & -\sin(\theta) \mathbbm{1} \end{pmatrix} \\
&=& \mathcal{B}_{BS}(\theta +\frac{\pi}{2}, \phi )
\end{eqnarray*}


For the phase parameter $\phi$, we get
\[ \partial_{\phi} \mathcal{B}_{BS}(\theta, \phi) = \begin{pmatrix} 0 & - e^{-i(\phi + \frac{\pi}{2})} \sin(\theta) \mathbbm{1}\\
e^{i(\phi + \frac{\pi}{2})}\sin(\theta) \mathbbm{1} & 0  \end{pmatrix}, \]
and consequently
\[\partial_{\phi} \mathcal{B}_{BS}(\theta, \phi) = \frac{1}{2}\mathcal{B}_{BS}(\theta, \phi + \frac{\pi}{2}) - \frac{1}{2}\mathcal{B}_{BS}(\theta, \phi + \frac{3\pi}{2}). \]


\paragraph{Cubic phase gate.}
The cubic phase gate $V(\mu)$ is a non-linear gate with the effect
\[V(\gamma)^{\dagger} \x V(\gamma) = \x, \;\; V(\gamma)^{\dagger} \p V(\gamma) = \p + \gamma \x^2. \]
Therefore, 
\begin{eqnarray}
\partial_{\gamma} V(\gamma)^{\dagger} \x V(\gamma) &=& 0\\
\partial_{\gamma} V(\gamma)^{\dagger} \p V(\gamma) &=& \x^2.
\end{eqnarray} 
The derivative of the operation for $\hat{q} = \x, \p$ is equivalent to 
\[ \partial_{\gamma} V(\gamma)^{\dagger} \hat{p} V(\gamma) =   V\left(1 \right)^{\dagger} \hat{p} V \left(1 \right) -  \hat{p}.
\]



\subsubsection{General case}

If we consider general observables $\hat{O}$ we could probably use a similar trick to the LCU-routine for qubits, using beam splitters. However, it is very difficult to decompose derivatives of gates into unitaries which represent elementary gate sequences 

\[ y = \bra{0}U^{\dagger} \G^{\dagger} V^{\dagger} \; \ketbra{n}{n} \; V \G U \ket{0} \]


\subsubsection{Photon number measurements}
Consider a fully Gaussian circuit with a subsequent photon number resolving measurement with observable $\ketbra{n}{n}$.  We have [Kruse et al 2018]
\[y = \bra{0}U^{\dagger} \G^{\dagger} V^{\dagger} \; \ketbra{n}{n} \; V \G U \ket{0} =  \frac{1}{\bar{n}! \sqrt{|\sigma_{Q}|} }\text{Haf}(\mathbf{A}_S).\]
The submatrix $\mathbf{A}_S$ depends on the photon sequence $\bar{n}$ which detemine which rows and columns of the circuit's A-matrix
\[ \mathbf{A} =\begin{pmatrix}0 & \mathbbm{1} \\ \mathbbm{1} & 0 \end{pmatrix} \left[ \mathbbm{1} - \sigma_Q^{-1} \right],\]
to construct it from. Here,
\[\sigma_Q = \sigma + \frac{1}{2} \mathbbm{1},\]
and $\sigma$ is the covariance matrix of the Gaussian state.
Quick and dirty, the gradient would be something like
\begin{multline}\partial_{\mu} y = -\frac{\sqrt{(|\sigma_Q| \tr \{ \sigma_Q^{-1} \partial_{\mu} \sigma \})}}{2\bar{n}! }\text{Haf}(\mathbf{A}_S) \\
+ \frac{1}{\bar{n}! \sqrt{|\sigma_{Q}|} )}\text{Haf}(\mathbf{A'}_S),
\end{multline}
with 
\[\mathbf{A}' = \begin{pmatrix}0 & \mathbbm{1} \\ \mathbbm{1} & 0 \end{pmatrix} \sigma_Q^{-1}(\partial_{\mu} \sigma) \sigma_Q^{-1}.\] 
\cg{As we discussed earlier, it seems pretty hopeless to calculate this. The reason is also clear: It is the whole point that calculating such probabilities is hard. Maybe one can do something for non-ideal particle number detectors? Like approximate their POVM elements by some polynomial in creation/annihilation operators and then take the derivative of that? It will not be the exact gradient but maybe it is good enough in practice?}

\maria{(Attempt 2: Not working!)}

Another starting point is to look at 
\[tr \{\rho \ketbra{n}{n}  \} \]
and 
\[\ketbra{n}{n} = (\adag)^n \ketbra{0}{0} (\a)^n \]
and consider the Heisenberg evolution of the mode operators for small photon numbers.
\[ (\G \adag \G^{\dagger})^n \ketbra{0}{0} ( \G^{\dagger} \a \G)^n  = (\mathcal{B}_{\G} (\adag))^n \ketbra{0}{0} (\mathcal{B}_{\G} (\a))^n  \]


\maria{(Attempt 3: Not working!)}
One could use a CV-LCU trick. For a gate $ \G = \e^{i\mu H}$ with hermitian generator $H$, we get $\partial_{\mu} \G = i H \G$. We can decompose $iH$ into a sum of two unitaries:
\[iH = i/2 \left[ \left( H +i \sqrt{1 - H^2}\right) +  \left( H -i \sqrt{1 - H^2} \right)\right] . \]
The problem is, how could we decompose and implement these unitaries with a given gate set? They will in general depend on rather nonlinear gates... 

%The LCU circuit is implemented with the help of an additional vacuum mode and beam splitters. We use the Heisenberg picture and will look at the evolution and measurement of the quadratures as an example. As explained above, we consider the observables $\c_1 = V^{\dagger} \x_1 V$ and $\d_1 = V^{\dagger} \p_1 V$, which result from evolving the original quadratures by the circuit $V$ that follows after the gate $\G$. \maria{How can we generalise?} \\
%
%Starting with $\ket{\psi}$ in the first mode (which just means that we already applied circuit $U$), and vacuum in the second, we apply the circuit shown in Fig. \ref{Fig:der_eval_cv}. The circuit consists of a 50-50 beam splitter with phase $\pi$, a Gaussian transformation on each mode separately, as well as another 50-50 beam splitter with phase $\pi$. We measure the expectation of the vector of the transformed quadrature operators $\c_1, \d$. \\
%
%
%\begin{figure}[t]
%$$
%\Qcircuit @C=1em @R=.7em {
%\lstick{\ket{\psi}} &  \multigate{1}{BS(\frac{\pi}{4}, \pi)}  & \gate{A}&  \multigate{1}{BS(\frac{\pi}{4}, \pi)} &  \qw \\
%\lstick{\ket{0}} &   \ghost{BS(\frac{\pi}{4}, \pi)}   & \gate{\G}&\ghost{BS(\frac{\pi}{4}, \pi)} & \qw \\
%}
%$$
%\caption{Derivative evaluation circuit.}
%\label{Fig:der_eval_cv}
%\end{figure}
%
%The overall initial quadrature vector is given by
%\[\xi = ( \c_1, \d_1, \x_2, \p_2 )^T, \]
%where we use $\x_2$ and $\p_2$ to describe the quadratures of the second mode (which does not get transformed by a subsequent circuit $V$).
%The Bogoliubov transformation of this vector by a beamsplitter, the Gaussian transformations $A$ and $\G$, and the second beamsplitter is given by
%\[ \frac{1}{2} \begin{pmatrix} \mathbbm{1} & -\mathbbm{1}\\ \mathbbm{1} & \mathbbm{1} \end{pmatrix} 
%\begin{pmatrix} \mathbf{S} & 0\\ 0 & \mathbf{R} \end{pmatrix}
%\begin{pmatrix} \mathbbm{1} & -\mathbbm{1}\\ \mathbbm{1} & \mathbbm{1} \end{pmatrix} 
%\xi + \frac{1}{\sqrt{2}} \begin{pmatrix} \mathbbm{1} & -\mathbbm{1}\\ \mathbbm{1} & \mathbbm{1} \end{pmatrix}  \begin{pmatrix} \mathbf{s} \\ \mathbf{r} \end{pmatrix}.
%\]
%Here, $(\mathbf{S}, \mathbf{s}) = \mathbf{S}\tilde{\xi} + \mathbf{s}$ is the symplectic transformation corresponding to the Gaussian gate $A$, and $(\mathbf{R}, \mathbf{r}) = \mathbf{R}\tilde{\xi} + \mathbf{r}$ the symplectic transformation corresponding to gate $\G$, acting on a quadrature vector $\tilde{\xi}$. The vectors $\mathbf{s}, \mathbf{r}$ contain the real and imaginary part of the displacement, i.e., $\mathbf{s} = (s_{\text{Re}}, s_{\text{Im}})^T$
%
%%After the second beam splitter, we get the final quadrature vector
%%\begin{widetext}
%%\[\begin{pmatrix} \c_1' \\ \d' \\ \x_2' \\ \p_2' \end{pmatrix} =  \frac{1}{2}\begin{pmatrix}(s_{11}- r_{11})\c_1 +(s_{12}- r_{12})\d_1  -(s_{11}+ r_{11})\x_2 - (s_{12}+ r_{12}) \p_2  +  \alpha_1 - \beta_1\\
%%(s_{21}- r_{21}) \c_1+ (s_{22}- r_{22}) \d_1 - (s_{21}+ r_{21}) \x_2 -(s_{22}+ r_{22}) \p_2  + \alpha_2  - \beta_2 \\
%%(s_{11}+r_{11}) \c_1 + (s_{12}+r_{12}) \d_1 + (r_{11}-s_{11}) \x_2 + (r_{12}-s_{12}) \p_2 + \alpha_1   + \beta_1\\
%%(s_{21}+r_{21}) \c_1 + ( s_{22}+ r_{22}) \d_1 + (r_{21}-s_{21}) \x_2 + ( r_{22}- s_{22}) \p_2 + \alpha_2  + \beta_2  \end{pmatrix}.\]
%%\end{widetext}
%This corresponds to following overall transformation:
%\[ \frac{1}{2} \begin{pmatrix} \mathbf{S}-\mathbf{R} & -\mathbf{S}-\mathbf{R}\\ \mathbf{S}+\mathbf{R} & -\mathbf{S}+\mathbf{R}\end{pmatrix}
%\xi + \frac{1}{\sqrt{2}} \begin{pmatrix} \mathbf{s}- \mathbf{r}\\  \mathbf{s} + \mathbf{r} \end{pmatrix}.
%\]
%Taking the expectation of the time evolved quadrature operators of the first mode with respect to the initial state $\ket{\psi} \otimes \ket{0}$ results in
%\begin{multline*}
%\c'=	\frac{1}{2} \bigg[ (s_{11}-r_{11}) \bra{\psi}\c_1\ket{\psi} \\ + (s_{12}-r_{12}) \bra{\psi}\d\ket{\psi} + s_{\text{Re}} - r_{\text{Re}} \bigg]
%\end{multline*}
%for the ``position'' quadrature, and
%\begin{multline*}
%\d' =	\frac{1}{2} \bigg[ (s_{21}-r_{21}) \bra{\psi}\c_1\ket{\psi} \\ +(s_{22}-r_{22}) \bra{\psi}\d\ket{\psi} + s_{\text{Im}} - r_{\text{Im}} \bigg]
%\end{multline*}
%for the ``momentum'' quadrature. Here we used that the expectations $\bra{0}\x_2\ket{0}$ and $\bra{0}\p_2\ket{0}$ evaluate to zero. The values $s_{11}, r_{11},...$ are the entries of $\mathbf{S}$ and $\mathbf{R}$. \\
%
%The operation is formally equivalent to the expectation of $(\c_1, \d_1)^{\dagger}$ with respect to $\ket{\psi} \otimes \ket{0}$ after the (non-symplectic) transformation
%\[(\mathbf{S}-\mathbf{R})\begin{pmatrix} \c_1 \\ \d_1 \end{pmatrix} + \mathbf{s} - \mathbf{r},\]
%which is what we required 
%To get the sum of the two, we simply consider a homodyne measurement of in the second mode. \\


\section{Heisenberg representation of gates}
For completeness, let's look at the effect of gates in the Heisenberg representation and how this affects differentiation formulas.
At the highest level of generality, we have an operator algebra which contains all possible polynomials of the quadrature operators $\hat{x}$, $\hat{p}$, and $\mathbbm{1}$. For notational convenience, let us represent each of these operators in the form $\hat{O}_i$, i.e., 
\begin{align}
 \hat{O}_0 = & \mathbbm{1} \\
 \hat{O}_1 = & \hat{x} \\
 \hat{O}_2 = & \hat{p}.
\end{align}

Consider now a parameterized unitary gate $\mathcal{G}=\mathcal{G}_\theta$, which we rewrite in the Heisenberg picture as a transformation $\Omega$, i.e.,
\begin{equation}
 \mathcal{G}^\dagger \hat{O}_i \mathcal{G} = \Omega[\hat{O}_i].
\end{equation}
Suppose the gate is Gaussian. Then it will map the operators $\{\hat{O}_j\}$ linearly amongst themselves: 
\begin{equation}
 \Omega[\hat{O}_j] = \sum_{i=0}^2 c_{ij} \hat{O}_i.
\end{equation}
Such a transformation has a representation as a $3\times 3$ matrix $A$ with two indices, i.e., 
\begin{equation}
 [A]_{ij} = c_{ij}.
\end{equation}

Since the gate $\Omega$ is unitary, it is trace-preserving, so we have 
\begin{align}
 \mathrm{Tr}(\mathcal{G}\rho \mathcal{G}^\dagger) 
 & = \mathrm{Tr}(\mathcal{G}^\dagger\mathbbm{1}\mathcal{G}\rho ) \nonumber \\
 & = \mathrm{Tr}(\Omega[\mathbbm{1}]\rho ) \nonumber \\
 & = 1,
\end{align}
which holds for all $\rho$. Hence $\Omega$ maps the identity to identity, or,
\begin{align}
 c_{00} = & 1 \\
 c_{10} = & c_{20} = 0.
\end{align}
For convenience, we will break apart the matrix $A$ into the following block form:
\begin{equation}
 A = \left[
       \begin{array}{c|cc}
        1 & N \\
        \hline
        0 & M
       \end{array}
     \right],
\end{equation}
where
\begin{equation}
 N = \begin{bmatrix}
      c_{01} & c_{02}
     \end{bmatrix}
\end{equation}
and
\begin{equation}
 M = \begin{bmatrix}
      c_{11} & c_{12} \\
      c_{21} & c_{22}
     \end{bmatrix}.
\end{equation}

The matrix $A$ specifies completely how the Gaussian gate $\Omega$ transforms the operators $\{\hat{O}_i\}$. What about higher powers of these operators, e.g., $\hat{x}^2$, $\hat{x}\hat{p}$, etc.?
Using the fact that $\Omega$ is a unitary conjugation, we must have
\begin{align}
 \Omega[\hat{O}_i\hat{O}_j] 
 & = \mathcal{G}^\dagger \hat{O}_i \hat{O}_j \mathcal{G} \nonumber \\
 & = \mathcal{G}^\dagger \hat{O}_i \mathcal{G} \mathcal{G}^\dagger\hat{O}_j \mathcal{G} \nonumber\\
 & = \Omega[\hat{O}_i] \Omega[\hat{O}_j].
\end{align}
Since the gate is Gaussian, it will not raise the order, so the result has only second-order or lower operator combinations:
\begin{align}
 \Omega[\hat{O}_i\hat{O}_j] 
 & = \Omega[\hat{O}_i] \Omega[\hat{O}_j] \nonumber \\
 & = \sum_{k=0}^2 \sum_{l=0}^2 c_{ki}c_{lj} \hat{O}_k \hat{O}_l.
\end{align}
Also notice that we don't really need to consider the effect of $\Omega[\hat{O}_0\hat{O}_j]$ or $\Omega[\hat{O}_i\hat{O}_0]$, since these products will be accounted for by the lower-order matrix $A$.
As before, we can represent the effect of the transformation $\Omega$ on these second-order operators as a matrix $B$ whose matrix elements are determined by the coefficients $c_{ij}$. Interestingly, we now have a tensor product structure:
\begin{equation}
 [B]_{kl,ij} = c_{ki} c_{lj} = A_{ki}A_{lj} = [A\otimes A]_{kl,ij}.
\end{equation}

Moving to the issue of computing gradients, we can use the unitary conjugatation structure, along with the Leibniz rule, to deduce the gradient circuits for second-order operators:
\begin{align}
 \nabla_\theta\left(\Omega[\hat{O}_i\hat{O}_j] \right)
 & = \nabla_\theta\left(\Omega[\hat{O}_i] \Omega[\hat{O}_j]\right) \nonumber \\
 & = \nabla_\theta\left( \Omega[\hat{O}_i] \right)\Omega[\hat{O}_j] 
 + \Omega[\hat{O}_i]\nabla_\theta \left( \Omega[\hat{O}_j] \right) \nonumber \\
 & = \nabla_\theta\Omega [\hat{O}_i] \Omega[\hat{O}_j] 
 + \Omega[\hat{O}_i]\nabla_\theta \Omega[\hat{O}_j],
\end{align}
where $\nabla_\theta\Omega$ is the derivative of the Bogoliubov transformation, as explored above.
Thus, the derivative of the second-order operator decomposes naturally as we would expect from the Leibniz rule (remembering that the operators may not commute).

As an first example, let's consider the phase-shift gate. For this, we have $\nabla_\phi\Omega_\phi = \Omega_{\phi+\frac{\pi}{2}}$. Looking at, for example, the effect on $\hat{O}_1\hat{O}_1=\hat{x}^2$, we have
\begin{align}
 \nabla_\phi\left( \Omega_\phi[\hat{x}^2] \right) 
 & = \Omega_{\phi+\frac{\pi}{2}}[\hat{x}]\Omega_{\phi}[\hat{x}]
   + \Omega_{\phi}[\hat{x}]\Omega_{\phi+\frac{\pi}{2}}[\hat{x}] \nonumber \\
 & = \hat{x}_{\phi+\frac{\pi}{2}} \hat{x}_\phi + \hat{x}_\phi \hat{x}_{\phi+\frac{\pi}{2}} \nonumber \\
 & = \hat{p}_\phi\hat{x}_\phi + \hat{x}_\phi\hat{p}_\phi \nonumber \\
 & = \Omega_\phi[\hat{p}\hat{x} + \hat{x}\hat{p}].
\end{align}
Hence, we can compute the gradient of the phase shift gate $\Omega_\phi$ applied to $\hat{x}^2$ by replacing the operator with another one of the same order. The beamsplitter gate should follow similarly to the phase-shift gate.

For a second example, we look at the (phaseless) squeeze gate. In this case, we have $\nabla_r\Omega_r = \Omega_{r-\log(\sqrt{2} - 1)} - \sqrt{2}\Omega_{r}$. Again, we look at the effect on $\hat{x}^2$, we have
\begin{align}
  \nabla_r\left( \Omega_r[\hat{x}^2] \right) 
 & = & \left( \Omega_{r-\log(\sqrt{2} - 1)}[\hat{x}] - \sqrt{2}\Omega_{r}[\hat{x}] \right)\Omega_{r}[\hat{x}] \nonumber \\
 &   & + \Omega_{r}[\hat{x}] \left( \Omega_{r-\log(\sqrt{2} - 1)}[\hat{x}] - \sqrt{2}\Omega_{r}[\hat{x}] \right) \nonumber \\
 & = & 2\left( e^{-r}(\sqrt{2} - 1)\hat{x} - \sqrt{2}e^{-r}\hat{x} \right)e^{-r}\hat{x} \nonumber \\
 & = & -2e^{-2r}\hat{x}^2 \nonumber \\
 & = & -2\Omega_{r}[\hat{x}^2].
\end{align}
This case is simpler than the previous one, since (phaseless) squeezing does not mix $\hat{x}$ and $\hat{p}$.. The gradient of $\Omega_r$ acting on $\hat{x}^2$ is just $-2$ times the action of the same gate acting on the same operator.

Using this recipe, we should be able to compute the gradients of any of the gates considered in the previous section when acting on higher-order quadrature powers (including cubic phase). The main concern is that some gates, when applied multiple times, may cause very quick blowup in the number of operators we have to keep track of, which may become inefficient.




 




\end{document}
